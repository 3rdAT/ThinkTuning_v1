Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.08s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                              [0m| 0/225 [00:00<?, ?it/s][0m
Preparing 4D causal attention mask with cache position
torch.Size([812])
tensor(1.0440, device='cuda:3', grad_fn=<MeanBackward0>)
The gate values are: tensor([[[0.1958],
         [0.2726],
         [0.1058],
         [0.3714],
         [0.3916],
         [0.2740],
         [0.4229],
         [0.7671],
         [0.4737],
         [0.5472],
         [0.9853],
         [0.9698],
         [0.7016],
         [0.9957],
         [0.9601],
         [0.7208],
         [0.9583],
         [0.9665],
         [0.9929],
         [0.8767],
         [0.8478],
         [0.4412],
         [0.4491],
         [0.2037],
         [0.9801],
         [0.9436],
         [0.5991],
         [0.7492],
         [0.9232],
         [0.9571],
         [0.9432],
         [0.9863],
         [0.9951],
         [0.8170],
         [0.9986],
         [0.9957],
         [0.9688],
         [0.9471],
         [0.9948],
         [0.9906],
         [0.9972],
         [0.9844],
         [0.9991],
         [0.8117],
         [0.7815],
         [0.9783],
         [0.8439],
         [0.6555],
         [0.9219],
         [0.8803],
         [0.7600],
         [0.9750],
         [0.9817],
         [0.9412],
         [0.8823],
         [0.9816],
         [0.9808],
         [0.9774],
         [0.9124],
         [0.8287],
         [0.8195],
         [0.9421],
         [0.9457],
         [0.8461],
         [0.9692],
         [0.9570],
         [0.6377],
         [0.6766],
         [0.9902],
         [0.9951],
         [0.9906],
         [0.9764],
         [0.9601],
         [0.8498],
         [0.3279],
         [0.9977],
         [0.9979],
         [0.9974],
         [0.9922],
         [0.9956],
         [0.9533],
         [0.9941],
         [0.9164],
         [0.8494],
         [0.3192],
         [0.8894],
         [0.6773],
         [0.9724],
         [0.9933],
         [0.9677],
         [0.9887],
         [0.9992],
         [0.9757],
         [0.6611],
         [0.9959],
         [0.9923],
         [0.9913],
         [0.9963],
         [0.9991],
         [0.9990],
         [0.9796],
         [0.9948],
         [0.9779],
         [0.9846],
         [0.9865],
         [0.9900],
         [0.9925],
         [0.9906],
         [0.9763],
         [0.9988],
         [0.9700],
         [0.9927],
         [0.9903],
         [0.9898],
         [0.9450],
         [0.9957],
         [0.8878],
         [0.9920],
         [0.9921],
         [0.9802],
         [0.9904],
         [0.9953],
         [0.9841],
         [0.7044],
         [0.9996],
         [0.9985],
         [0.9653],
         [0.9930],
         [0.9749],
         [0.9981],
         [0.7351],
         [0.7268],
         [0.2516],
         [0.9750],
         [0.9842],
         [0.8421],
         [0.9204],
         [0.9891],
         [0.9761],
         [0.9539],
         [0.9472],
         [0.9531],
         [0.9887],
         [0.8794],
         [0.9960],
         [0.8343],
         [0.9966],
         [0.8455],
         [0.9923],
         [0.9362],
         [0.4912],
         [0.9965],
         [0.9975],
         [0.9786],
         [0.9877],
         [0.9467],
         [0.9766],
         [0.9790],
         [0.9067],
         [0.9876],
         [0.9828],
         [0.6209],
         [0.9862],
         [0.9853],
         [0.9661],
         [0.7842],
         [0.9964],
         [0.8647],
         [0.9970],
         [0.9320],
         [0.9889],
         [0.9515],
         [0.6633],
         [0.9331],
         [0.9247],
         [0.9368],
         [0.9955],
         [0.8816],
         [0.1045],
         [0.9421],
         [0.9334],
         [0.9852],
         [0.9152],
         [0.8538],
         [0.9490],
         [0.9465],
         [0.9812],
         [0.9932],
         [0.7360],
         [0.9986],
         [0.9973],
         [0.9758],
         [0.9919],
         [0.9818],
         [0.9964],
         [0.9949],
         [0.9795],
         [0.9133],
         [0.9968],
         [0.9605],
         [0.9918],
         [0.9589],
         [0.6934],
         [0.6736]],

        [[0.1958],
         [0.2726],
         [0.1058],
         [0.3714],
         [0.3916],
         [0.1924],
         [0.2572],
         [0.8090],
         [0.9111],
         [0.9179],
         [0.9950],
         [0.9839],
         [0.9940],
         [0.7770],
         [0.9811],
         [0.9933],
         [0.6882],
         [0.9837],
         [0.9094],
         [0.9769],
         [0.9899],
         [0.1503],
         [0.2355],
         [0.9846],
         [0.2436],
         [0.7386],
         [0.9951],
         [0.5121],
         [0.7069],
         [0.9795],
         [0.9938],
         [0.8095],
         [0.9882],
         [0.9569],
         [0.9903],
         [0.6206],
         [0.7246],
         [0.8269],
         [0.9609],
         [0.9663],
         [0.9976],
         [0.9790],
         [0.9706],
         [0.9278],
         [0.2898],
         [0.6708],
         [0.9796],
         [0.9160],
         [0.7105],
         [0.6537],
         [0.9910],
         [0.9803],
         [0.9597],
         [0.9617],
         [0.9956],
         [0.9920],
         [0.9971],
         [0.9951],
         [0.8420],
         [0.9260],
         [0.9133],
         [0.7976],
         [0.9133],
         [0.4453],
         [0.2466],
         [0.9610],
         [0.8913],
         [0.5554],
         [0.8231],
         [0.9677],
         [0.9463],
         [0.9681],
         [0.9748],
         [0.9611],
         [0.9771],
         [0.9951],
         [0.9904],
         [0.9683],
         [0.9889],
         [0.9585],
         [0.5468],
         [0.9844],
         [0.8579],
         [0.8085],
         [0.9978],
         [0.9807],
         [0.7777],
         [0.9807],
         [0.9913],
         [0.9375],
         [0.9499],
         [0.9916],
         [0.9967],
         [0.9934],
         [0.9973],
         [0.9931],
         [0.9958],
         [0.9486],
         [0.3365],
         [0.3408],
         [0.9638],
         [0.9889],
         [0.9858],
         [0.8762],
         [0.8020],
         [0.9792],
         [0.9001],
         [0.9731],
         [0.9884],
         [0.9938],
         [0.9647],
         [0.7967],
         [0.9926],
         [0.9931],
         [0.7239],
         [0.1761],
         [0.9614],
         [0.9204],
         [0.8408],
         [0.9772],
         [0.9950],
         [0.9891],
         [0.9724],
         [0.9781],
         [0.5163],
         [0.5434],
         [0.5121],
         [0.9783],
         [0.9275],
         [0.6925],
         [0.9903],
         [0.9576],
         [0.9107],
         [0.9810],
         [0.9766],
         [0.9973],
         [0.9983],
         [0.9949],
         [0.9879],
         [0.9788],
         [0.9922],
         [0.9964],
         [0.9738],
         [0.5546],
         [0.9529],
         [0.9660],
         [0.9505],
         [0.8740],
         [0.7540],
         [0.9779],
         [0.9975],
         [0.9859],
         [0.8671],
         [0.9977],
         [0.9120],
         [0.9108],
         [0.9423],
         [0.9888],
         [0.9973],
         [0.8047],
         [0.1594],
         [0.9365],
         [0.9751],
         [0.9821],
         [0.9977],
         [0.9963],
         [0.9884],
         [0.7516],
         [0.9878],
         [0.9565],
         [0.8299],
         [0.9797],
         [0.9856],
         [0.9932],
         [0.9856],
         [0.8911],
         [0.9616],
         [0.9861],
         [0.7734],
         [0.9753],
         [0.9255],
         [0.9694],
         [0.9746],
         [0.9543],
         [0.9885],
         [0.9115],
         [0.7952],
         [0.9679],
         [0.6583],
         [0.8841],
         [0.9563],
         [0.9188],
         [0.5950],
         [0.9909],
         [0.4460],
         [0.4480],
         [0.4288],
         [0.9622],
         [0.9028],
         [0.8371],
         [0.9431],
         [0.8832],
         [0.6106],
         [0.5812]],

        [[0.1958],
         [0.2726],
         [0.1058],
         [0.3714],
         [0.3916],
         [0.8335],
         [0.5896],
         [0.5650],
         [0.7713],
         [0.9301],
         [0.4707],
         [0.0644],
         [0.9656],
         [0.2320],
         [0.9854],
         [0.7846],
         [0.0969],
         [0.9854],
         [0.9653],
         [0.8921],
         [0.9321],
         [0.8769],
         [0.9938],
         [0.9265],
         [0.9311],
         [0.5320],
         [0.9595],
         [0.9786],
         [0.8754],
         [0.9764],
         [0.8814],
         [0.9940],
         [0.9583],
         [0.8428],
         [0.9317],
         [0.8810],
         [0.9164],
         [0.9076],
         [0.7848],
         [0.9174],
         [0.9910],
         [0.9563],
         [0.9723],
         [0.9937],
         [0.9943],
         [0.7615],
         [0.9472],
         [0.9822],
         [0.3820],
         [0.0094],
         [0.9811],
         [0.7649],
         [0.9597],
         [0.9962],
         [0.9794],
         [0.8379],
         [0.2501],
         [0.0078],
         [0.9669],
         [0.9296],
         [0.9899],
         [0.8910],
         [0.9136],
         [0.8827],
         [0.9712],
         [0.8682],
         [0.9668],
         [0.9676],
         [0.9561],
         [0.8619],
         [0.9107],
         [0.6051],
         [0.2475],
         [0.9363],
         [0.8233],
         [0.6330],
         [0.9821],
         [0.7505],
         [0.7215],
         [0.9124],
         [0.9390],
         [0.7970],
         [0.9110],
         [0.9483],
         [0.9203],
         [0.9583],
         [0.9225],
         [0.9783],
         [0.9811],
         [0.9854],
         [0.8186],
         [0.9135],
         [0.9903],
         [0.9480],
         [0.9822],
         [0.9834],
         [0.8304],
         [0.7811],
         [0.9896],
         [0.9971],
         [0.8484],
         [0.8789],
         [0.9866],
         [0.9956],
         [0.8918],
         [0.3818],
         [0.4864],
         [0.9705],
         [0.9974],
         [0.9939],
         [0.4383],
         [0.9050],
         [0.9849],
         [0.9852],
         [0.9854],
         [0.8816],
         [0.9400],
         [0.9930],
         [0.9875],
         [0.2433],
         [0.9435],
         [0.9915],
         [0.9691],
         [0.7979],
         [0.5415],
         [0.9926],
         [0.9736],
         [0.2351],
         [0.9529],
         [0.9156],
         [0.8420],
         [0.3099],
         [0.8919],
         [0.9806],
         [0.9833],
         [0.9571],
         [0.8726],
         [0.9774],
         [0.9246],
         [0.9698],
         [0.9805],
         [0.9866],
         [0.8915],
         [0.9487],
         [0.9039],
         [0.9780],
         [0.9913],
         [0.9726],
         [0.9827],
         [0.9751],
         [0.9857],
         [0.9972],
         [0.9385],
         [0.8581],
         [0.8939],
         [0.9860],
         [0.8899],
         [0.9276],
         [0.9950],
         [0.9993],
         [0.9149],
         [0.9336],
         [0.9948],
         [0.9795],
         [0.6926],
         [0.9994],
         [0.9757],
         [0.8958],
         [0.7944],
         [0.8420],
         [0.9962],
         [0.9946],
         [0.5356],
         [0.9798],
         [0.9972],
         [0.9518],
         [0.7232],
         [0.9947],
         [0.9978],
         [0.7436],
         [0.2270],
         [0.9862],
         [0.9761],
         [0.8206],
         [0.9743],
         [0.8851],
         [0.1919],
         [0.9132],
         [0.8603],
         [0.9630],
         [0.9366],
         [0.9842],
         [0.9264],
         [0.9756],
         [0.9939],
         [0.9919],
         [0.6970],
         [0.9915],
         [0.1619],
         [0.0553],
         [0.9813],
         [0.9571],
         [0.9161],
         [0.6229]],

        [[0.1958],
         [0.2726],
         [0.1058],
         [0.3714],
         [0.3916],
         [0.2740],
         [0.4229],
         [0.7671],
         [0.4737],
         [0.5472],
         [0.8510],
         [0.9844],
         [0.9788],
         [0.8366],
         [0.9892],
         [0.9968],
         [0.9870],
         [0.9450],
         [0.9448],
         [0.3973],
         [0.9772],
         [0.9964],
         [0.9403],
         [0.9080],
         [0.6449],
         [0.4979],
         [0.2236],
         [0.9833],
         [0.9497],
         [0.7256],
         [0.5533],
         [0.8923],
         [0.5837],
         [0.9127],
         [0.9451],
         [0.9316],
         [0.4025],
         [0.9831],
         [0.9822],
         [0.9200],
         [0.9911],
         [0.9926],
         [0.9926],
         [0.9402],
         [0.9821],
         [0.8373],
         [0.9893],
         [0.9840],
         [0.9464],
         [0.9901],
         [0.9870],
         [0.9944],
         [0.9914],
         [0.9326],
         [0.9805],
         [0.6720],
         [0.9969],
         [0.9757],
         [0.9734],
         [0.9465],
         [0.9840],
         [0.8899],
         [0.9993],
         [0.9236],
         [0.9260],
         [0.9674],
         [0.7399],
         [0.9934],
         [0.9010],
         [0.9189],
         [0.9601],
         [0.9769],
         [0.9905],
         [0.9830],
         [0.9922],
         [0.9626],
         [0.9402],
         [0.7671],
         [0.9973],
         [0.9960],
         [0.9261],
         [0.4112],
         [0.4108],
         [0.7398],
         [0.6596],
         [0.9730],
         [0.9777],
         [0.8924],
         [0.9950],
         [0.9310],
         [0.9954],
         [0.8828],
         [0.9971],
         [0.9931],
         [0.9735],
         [0.9463],
         [0.9052],
         [0.9983],
         [0.9538],
         [0.9310],
         [0.5442],
         [0.9901],
         [0.9809],
         [0.8706],
         [0.8736],
         [0.9871],
         [0.9799],
         [0.4541],
         [0.5752],
         [0.9944],
         [0.9822],
         [0.6658],
         [0.8044],
         [0.9879],
         [0.9923],
         [0.5529],
         [0.6193],
         [0.9903],
         [0.9930],
         [0.7317],
         [0.4288],
         [0.9758],
         [0.9948],
         [0.8269],
         [0.4099],
         [0.9740],
         [0.9391],
         [0.7134],
         [0.3067],
         [0.9856],
         [0.9866],
         [0.9941],
         [0.9925],
         [0.9965],
         [0.9993],
         [0.9969],
         [0.9834],
         [0.9439],
         [0.4077],
         [0.9407],
         [0.9882],
         [0.9535],
         [0.9969],
         [0.9903],
         [0.9798],
         [0.9493],
         [0.9981],
         [0.9706],
         [0.6286],
         [0.9442],
         [0.9900],
         [0.9766],
         [0.5149],
         [0.6645],
         [0.9924],
         [0.9484],
         [0.5161],
         [0.8854],
         [0.9909],
         [0.9865],
         [0.7959],
         [0.8878],
         [0.9926],
         [0.9917],
         [0.8364],
         [0.9293],
         [0.9929],
         [0.9949],
         [0.9742],
         [0.8994],
         [0.9871],
         [0.9802],
         [0.7783],
         [0.9115],
         [0.9171],
         [0.9831],
         [0.7768],
         [0.0891],
         [0.9267],
         [0.9116],
         [0.9694],
         [0.8376],
         [0.8055],
         [0.9698],
         [0.9655],
         [0.5615],
         [0.9871],
         [0.9911],
         [0.9190],
         [0.9980],
         [0.9944],
         [0.9921],
         [0.9636],
         [0.9899],
         [0.8755],
         [0.9976],
         [0.9932],
         [0.9906],
         [0.9552],
         [0.8536],
         [0.8595],
         [0.9950],
         [0.9343],
         [0.5962]]], device='cuda:3', grad_fn=<SigmoidBackward0>)
The shape is: torch.Size([4, 204, 1])
The shape of new_sequence: torch.Size([4, 204])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:3')
torch.Size([72])
torch.Size([1])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([72])
torch.Size([1])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([305])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8058)
idx:  71
Gate loss:  tensor([[0.1578],
        [0.2196],
        [0.0852],
        [0.2993],
        [0.3156],
        [0.2208],
        [0.3408],
        [0.6182],
        [0.3817],
        [0.4410],
        [0.7940],
        [0.7815],
        [0.5654],
        [0.8024],
        [0.7737],
        [0.5808],
        [0.7723],
        [0.7788],
        [0.8001],
        [0.7065],
        [0.6832],
        [0.3555],
        [0.3619],
        [0.1642],
        [0.7898],
        [0.7604],
        [0.4828],
        [0.6037],
        [0.7440],
        [0.7713],
        [0.7601],
        [0.7948],
        [0.8019],
        [0.6584],
        [0.8047],
        [0.8023],
        [0.7807],
        [0.7632],
        [0.8016],
        [0.7983],
        [0.8036],
        [0.7933],
        [0.8051],
        [0.6541],
        [0.6297],
        [0.7883],
        [0.6801],
        [0.5282],
        [0.7429],
        [0.7093],
        [0.6125],
        [0.7857],
        [0.7911],
        [0.7585],
        [0.7110],
        [0.7910],
        [0.7904],
        [0.7876],
        [0.7352],
        [0.6678],
        [0.6604],
        [0.7592],
        [0.7621],
        [0.6818],
        [0.7810],
        [0.7712],
        [0.5139],
        [0.5452],
        [0.7980],
        [0.8019],
        [0.7983],
        [0.7868],
        [0.7737],
        [0.6848],
        [0.2643],
        [0.8040],
        [0.8042],
        [0.8037],
        [0.7996],
        [0.8023],
        [0.7682],
        [0.8011],
        [0.7384],
        [0.6845],
        [0.2572],
        [0.7167],
        [0.5458],
        [0.7836],
        [0.8005],
        [0.7798],
        [0.7967],
        [0.8052],
        [0.7863],
        [0.5327],
        [0.8026],
        [0.7996],
        [0.7988],
        [0.8029],
        [0.8051],
        [0.8050],
        [0.7894],
        [0.8016],
        [0.7880],
        [0.7935],
        [0.7950],
        [0.7978],
        [0.7998],
        [0.7983],
        [0.7867],
        [0.8049],
        [0.7817],
        [0.8000],
        [0.7980],
        [0.7976],
        [0.7615],
        [0.8024],
        [0.7154],
        [0.7994],
        [0.7995],
        [0.7899],
        [0.7981],
        [0.8021],
        [0.7931],
        [0.5676],
        [0.8055],
        [0.8046],
        [0.7779],
        [0.8002],
        [0.7856],
        [0.8043],
        [0.5924],
        [0.5857],
        [0.2028],
        [0.7857],
        [0.7931],
        [0.6786],
        [0.7417],
        [0.7970],
        [0.7866],
        [0.7687],
        [0.7633],
        [0.7681],
        [0.7967],
        [0.7086],
        [0.8026],
        [0.6723],
        [0.8031],
        [0.6813],
        [0.7996],
        [0.7544],
        [0.3958],
        [0.8030],
        [0.8038],
        [0.7886],
        [0.7959],
        [0.7629],
        [0.7870],
        [0.7889],
        [0.7307],
        [0.7958],
        [0.7920],
        [0.5004],
        [0.7947],
        [0.7940],
        [0.7785],
        [0.6320],
        [0.8030],
        [0.6968],
        [0.8034],
        [0.7511],
        [0.7969],
        [0.7668],
        [0.5345],
        [0.7519],
        [0.7451],
        [0.7549],
        [0.8022],
        [0.7104],
        [0.0842],
        [0.7592],
        [0.7522],
        [0.7939],
        [0.7375],
        [0.6880],
        [0.7647],
        [0.7627],
        [0.7907],
        [0.8003],
        [0.5931],
        [0.8047],
        [0.8037],
        [0.7863],
        [0.7993],
        [0.7911],
        [0.8029],
        [0.8017],
        [0.7893],
        [0.7359],
        [0.8033],
        [0.7740],
        [0.7992],
        [0.7727],
        [0.5588],
        [0.5428]], device='cuda:3', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:3')
torch.Size([73])
torch.Size([1])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7714)
idx:  72
Gate loss:  tensor([[0.3088],
        [0.4299],
        [0.1668],
        [0.5858],
        [0.6177],
        [0.4322],
        [0.6670],
        [1.2099],
        [0.7471],
        [0.8631],
        [1.5541],
        [1.5296],
        [1.1066],
        [1.5705],
        [1.5143],
        [1.1368],
        [1.5115],
        [1.5244],
        [1.5660],
        [1.3828],
        [1.3371],
        [0.6958],
        [0.7083],
        [0.3213],
        [1.5458],
        [1.4883],
        [0.9449],
        [1.1817],
        [1.4562],
        [1.5096],
        [1.4877],
        [1.5556],
        [1.5696],
        [1.2886],
        [1.5750],
        [1.5704],
        [1.5281],
        [1.4937],
        [1.5690],
        [1.5625],
        [1.5729],
        [1.5526],
        [1.5757],
        [1.2802],
        [1.2326],
        [1.5430],
        [1.3311],
        [1.0339],
        [1.4541],
        [1.3884],
        [1.1987],
        [1.5378],
        [1.5483],
        [1.4845],
        [1.3915],
        [1.5483],
        [1.5470],
        [1.5416],
        [1.4390],
        [1.3071],
        [1.2926],
        [1.4859],
        [1.4916],
        [1.3345],
        [1.5286],
        [1.5094],
        [1.0058],
        [1.0671],
        [1.5618],
        [1.5696],
        [1.5625],
        [1.5399],
        [1.5143],
        [1.3403],
        [0.5172],
        [1.5737],
        [1.5740],
        [1.5731],
        [1.5650],
        [1.5703],
        [1.5035],
        [1.5679],
        [1.4453],
        [1.3398],
        [0.5034],
        [1.4028],
        [1.0683],
        [1.5337],
        [1.5667],
        [1.5263],
        [1.5594],
        [1.5759],
        [1.5390],
        [1.0427],
        [1.5708],
        [1.5650],
        [1.5635],
        [1.5714],
        [1.5758],
        [1.5757],
        [1.5451],
        [1.5690],
        [1.5423],
        [1.5530],
        [1.5560],
        [1.5615],
        [1.5653],
        [1.5624],
        [1.5398],
        [1.5753],
        [1.5299],
        [1.5657],
        [1.5620],
        [1.5612],
        [1.4904],
        [1.5705],
        [1.4003],
        [1.5646],
        [1.5648],
        [1.5461],
        [1.5621],
        [1.5699],
        [1.5522],
        [1.1110],
        [1.5766],
        [1.5749],
        [1.5225],
        [1.5662],
        [1.5376],
        [1.5742],
        [1.1594],
        [1.1463],
        [0.3969],
        [1.5378],
        [1.5522],
        [1.3281],
        [1.4517],
        [1.5600],
        [1.5395],
        [1.5046],
        [1.4940],
        [1.5033],
        [1.5593],
        [1.3870],
        [1.5709],
        [1.3158],
        [1.5719],
        [1.3335],
        [1.5650],
        [1.4766],
        [0.7748],
        [1.5717],
        [1.5732],
        [1.5434],
        [1.5578],
        [1.4932],
        [1.5403],
        [1.5442],
        [1.4301],
        [1.5577],
        [1.5501],
        [0.9793],
        [1.5555],
        [1.5540],
        [1.5237],
        [1.2369],
        [1.5716],
        [1.3638],
        [1.5725],
        [1.4700],
        [1.5597],
        [1.5007],
        [1.0462],
        [1.4717],
        [1.4584],
        [1.4775],
        [1.5701],
        [1.3905],
        [0.1648],
        [1.4859],
        [1.4722],
        [1.5539],
        [1.4435],
        [1.3466],
        [1.4968],
        [1.4928],
        [1.5476],
        [1.5665],
        [1.1608],
        [1.5751],
        [1.5730],
        [1.5391],
        [1.5645],
        [1.5485],
        [1.5715],
        [1.5691],
        [1.5449],
        [1.4404],
        [1.5722],
        [1.5149],
        [1.5643],
        [1.5124],
        [1.0937],
        [1.0625]], device='cuda:3', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:3')
torch.Size([74])
torch.Size([1])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([305])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1102)
idx:  73
Gate loss:  tensor([[0.5262],
        [0.7324],
        [0.2843],
        [0.9982],
        [1.0524],
        [0.7364],
        [1.1366],
        [2.0616],
        [1.2730],
        [1.4706],
        [2.6479],
        [2.6063],
        [1.8855],
        [2.6760],
        [2.5802],
        [1.9369],
        [2.5754],
        [2.5974],
        [2.6682],
        [2.3561],
        [2.2783],
        [1.1856],
        [1.2068],
        [0.5475],
        [2.6338],
        [2.5358],
        [1.6101],
        [2.0134],
        [2.4811],
        [2.5721],
        [2.5348],
        [2.6505],
        [2.6743],
        [2.1956],
        [2.6836],
        [2.6757],
        [2.6037],
        [2.5451],
        [2.6734],
        [2.6622],
        [2.6799],
        [2.6455],
        [2.6848],
        [2.1814],
        [2.1001],
        [2.6290],
        [2.2679],
        [1.7616],
        [2.4776],
        [2.3656],
        [2.0425],
        [2.6202],
        [2.6382],
        [2.5294],
        [2.3710],
        [2.6380],
        [2.6359],
        [2.6267],
        [2.4519],
        [2.2271],
        [2.2024],
        [2.5318],
        [2.5415],
        [2.2737],
        [2.6046],
        [2.5719],
        [1.7137],
        [1.8182],
        [2.6611],
        [2.6743],
        [2.6622],
        [2.6238],
        [2.5801],
        [2.2837],
        [0.8813],
        [2.6813],
        [2.6819],
        [2.6804],
        [2.6665],
        [2.6757],
        [2.5618],
        [2.6715],
        [2.4626],
        [2.2828],
        [0.8578],
        [2.3902],
        [1.8202],
        [2.6131],
        [2.6694],
        [2.6005],
        [2.6569],
        [2.6852],
        [2.6222],
        [1.7766],
        [2.6764],
        [2.6666],
        [2.6639],
        [2.6775],
        [2.6850],
        [2.6847],
        [2.6327],
        [2.6733],
        [2.6279],
        [2.6461],
        [2.6512],
        [2.6606],
        [2.6671],
        [2.6622],
        [2.6236],
        [2.6841],
        [2.6068],
        [2.6678],
        [2.6614],
        [2.6601],
        [2.5395],
        [2.6759],
        [2.3859],
        [2.6659],
        [2.6662],
        [2.6343],
        [2.6617],
        [2.6748],
        [2.6448],
        [1.8930],
        [2.6863],
        [2.6834],
        [2.5941],
        [2.6686],
        [2.6199],
        [2.6823],
        [1.9755],
        [1.9531],
        [0.6762],
        [2.6202],
        [2.6448],
        [2.2629],
        [2.4735],
        [2.6580],
        [2.6231],
        [2.5636],
        [2.5455],
        [2.5614],
        [2.6569],
        [2.3632],
        [2.6767],
        [2.2420],
        [2.6784],
        [2.2721],
        [2.6666],
        [2.5159],
        [1.3201],
        [2.6780],
        [2.6806],
        [2.6297],
        [2.6543],
        [2.5441],
        [2.6245],
        [2.6310],
        [2.4367],
        [2.6541],
        [2.6412],
        [1.6686],
        [2.6503],
        [2.6478],
        [2.5962],
        [2.1075],
        [2.6778],
        [2.3238],
        [2.6793],
        [2.5047],
        [2.6575],
        [2.5571],
        [1.7825],
        [2.5076],
        [2.4849],
        [2.5175],
        [2.6753],
        [2.3692],
        [0.2808],
        [2.5318],
        [2.5085],
        [2.6477],
        [2.4595],
        [2.2944],
        [2.5503],
        [2.5436],
        [2.6369],
        [2.6690],
        [1.9778],
        [2.6838],
        [2.6802],
        [2.6224],
        [2.6657],
        [2.6384],
        [2.6776],
        [2.6736],
        [2.6323],
        [2.4543],
        [2.6788],
        [2.5811],
        [2.6654],
        [2.5770],
        [1.8636],
        [1.8104]], device='cuda:3', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(302, device='cuda:3')
torch.Size([76])
torch.Size([1])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([305])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8967)
idx:  75
Gate loss:  tensor([[0.7017],
        [0.9768],
        [0.3791],
        [1.3312],
        [1.4036],
        [0.9821],
        [1.5158],
        [2.7494],
        [1.6977],
        [1.9612],
        [3.5314],
        [3.4759],
        [2.5146],
        [3.5688],
        [3.4411],
        [2.5832],
        [3.4347],
        [3.4640],
        [3.5585],
        [3.1422],
        [3.0385],
        [1.5812],
        [1.6094],
        [0.7301],
        [3.5126],
        [3.3819],
        [2.1473],
        [2.6851],
        [3.3090],
        [3.4303],
        [3.3806],
        [3.5349],
        [3.5666],
        [2.9282],
        [3.5789],
        [3.5685],
        [3.4724],
        [3.3943],
        [3.5653],
        [3.5505],
        [3.5741],
        [3.5282],
        [3.5806],
        [2.9092],
        [2.8008],
        [3.5061],
        [3.0247],
        [2.3493],
        [3.3042],
        [3.1549],
        [2.7240],
        [3.4945],
        [3.5184],
        [3.3734],
        [3.1621],
        [3.5182],
        [3.5154],
        [3.5031],
        [3.2700],
        [2.9701],
        [2.9373],
        [3.3765],
        [3.3895],
        [3.0324],
        [3.4736],
        [3.4300],
        [2.2854],
        [2.4248],
        [3.5490],
        [3.5666],
        [3.5505],
        [3.4993],
        [3.4410],
        [3.0456],
        [1.1753],
        [3.5760],
        [3.5767],
        [3.5747],
        [3.5562],
        [3.5684],
        [3.4165],
        [3.5628],
        [3.2843],
        [3.0444],
        [1.1440],
        [3.1877],
        [2.4275],
        [3.4850],
        [3.5601],
        [3.4682],
        [3.5434],
        [3.5811],
        [3.4971],
        [2.3694],
        [3.5694],
        [3.5563],
        [3.5528],
        [3.5709],
        [3.5808],
        [3.5805],
        [3.5111],
        [3.5653],
        [3.5048],
        [3.5290],
        [3.5357],
        [3.5484],
        [3.5570],
        [3.5504],
        [3.4990],
        [3.5797],
        [3.4766],
        [3.5579],
        [3.5494],
        [3.5476],
        [3.3868],
        [3.5688],
        [3.1820],
        [3.5554],
        [3.5558],
        [3.5132],
        [3.5498],
        [3.5673],
        [3.5272],
        [2.5246],
        [3.5827],
        [3.5787],
        [3.4597],
        [3.5590],
        [3.4940],
        [3.5772],
        [2.6347],
        [2.6048],
        [0.9018],
        [3.4944],
        [3.5273],
        [3.0180],
        [3.2988],
        [3.5448],
        [3.4983],
        [3.4189],
        [3.3948],
        [3.4160],
        [3.5434],
        [3.1518],
        [3.5697],
        [2.9900],
        [3.5720],
        [3.0302],
        [3.5563],
        [3.3553],
        [1.7605],
        [3.5715],
        [3.5750],
        [3.5072],
        [3.5399],
        [3.3930],
        [3.5002],
        [3.5089],
        [3.2498],
        [3.5396],
        [3.5225],
        [2.2254],
        [3.5346],
        [3.5312],
        [3.4624],
        [2.8107],
        [3.5713],
        [3.0991],
        [3.5733],
        [3.3404],
        [3.5442],
        [3.4102],
        [2.3773],
        [3.3442],
        [3.3141],
        [3.3575],
        [3.5679],
        [3.1597],
        [0.3745],
        [3.3766],
        [3.3455],
        [3.5311],
        [3.2801],
        [3.0600],
        [3.4012],
        [3.3923],
        [3.5168],
        [3.5596],
        [2.6377],
        [3.5792],
        [3.5744],
        [3.4973],
        [3.5551],
        [3.5187],
        [3.5711],
        [3.5656],
        [3.5106],
        [3.2731],
        [3.5727],
        [3.4423],
        [3.5547],
        [3.4368],
        [2.4853],
        [2.4144]], device='cuda:3', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:3')
torch.Size([77])
torch.Size([1])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([305])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 840, in <module>
    fire.Fire(main)
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 822, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 388, in train
    outputs = model(**batch)
              ^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/modeling_llama.py", line 1438, in forward
    nll_signal = torch.tensor([(self.reward_decay ** i+1) * loss for i, loss in enumerate(packed_loss[index][thought['thought_end_index']:thought['end_index']])]).to(device=unreduced_loss.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
