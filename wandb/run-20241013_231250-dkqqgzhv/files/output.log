Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.53s/it]
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
The gate was not present, so initializing it!
/data/data/arrv/env/test/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                           [0m| 0/225 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
The indices are: tensor([  0,   4,   8,  13,  38,  41,  43, 165, 221, 233, 236, 237],
       device='cuda:1')
The indices are: tensor([  0,   4,  10,  23,  24,  25,  41,  42,  72,  73,  76,  78,  79, 113,
        135, 150, 151, 164, 165, 167, 185, 201, 212, 216, 217, 237],
       device='cuda:1')
The indices are: tensor([  0,   4,  15,  19,  21,  40,  47,  48,  51,  63,  86,  87, 117, 194,
        212, 237], device='cuda:1')
The indices are: tensor([  0,   4,   5,   7,   8,  15,  16,  22,  24,  27,  28,  40,  42,  47,
         48,  51,  58,  66,  67,  76,  99, 120, 224, 237], device='cuda:1')
The selected-indices are: [[13, 4, 237], [10, 167, 73], [21, 19, 47], [58, 40, 99]]
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
torch.Size([1, 750])
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  51474 MiB |  64326 MiB | 175736 MiB | 124262 MiB |
|---------------------------------------------------------------------------|
| Active memory         |  51474 MiB |  64326 MiB | 175736 MiB | 124262 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |  51474 MiB |  64326 MiB | 173890 MiB | 122416 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77594 MiB |  77594 MiB |  77596 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16353 KiB | 283994 KiB |  10792 MiB |  10776 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     601    |    1139    |  248866    |  248265    |
|---------------------------------------------------------------------------|
| Active allocs         |     601    |    1139    |  248866    |  248265    |
|---------------------------------------------------------------------------|
| GPU reserved segments |    1268    |    1268    |    1269    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      17    |     198    |  108940    |  108923    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Training Epoch: 1/1, step 0/225 completed (loss: 1.1005605459213257):   0%|[34mâ–Ž                                                                             [0m| 1/225 [00:20<1:17:39, 20.80s/it][0m

The indices are: tensor([  1,   2,   3,   4,   8,  11,  16,  19,  20,  22,  25,  31,  41,  49,
         50,  52,  64,  65,  67,  70,  74,  75,  83,  88,  89,  92, 103, 108,
        109, 113, 114, 115, 116, 122, 127, 136, 137, 144, 151, 152, 174, 175,
        200, 203, 206, 207, 219, 223, 230, 231, 245, 254, 274, 281, 282, 306,
        314, 315, 342, 356, 357, 364, 388, 389], device='cuda:1')
The indices are: tensor([  1,   2,   3,   4,   6,   9,  10,  12,  13,  17,  18,  20,  24,  34,
         37,  38,  39,  42,  43,  44,  51,  59,  67,  69,  70,  75,  76,  84,
         87,  90,  91,  93,  97,  98, 102, 111, 115, 116, 117, 118, 176, 177,
        188, 196, 208, 212, 230, 231, 233, 241, 246, 251, 252, 254, 255, 256,
        257, 273, 274, 276, 278, 287, 295, 308, 374, 375, 376, 377, 378, 388,
        389], device='cuda:1')
The indices are: tensor([  1,   2,   3,   4,   8,  13,  20,  21,  26,  41,  44,  55,  72,  73,
         83,  99, 100, 103, 113, 119, 128, 137, 139, 160, 163, 164, 182, 209,
        210, 229, 238, 268, 274, 388, 389], device='cuda:1')
The indices are: tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  26,  36,  44,  46,  53,
         54,  56,  65,  67,  71,  90, 104, 113, 114, 122, 176, 180, 181, 369,
        386, 389], device='cuda:1')
The selected-indices are: [[254, 274, 3], [12, 97, 212], [100, 274, 268], [2, 36, 181]]
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
torch.Size([1, 1206])
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  51474 MiB |  72399 MiB | 377306 MiB | 325832 MiB |
|---------------------------------------------------------------------------|
| Active memory         |  51474 MiB |  72399 MiB | 377306 MiB | 325832 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |  51474 MiB |  72395 MiB | 375424 MiB | 323950 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  81186 MiB |  81186 MiB |  81188 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6098 KiB |   3772 MiB | 132631 MiB | 132625 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     601    |    1575    |  497342    |  496741    |
|---------------------------------------------------------------------------|
| Active allocs         |     601    |    1575    |  497342    |  496741    |
|---------------------------------------------------------------------------|
| GPU reserved segments |    1392    |    1392    |    1393    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      13    |     483    |  223937    |  223924    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

The indices are: tensor([  0,   1,   2,   3,   4,  19,  26,  29,  30,  36,  37,  43,  44,  45,
         47,  48,  51,  61,  63,  64,  65,  67,  79,  81,  84,  86,  88,  90,
         91, 103, 108, 110, 113, 127, 135, 136, 141, 149, 154, 170, 182, 183,
        191], device='cuda:1')
The indices are: tensor([  0,   1,   2,   3,   4,   6,   7,   8,  16,  19,  23,  24,  25,  26,
         27,  29,  31,  33,  36,  38,  39,  46,  47,  48,  51,  53,  54,  55,
         61,  63,  65,  69,  74,  75,  76,  78,  80,  81,  85,  86,  91,  94,
         95,  96, 104, 107, 110, 113, 114, 115, 116, 124, 127, 134, 136, 137,
        151, 152, 154, 155, 159, 166, 170, 172, 173, 180, 188, 191],
       device='cuda:1')
The indices are: tensor([  0,   1,   2,   3,   4,  10,  11,  12,  14,  15,  24,  26,  30,  35,
         38,  41,  42,  44,  49,  50,  51,  52,  53,  54,  61,  69,  70,  82,
         83,  87,  88,  92,  94, 101, 106, 129, 130, 133, 140, 141, 154, 156,
        158, 162, 171, 172, 176, 191], device='cuda:1')
The indices are: tensor([  0,   1,   2,   3,   4,   7,  10,  11,  19,  20,  26,  39,  43,  48,
         49,  50,  53,  55,  56,  72,  86,  88,  93,  94,  97, 102, 121, 122,
        128, 139, 140, 143, 181, 191], device='cuda:1')
The selected-indices are: [[103, 88, 64], [91, 51, 38], [141, 171, 106], [140, 11, 50]]
torch.Size([1, 612])
torch.Size([1, 612])
torch.Size([1, 612])
torch.Size([1, 612])
torch.Size([1, 612])
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 859, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 841, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 391, in train
    outputs = think_tuner_step(batch, model=model, tokenizer=tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 304, in think_tuner_step
    total_gate_loss, total_reinforce_loss, total_nll_thought, logy = start_thinking(batch["input_ids"], outputs.last_hidden_state, logits,  batch["labels"], unreduced_loss, model, tokenizer)
                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 228, in start_thinking
    new_greedy_sequence_decoding = model.generate(**batch, max_new_tokens=10, do_sample=True, temperature=1.0 ,use_cache= True, top_p=1.0)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/generation/utils.py", line 2024, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/generation/utils.py", line 2982, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 364, in pre_forward
    return send_to_device(args, self.execution_device), send_to_device(
                                                        ^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/utils/operations.py", line 185, in send_to_device
    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/utils/operations.py", line 175, in send_to_device
    return honor_type(
           ^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/utils/operations.py", line 82, in honor_type
    return type(obj)(generator)
           ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/utils/operations.py", line 176, in <genexpr>
    tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/utils/operations.py", line 156, in send_to_device
    return tensor.to(device, non_blocking=non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
