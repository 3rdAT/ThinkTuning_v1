Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.97s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                    [0m| 0/225 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/modeling_llama.py:1373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
Preparing 4D causal attention mask with cache position
torch.Size([772])
tensor(1.0493, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 194])
gate shape:  torch.Size([4, 194])
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([78])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6537)
idx:  77
Gate loss:  tensor(0.6537, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6471)
idx:  89
Gate loss:  tensor(1.3008, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6938)
idx:  99
Gate loss:  tensor(1.9946, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3896)
idx:  67
Gate loss:  tensor(2.3842, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9768)
idx:  135
Gate loss:  tensor(3.3610, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-35.3472)
idx:  192
Gate loss:  tensor(38.7082, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3486)
idx:  22
Gate loss:  tensor(39.0568, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8385)
idx:  102
Gate loss:  tensor(39.8952, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6945)
idx:  104
Gate loss:  tensor(40.5897, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4912, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3934)
idx:  46
Gate loss:  tensor(40.9831, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6110, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9672)
idx:  150
Gate loss:  tensor(41.9503, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(326, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.0765)
idx:  189
Gate loss:  tensor(59.0268, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Training Epoch: 1/1, step 0/225 completed (loss: 1.0493242740631104):   0%|[34m                       [0m| 1/225 [00:18<1:10:22, 18.85s/it][0m
Preparing 4D causal attention mask with cache position
torch.Size([1676])
tensor(6.5173, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 420])
gate shape:  torch.Size([4, 420])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1128, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3729)
idx:  48
Gate loss:  tensor(0.3729, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24679, device='cuda:1')
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0138)
idx:  266
Gate loss:  tensor(2.3866, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1321, device='cuda:1')
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1785)
idx:  331
Gate loss:  tensor(5.5651, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4038, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6291)
idx:  81
Gate loss:  tensor(6.1942, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16934, device='cuda:1')
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7706)
idx:  293
Gate loss:  tensor(7.9648, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(607, device='cuda:1')
torch.Size([388])
torch.Size([1])
torch.Size([388])
Inside custom generate sample func
torch.Size([388])
torch.Size([1])
torch.Size([388])
Inside custom generate sample func
torch.Size([388])
torch.Size([1])
torch.Size([388])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.7341)
idx:  387
Gate loss:  tensor(13.6988, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9560)
idx:  228
Gate loss:  tensor(15.6548, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1644, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8537)
idx:  260
Gate loss:  tensor(17.5085, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0875)
idx:  329
Gate loss:  tensor(21.5960, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(16934, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6606)
idx:  67
Gate loss:  tensor(22.2566, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9646)
idx:  268
Gate loss:  tensor(24.2212, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
reasoning_path shape:  torch.Size([431])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.6805)
idx:  355
Gate loss:  tensor(28.9017, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1020])
tensor(1.4563, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 256])
gate shape:  torch.Size([4, 256])
The shape of hidden_states: torch.Size([4096])
The topk tensor(686, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6023)
idx:  26
Gate loss:  tensor(0.6023, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1303, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8028)
idx:  163
Gate loss:  tensor(1.4051, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7077)
idx:  190
Gate loss:  tensor(3.1128, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6819)
idx:  111
Gate loss:  tensor(3.7946, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0268)
idx:  180
Gate loss:  tensor(4.8214, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0622)
idx:  234
Gate loss:  tensor(8.8836, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(910, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6358)
idx:  129
Gate loss:  tensor(9.5195, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1982)
idx:  150
Gate loss:  tensor(10.7177, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3155)
idx:  159
Gate loss:  tensor(12.0332, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(510, device='cuda:1')
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1688)
idx:  3
Gate loss:  tensor(12.2020, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(342, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([267])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3712)
idx:  93
Gate loss:  tensor(12.5732, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 2
Preparing 4D causal attention mask with cache position
torch.Size([668])
tensor(1.6861, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 168])
gate shape:  torch.Size([4, 168])
The shape of hidden_states: torch.Size([4096])
The topk tensor(505, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5406)
idx:  28
Gate loss:  tensor(0.5406, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(567, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0850)
idx:  92
Gate loss:  tensor(1.6257, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5101, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9764)
idx:  98
Gate loss:  tensor(2.6021, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4468)
idx:  54
Gate loss:  tensor(3.0489, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4426)
idx:  68
Gate loss:  tensor(3.4915, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6920)
idx:  74
Gate loss:  tensor(4.1836, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9407)
idx:  105
Gate loss:  tensor(5.1242, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1037)
idx:  117
Gate loss:  tensor(6.2279, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.3796)
idx:  160
Gate loss:  tensor(11.6075, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5032)
idx:  39
Gate loss:  tensor(12.1107, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4321)
idx:  54
Gate loss:  tensor(12.5428, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2719, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([179])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4977)
idx:  131
Gate loss:  tensor(14.0405, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([788])
tensor(1.4892, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 198])
gate shape:  torch.Size([4, 198])
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5656)
idx:  41
Gate loss:  tensor(0.5656, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8394, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.5011)
idx:  185
Gate loss:  tensor(11.0668, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.9570)
idx:  187
Gate loss:  tensor(23.0238, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2277, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4356)
idx:  40
Gate loss:  tensor(23.4595, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7386)
idx:  71
Gate loss:  tensor(24.1980, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9287)
idx:  88
Gate loss:  tensor(25.1267, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3834)
idx:  133
Gate loss:  tensor(26.5101, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3515)
idx:  171
Gate loss:  tensor(28.8616, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0193)
idx:  172
Gate loss:  tensor(32.8809, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3324)
idx:  34
Gate loss:  tensor(33.2133, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5440)
idx:  116
Gate loss:  tensor(33.7573, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([205])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-25.3184)
idx:  196
Gate loss:  tensor(59.0757, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1104])
tensor(1.3252, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 277])
gate shape:  torch.Size([4, 277])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3546)
idx:  67
Gate loss:  tensor(0.3546, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7832)
idx:  163
Gate loss:  tensor(1.1378, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.0274)
idx:  271
Gate loss:  tensor(18.1652, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3145)
idx:  8
Gate loss:  tensor(18.4797, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1179)
idx:  190
Gate loss:  tensor(19.5975, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29882, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.1145)
idx:  260
Gate loss:  tensor(24.7120, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1868)
idx:  66
Gate loss:  tensor(24.8988, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2196)
idx:  232
Gate loss:  tensor(26.1185, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0743)
idx:  239
Gate loss:  tensor(27.1927, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3363)
idx:  44
Gate loss:  tensor(27.5290, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11015, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5320)
idx:  133
Gate loss:  tensor(28.0610, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([288])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0173)
idx:  221
Gate loss:  tensor(29.0783, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([968])
tensor(1.2139, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 243])
gate shape:  torch.Size([4, 243])
The shape of hidden_states: torch.Size([4096])
The topk tensor(4395, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3033)
idx:  27
Gate loss:  tensor(0.3033, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(583, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2686)
idx:  29
Gate loss:  tensor(0.5719, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3960)
idx:  82
Gate loss:  tensor(0.9679, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2906)
idx:  35
Gate loss:  tensor(1.2585, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12070, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4383)
idx:  100
Gate loss:  tensor(1.6968, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0839)
idx:  168
Gate loss:  tensor(2.7807, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2961)
idx:  73
Gate loss:  tensor(3.0768, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5265)
idx:  169
Gate loss:  tensor(3.6033, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1980)
idx:  180
Gate loss:  tensor(4.8013, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3043)
idx:  27
Gate loss:  tensor(5.1056, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3459)
idx:  34
Gate loss:  tensor(5.4515, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5204)
idx:  144
Gate loss:  tensor(5.9719, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([824])
tensor(1.3904, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 207])
gate shape:  torch.Size([4, 207])
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9080)
idx:  73
Gate loss:  tensor(0.9080, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9677)
idx:  91
Gate loss:  tensor(1.8756, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4396)
idx:  157
Gate loss:  tensor(4.3152, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7698)
idx:  48
Gate loss:  tensor(5.0851, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3800, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2511)
idx:  116
Gate loss:  tensor(6.3361, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8396)
idx:  148
Gate loss:  tensor(8.1757, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3403)
idx:  12
Gate loss:  tensor(8.5160, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5186, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4468)
idx:  30
Gate loss:  tensor(8.9629, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0362)
idx:  101
Gate loss:  tensor(9.9991, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2182)
idx:  3
Gate loss:  tensor(10.2174, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13791, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4275)
idx:  79
Gate loss:  tensor(10.6449, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2618, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([218])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4285)
idx:  142
Gate loss:  tensor(12.0734, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([972])
tensor(1.2947, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 244])
gate shape:  torch.Size([4, 244])
The shape of hidden_states: torch.Size([4096])
The topk tensor(3367, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3848)
idx:  82
Gate loss:  tensor(0.3848, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2707, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5801)
idx:  90
Gate loss:  tensor(0.9649, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8752)
idx:  166
Gate loss:  tensor(1.8400, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6295)
idx:  114
Gate loss:  tensor(2.4695, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5543)
idx:  134
Gate loss:  tensor(3.0238, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7321)
idx:  220
Gate loss:  tensor(4.7559, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2815)
idx:  49
Gate loss:  tensor(5.0374, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7287)
idx:  167
Gate loss:  tensor(5.7661, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4167)
idx:  223
Gate loss:  tensor(9.1828, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5614)
idx:  144
Gate loss:  tensor(9.7442, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5160)
idx:  186
Gate loss:  tensor(11.2602, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1208)
idx:  194
Gate loss:  tensor(12.3810, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([764])
tensor(1.0993, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 192])
gate shape:  torch.Size([4, 192])
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5436)
idx:  62
Gate loss:  tensor(0.5436, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6231)
idx:  82
Gate loss:  tensor(1.1667, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6441)
idx:  144
Gate loss:  tensor(2.8107, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4831)
idx:  97
Gate loss:  tensor(3.2939, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1729)
idx:  171
Gate loss:  tensor(6.4668, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([197])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.7662)
idx:  185
Gate loss:  tensor(19.2330, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4246)
idx:  42
Gate loss:  tensor(19.6576, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(366, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6000)
idx:  93
Gate loss:  tensor(20.2576, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1338, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4620)
idx:  133
Gate loss:  tensor(21.7196, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3570)
idx:  45
Gate loss:  tensor(22.0766, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5240, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5045)
idx:  79
Gate loss:  tensor(22.5811, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4761)
idx:  153
Gate loss:  tensor(24.0572, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([680])
tensor(1.0103, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 171])
gate shape:  torch.Size([4, 171])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6355)
idx:  61
Gate loss:  tensor(0.6355, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7347)
idx:  62
Gate loss:  tensor(1.3702, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0248)
idx:  94
Gate loss:  tensor(2.3950, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6108)
idx:  36
Gate loss:  tensor(3.0058, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8550)
idx:  74
Gate loss:  tensor(3.8608, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2440, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1758)
idx:  145
Gate loss:  tensor(6.0366, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5252)
idx:  7
Gate loss:  tensor(6.5618, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1105, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0355)
idx:  140
Gate loss:  tensor(8.5973, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-23.9612)
idx:  169
Gate loss:  tensor(32.5584, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2999, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8190)
idx:  105
Gate loss:  tensor(33.3774, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3105)
idx:  128
Gate loss:  tensor(34.6879, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([182])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2582)
idx:  144
Gate loss:  tensor(36.9461, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1404])
tensor(1.2912, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 352])
gate shape:  torch.Size([4, 352])
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2018)
idx:  3
Gate loss:  tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1608, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2647)
idx:  32
Gate loss:  tensor(0.4664, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3893)
idx:  115
Gate loss:  tensor(0.8557, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1757)
idx:  13
Gate loss:  tensor(1.0314, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2873)
idx:  113
Gate loss:  tensor(1.3187, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3451)
idx:  227
Gate loss:  tensor(1.6638, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1668)
idx:  11
Gate loss:  tensor(1.8305, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2454)
idx:  100
Gate loss:  tensor(2.0759, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2828)
idx:  145
Gate loss:  tensor(2.3587, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1644)
idx:  87
Gate loss:  tensor(2.5231, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4730)
idx:  298
Gate loss:  tensor(3.9961, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
reasoning_path shape:  torch.Size([363])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0640)
idx:  303
Gate loss:  tensor(5.0601, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1008])
tensor(1.1546, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 253])
gate shape:  torch.Size([4, 253])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3859)
idx:  25
Gate loss:  tensor(0.3859, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7402)
idx:  157
Gate loss:  tensor(1.1261, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6398)
idx:  214
Gate loss:  tensor(2.7659, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2149)
idx:  51
Gate loss:  tensor(2.9807, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29905, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3311)
idx:  97
Gate loss:  tensor(3.3118, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7836)
idx:  161
Gate loss:  tensor(4.0954, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2113)
idx:  3
Gate loss:  tensor(4.3067, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2019)
idx:  24
Gate loss:  tensor(4.5087, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4953)
idx:  151
Gate loss:  tensor(5.0040, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2637)
idx:  29
Gate loss:  tensor(5.2676, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4796, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2417)
idx:  31
Gate loss:  tensor(5.5093, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4796, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2633)
idx:  227
Gate loss:  tensor(8.7726, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1100])
tensor(1.1159, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 276])
gate shape:  torch.Size([4, 276])
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3555)
idx:  18
Gate loss:  tensor(0.3555, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(470, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8572)
idx:  144
Gate loss:  tensor(1.2127, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8249)
idx:  255
Gate loss:  tensor(6.0376, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4631)
idx:  95
Gate loss:  tensor(6.5008, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6282)
idx:  120
Gate loss:  tensor(7.1290, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5097)
idx:  169
Gate loss:  tensor(7.6387, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2346)
idx:  2
Gate loss:  tensor(7.8733, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4094, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3420)
idx:  45
Gate loss:  tensor(8.2153, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1933, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4209)
idx:  123
Gate loss:  tensor(8.6362, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29893, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2429)
idx:  20
Gate loss:  tensor(8.8791, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(21266, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3521)
idx:  131
Gate loss:  tensor(9.2312, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(354, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6736)
idx:  233
Gate loss:  tensor(10.9048, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([612])
tensor(1.1003, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 154])
gate shape:  torch.Size([4, 154])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4526)
idx:  46
Gate loss:  tensor(0.4526, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5600)
idx:  83
Gate loss:  tensor(1.0126, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6690)
idx:  102
Gate loss:  tensor(2.6816, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(11872, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4016)
idx:  15
Gate loss:  tensor(3.0832, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7821)
idx:  105
Gate loss:  tensor(3.8653, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0006)
idx:  87
Gate loss:  tensor(4.8659, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2534)
idx:  113
Gate loss:  tensor(7.1193, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8397)
idx:  136
Gate loss:  tensor(9.9590, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5224)
idx:  22
Gate loss:  tensor(10.4814, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6763)
idx:  39
Gate loss:  tensor(11.1578, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7579, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([165])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3957)
idx:  52
Gate loss:  tensor(11.5534, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([956])
tensor(1.0625, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 240])
gate shape:  torch.Size([4, 240])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3640)
idx:  27
Gate loss:  tensor(0.3640, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3653)
idx:  75
Gate loss:  tensor(0.7292, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4293)
idx:  70
Gate loss:  tensor(1.1585, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6994)
idx:  147
Gate loss:  tensor(1.8579, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29956, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0603)
idx:  174
Gate loss:  tensor(2.9182, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2841)
idx:  18
Gate loss:  tensor(3.2023, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(454, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5582)
idx:  158
Gate loss:  tensor(3.7606, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6295, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4226)
idx:  221
Gate loss:  tensor(6.1832, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3198)
idx:  53
Gate loss:  tensor(6.5030, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19995, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9633)
idx:  197
Gate loss:  tensor(7.4664, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7637)
idx:  211
Gate loss:  tensor(9.2301, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([1144])
tensor(1.0406, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 287])
gate shape:  torch.Size([4, 287])
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2905)
idx:  59
Gate loss:  tensor(0.2905, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7586, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9227)
idx:  222
Gate loss:  tensor(1.2131, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.7807)
idx:  278
Gate loss:  tensor(7.9939, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2839)
idx:  100
Gate loss:  tensor(8.2778, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3753)
idx:  164
Gate loss:  tensor(8.6531, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11536, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8619)
idx:  237
Gate loss:  tensor(9.5150, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4801)
idx:  145
Gate loss:  tensor(9.9950, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0548)
idx:  223
Gate loss:  tensor(11.0498, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([297])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8684)
idx:  260
Gate loss:  tensor(13.9182, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(569, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2269)
idx:  35
Gate loss:  tensor(14.1451, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2578)
idx:  53
Gate loss:  tensor(14.4030, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(286, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3880)
idx:  212
Gate loss:  tensor(14.7909, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([792])
tensor(1.0727, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 199])
gate shape:  torch.Size([4, 199])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6745)
idx:  31
Gate loss:  tensor(0.6745, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3588, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4987)
idx:  106
Gate loss:  tensor(1.1732, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 2
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5522)
idx:  116
Gate loss:  tensor(1.7254, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1269, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7146)
idx:  125
Gate loss:  tensor(2.4401, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26807, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5146)
idx:  151
Gate loss:  tensor(3.9546, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3466)
idx:  20
Gate loss:  tensor(4.3012, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3063, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3489)
idx:  42
Gate loss:  tensor(4.6501, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4947)
idx:  64
Gate loss:  tensor(5.1448, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2679, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4728)
idx:  6
Gate loss:  tensor(5.6176, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3000)
idx:  29
Gate loss:  tensor(5.9176, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([210])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8788)
idx:  160
Gate loss:  tensor(6.7964, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([288])
tensor(1.3090, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 73])
gate shape:  torch.Size([4, 73])
The shape of hidden_states: torch.Size([4096])
The topk tensor(8341, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6339)
idx:  29
Gate loss:  tensor(2.6339, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6293, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0075)
idx:  36
Gate loss:  tensor(5.6414, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.0223)
idx:  65
Gate loss:  tensor(19.6637, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2144)
idx:  43
Gate loss:  tensor(21.8780, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.7839)
idx:  63
Gate loss:  tensor(28.6620, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([81])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.5167)
idx:  65
Gate loss:  tensor(39.1787, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3292)
idx:  27
Gate loss:  tensor(40.5079, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3589)
idx:  31
Gate loss:  tensor(41.8668, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(542, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.1113)
idx:  66
Gate loss:  tensor(51.9782, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9255)
idx:  24
Gate loss:  tensor(52.9036, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0891)
idx:  45
Gate loss:  tensor(54.9927, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1473, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([84])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.6333)
idx:  63
Gate loss:  tensor(58.6259, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([1220])
tensor(0.9923, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 306])
gate shape:  torch.Size([4, 306])
The shape of hidden_states: torch.Size([4096])
The topk tensor(902, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2315)
idx:  22
Gate loss:  tensor(0.2315, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3078)
idx:  90
Gate loss:  tensor(0.5393, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(749, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8856)
idx:  241
Gate loss:  tensor(1.4249, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4426)
idx:  147
Gate loss:  tensor(1.8675, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5852)
idx:  149
Gate loss:  tensor(2.4527, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6085)
idx:  163
Gate loss:  tensor(3.0612, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(5227, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1954)
idx:  70
Gate loss:  tensor(3.2566, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4359)
idx:  101
Gate loss:  tensor(3.6925, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3295)
idx:  117
Gate loss:  tensor(4.0220, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([313])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8598)
idx:  141
Gate loss:  tensor(4.8818, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4557, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4863)
idx:  229
Gate loss:  tensor(5.3681, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
reasoning_path shape:  torch.Size([314])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.9069)
idx:  297
Gate loss:  tensor(15.2751, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([604])
tensor(1.2091, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 152])
gate shape:  torch.Size([4, 152])
The shape of hidden_states: torch.Size([4096])
The topk tensor(12061, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6491)
idx:  11
Gate loss:  tensor(0.6491, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8853)
idx:  50
Gate loss:  tensor(1.5344, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3522)
idx:  133
Gate loss:  tensor(5.8866, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7028)
idx:  57
Gate loss:  tensor(6.5894, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0744)
idx:  128
Gate loss:  tensor(8.6638, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4176)
idx:  23
Gate loss:  tensor(9.0815, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(325, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([158])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5238)
idx:  101
Gate loss:  tensor(10.6053, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3281)
idx:  128
Gate loss:  tensor(12.9334, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(16701, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4590)
idx:  15
Gate loss:  tensor(13.3924, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3675)
idx:  35
Gate loss:  tensor(13.7599, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([163])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5651)
idx:  53
Gate loss:  tensor(14.3250, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([620])
tensor(0.9609, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 156])
gate shape:  torch.Size([4, 156])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7501)
idx:  70
Gate loss:  tensor(0.7501, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0836)
idx:  113
Gate loss:  tensor(1.8338, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2425)
idx:  124
Gate loss:  tensor(3.0762, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3713)
idx:  43
Gate loss:  tensor(3.4475, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7609)
idx:  81
Gate loss:  tensor(4.2084, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8691)
idx:  90
Gate loss:  tensor(5.0775, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3478)
idx:  3
Gate loss:  tensor(5.4253, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4323)
idx:  43
Gate loss:  tensor(5.8576, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([162])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.4273)
idx:  150
Gate loss:  tensor(17.2849, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4672)
idx:  54
Gate loss:  tensor(17.7521, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6295, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([167])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5615)
idx:  137
Gate loss:  tensor(19.3135, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 2
Preparing 4D causal attention mask with cache position
torch.Size([1148])
tensor(0.9639, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 288])
gate shape:  torch.Size([4, 288])
The shape of hidden_states: torch.Size([4096])
The topk tensor(2744, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2155)
idx:  79
Gate loss:  tensor(0.2155, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3980)
idx:  150
Gate loss:  tensor(0.6135, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6230)
idx:  155
Gate loss:  tensor(1.2365, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(6743, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2623)
idx:  79
Gate loss:  tensor(1.4988, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1716)
idx:  243
Gate loss:  tensor(2.6705, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1790)
idx:  265
Gate loss:  tensor(4.8495, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2699, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2252)
idx:  37
Gate loss:  tensor(5.0748, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4208, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2974)
idx:  135
Gate loss:  tensor(5.3722, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13105, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5715)
idx:  199
Gate loss:  tensor(5.9436, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3078)
idx:  107
Gate loss:  tensor(6.2515, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3232)
idx:  126
Gate loss:  tensor(6.5747, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3881, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([299])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6322)
idx:  172
Gate loss:  tensor(7.2069, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1312])
tensor(0.8843, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 329])
gate shape:  torch.Size([4, 329])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1644, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3052)
idx:  14
Gate loss:  tensor(0.3052, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(326, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2683)
idx:  34
Gate loss:  tensor(0.5735, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-15.0685)
idx:  324
Gate loss:  tensor(15.6420, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3600)
idx:  99
Gate loss:  tensor(16.0020, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3751)
idx:  161
Gate loss:  tensor(16.3770, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.6679)
idx:  312
Gate loss:  tensor(22.0450, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2277, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1555)
idx:  45
Gate loss:  tensor(22.2005, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(470, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2674)
idx:  57
Gate loss:  tensor(22.4679, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1438, device='cuda:1')
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5169)
idx:  281
Gate loss:  tensor(23.9848, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13799, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2592)
idx:  84
Gate loss:  tensor(24.2440, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([340])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4034)
idx:  108
Gate loss:  tensor(24.6474, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
reasoning_path shape:  torch.Size([334])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.1318)
idx:  326
Gate loss:  tensor(40.7792, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1092])
tensor(1.1045, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 274])
gate shape:  torch.Size([4, 274])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2441)
idx:  73
Gate loss:  tensor(0.2441, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5144)
idx:  159
Gate loss:  tensor(0.7586, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5983)
idx:  182
Gate loss:  tensor(1.3569, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5769)
idx:  136
Gate loss:  tensor(1.9338, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5299)
idx:  155
Gate loss:  tensor(2.4637, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8518)
idx:  197
Gate loss:  tensor(3.3155, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2313)
idx:  2
Gate loss:  tensor(3.5468, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2598)
idx:  41
Gate loss:  tensor(3.8066, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6453, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3286)
idx:  78
Gate loss:  tensor(4.1352, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1478, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2907)
idx:  25
Gate loss:  tensor(4.4259, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(607, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2322)
idx:  77
Gate loss:  tensor(4.6582, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
reasoning_path shape:  torch.Size([285])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8615)
idx:  247
Gate loss:  tensor(5.5196, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1748])
tensor(1.0056, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 438])
gate shape:  torch.Size([4, 438])
The shape of hidden_states: torch.Size([4096])
The topk tensor(2277, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2740)
idx:  104
Gate loss:  tensor(0.2740, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5305)
idx:  181
Gate loss:  tensor(0.8045, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(674, device='cuda:1')
torch.Size([413])
torch.Size([1])
torch.Size([413])
Inside custom generate sample func
torch.Size([413])
torch.Size([1])
torch.Size([413])
Inside custom generate sample func
torch.Size([413])
torch.Size([1])
torch.Size([413])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8799)
idx:  412
Gate loss:  tensor(5.6844, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(10693, device='cuda:1')
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9534)
idx:  353
Gate loss:  tensor(6.6379, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29903, device='cuda:1')
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2143)
idx:  377
Gate loss:  tensor(7.8522, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([399])
torch.Size([1])
torch.Size([399])
Inside custom generate sample func
torch.Size([399])
torch.Size([1])
torch.Size([399])
Inside custom generate sample func
torch.Size([399])
torch.Size([1])
torch.Size([399])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7091)
idx:  398
Gate loss:  tensor(9.5613, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(366, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2417)
idx:  43
Gate loss:  tensor(9.8030, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3215)
idx:  207
Gate loss:  tensor(10.1245, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([379])
torch.Size([1])
torch.Size([379])
Inside custom generate sample func
torch.Size([379])
torch.Size([1])
torch.Size([379])
Inside custom generate sample func
torch.Size([379])
torch.Size([1])
torch.Size([379])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8083)
idx:  378
Gate loss:  tensor(11.9328, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1838)
idx:  193
Gate loss:  tensor(12.1166, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2566)
idx:  225
Gate loss:  tensor(12.3731, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([449])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3580)
idx:  241
Gate loss:  tensor(12.7312, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([780])
tensor(0.9518, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 196])
gate shape:  torch.Size([4, 196])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3321)
idx:  43
Gate loss:  tensor(0.3321, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1108, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5739)
idx:  64
Gate loss:  tensor(0.9060, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6834)
idx:  140
Gate loss:  tensor(2.5894, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3838)
idx:  60
Gate loss:  tensor(2.9732, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7346)
idx:  107
Gate loss:  tensor(3.7078, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7267)
idx:  139
Gate loss:  tensor(4.4345, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2738)
idx:  19
Gate loss:  tensor(4.7083, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6466, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2227)
idx:  37
Gate loss:  tensor(4.9310, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3967)
idx:  55
Gate loss:  tensor(5.3277, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4764)
idx:  59
Gate loss:  tensor(5.8040, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5649)
idx:  63
Gate loss:  tensor(6.3690, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([207])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5945)
idx:  143
Gate loss:  tensor(6.9635, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([808])
tensor(1.1118, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 203])
gate shape:  torch.Size([4, 203])
The shape of hidden_states: torch.Size([4096])
The topk tensor(451, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1892)
idx:  143
Gate loss:  tensor(1.1892, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2717)
idx:  144
Gate loss:  tensor(2.4609, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2211, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3846)
idx:  145
Gate loss:  tensor(3.8454, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4438)
idx:  29
Gate loss:  tensor(4.2893, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7665)
idx:  159
Gate loss:  tensor(6.0558, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3676)
idx:  183
Gate loss:  tensor(9.4233, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(10454, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3911)
idx:  82
Gate loss:  tensor(9.8144, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6256)
idx:  169
Gate loss:  tensor(11.4400, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([209])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3037)
idx:  173
Gate loss:  tensor(13.7437, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3333)
idx:  43
Gate loss:  tensor(14.0770, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4077)
idx:  84
Gate loss:  tensor(14.4847, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13235, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([214])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5812)
idx:  142
Gate loss:  tensor(15.0659, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([152])
tensor(1.2026, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 39])
gate shape:  torch.Size([4, 39])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1762, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.3869)
idx:  23
Gate loss:  tensor(6.3869, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(544, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.5447)
idx:  26
Gate loss:  tensor(14.9316, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1608, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.0092)
idx:  27
Gate loss:  tensor(25.9407, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7829)
idx:  10
Gate loss:  tensor(29.7237, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(27280, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.0198)
idx:  25
Gate loss:  tensor(38.7435, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.0869)
idx:  35
Gate loss:  tensor(56.8304, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(835, device='cuda:1')
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0413)
idx:  0
Gate loss:  tensor(58.8717, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.0388)
idx:  27
Gate loss:  tensor(66.9105, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7621, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([49])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.2140)
idx:  28
Gate loss:  tensor(75.1245, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
torch.Size([4])
torch.Size([1])
torch.Size([4])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2830)
idx:  3
Gate loss:  tensor(76.4075, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6862, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3172)
idx:  9
Gate loss:  tensor(78.7247, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([50])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.7630)
idx:  33
Gate loss:  tensor(86.4877, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1348])
tensor(1.1182, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 338])
gate shape:  torch.Size([4, 338])
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1980)
idx:  49
Gate loss:  tensor(0.1980, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2900)
idx:  77
Gate loss:  tensor(0.4879, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3555)
idx:  124
Gate loss:  tensor(0.8434, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4328, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3120)
idx:  117
Gate loss:  tensor(1.1554, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10693, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5513)
idx:  183
Gate loss:  tensor(1.7068, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9159)
idx:  249
Gate loss:  tensor(2.6226, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2699, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2823)
idx:  116
Gate loss:  tensor(2.9049, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6520, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4800)
idx:  179
Gate loss:  tensor(3.3849, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4608)
idx:  189
Gate loss:  tensor(3.8457, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(975, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2711)
idx:  90
Gate loss:  tensor(4.1168, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3377)
idx:  294
Gate loss:  tensor(5.4545, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([4016])
tensor(1.1195, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 1005])
gate shape:  torch.Size([4, 1005])
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([1016])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 847, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 829, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 389, in train
    outputs = model(**batch, tokenizer=tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/modeling_llama.py", line 1407, in forward
    packed_reasoning_path, _, packed_reasoning_path_casual_mask, packed = get_packed_inputs(reasoning_path, max_length=4090, pad_token_id=self.config.eos_token_id, thought_index=thought_index)
                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/pack_input_ids.py", line 127, in get_packed_inputs
    casual_mask = build_batched_causal_mask_from_attention_mask(attention_mask, torch.float).to(input_ids[0].device)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/pack_input_ids.py", line 105, in build_batched_causal_mask_from_attention_mask
    masks_per_j = (k_indices >= j_indices) & (l_indices <= j_indices)  # Shape: [1, seq_len, seq_len, seq_len]
                  ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.45 GiB. GPU 0 has a total capacity of 93.12 GiB of which 18.49 GiB is free. Including non-PyTorch memory, this process has 74.62 GiB memory in use. Of the allocated memory 71.11 GiB is allocated by PyTorch, and 2.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
