Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.72s/it]
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 9000
--> Validation Set Length = 1000
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                [0m| 0/4500 [00:00<?, ?it/s][0m
> [0;32m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py[0m(288)[0;36mstart_thinking[0;34m()[0m
[0;32m    287 [0;31m[0;34m[0m[0m
[0m[0;32m--> 288 [0;31m    [0;32mfor[0m [0mi[0m[0;34m,[0m [0mindices[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mselected_indices[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    289 [0;31m        [0;32mfor[0m [0midx[0m [0;32min[0m [0mindices[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
tensor([[6.2651e+00, 5.0770e+00, 4.4337e+00, 2.8952e+00, 3.0272e+00, 1.3339e+00,
         3.2072e+00, 2.1036e+00, 5.6732e+00, 1.6338e-01, 6.6122e+00, 2.1985e+00,
         3.5838e+00, 2.4795e+00, 1.6721e+00, 7.1178e-01, 7.4313e-02, 1.9560e+00,
         6.9544e-01, 2.0172e+00, 2.4215e+00, 4.4070e+00, 1.1690e-02, 1.3636e+00,
         4.0892e+00, 2.9519e+00, 5.9615e-01, 2.6670e+00, 2.0388e+00, 3.4913e+00,
         1.7541e-01, 4.3771e+00, 2.1685e+00, 9.6750e-01, 1.4360e+00, 4.3753e-01,
         5.1750e-02, 3.6603e+00, 1.3646e-01, 1.4487e-01, 1.4587e+00, 3.1711e+00,
         1.5791e+00, 3.6638e+00, 1.5643e-01, 3.0394e+00, 2.9458e+00, 3.0967e+00,
         1.1848e+00, 3.0756e+00, 1.5530e+00, 2.8099e+00, 1.2597e+00, 7.3260e-01,
         4.5646e-01, 2.0499e+00, 2.6711e+00, 2.4169e+00, 1.3795e+00, 7.8474e-01,
         1.7587e+00, 3.0499e+00, 3.0986e+00, 4.4493e-01, 6.4524e-01, 4.3940e+00,
         4.2037e+00, 1.7396e+00, 2.3667e+00, 1.6701e+00, 2.5918e-01, 2.6035e+00,
         2.3565e+00, 1.4240e+00, 5.8918e-01, 7.2590e-01, 3.7801e-02, 2.8024e-01,
         1.8621e-01, 7.8967e-02, 3.5114e-03, 1.6237e-01, 9.0226e-01, 5.3538e+00,
         9.3395e-01, 3.2152e+00, 2.5020e+00, 3.4677e-01, 5.5726e-01, 4.1605e-02,
         1.2317e-02, 2.5702e-02, 1.5492e+00, 3.2916e+00, 2.7331e+00, 3.9574e+00,
         2.7736e+00, 3.6763e+00, 3.6004e+00, 2.2902e+00, 2.5290e+00, 1.7211e+00,
         2.8178e-01, 2.0265e+00, 1.9616e+00, 2.0189e-01, 2.7148e+00, 1.5654e+00,
         6.5115e-01, 3.2991e+00, 4.9716e-01, 1.7104e+00, 1.8473e+00, 1.7823e+00,
         9.4819e-01, 6.4214e-01, 4.0271e-01, 7.8341e-02, 1.5406e-01, 1.0290e-01,
         3.1917e-02, 5.4542e-01, 8.0862e-01, 8.1633e-01, 4.4765e-01, 1.5222e-01,
         7.8674e-01, 7.0187e-01, 1.5755e+00, 9.0335e-01, 3.2282e-02, 5.0857e-02,
         6.0011e-01, 2.2655e-01, 1.4271e-01, 4.1894e-01, 1.2798e+00, 2.1525e+00,
         2.9358e+00, 3.2728e+00, 7.9637e-01, 2.7924e+00, 8.3974e-01, 2.3957e-03,
         2.8096e+00, 7.1084e-01, 8.0478e-01, 1.8359e-01, 2.0003e-02, 1.8449e+00,
         7.4003e-01, 1.1569e+00, 2.0225e+00, 1.3270e-01, 9.0073e-01, 1.4329e+00,
         2.3097e+00, 6.0724e-01, 6.7335e-02, 6.6054e-01, 2.1421e-01, 5.4796e-01,
         4.7693e-02, 5.8083e-01, 2.5325e-01, 9.8586e-01, 9.8688e-01, 6.4786e-01,
         6.8990e-02, 1.5509e-02, 3.1702e-02, 5.4499e-02, 5.8383e-01, 1.1701e+00,
         1.3482e-01, 1.5115e+00, 5.1889e-02, 3.3053e-02, 7.6764e-01, 5.2370e-01,
         6.4120e-01, 9.3876e-01, 4.5189e-02, 4.0134e-02, 9.2721e-01, 1.0619e-01,
         1.6509e-02, 9.5935e-02, 1.3974e+00, 2.0943e-01, 5.6499e-01, 1.4116e+00,
         3.2213e+00, 8.0079e-01, 2.5356e+00, 1.7319e-02, 3.3131e-01, 2.3233e-02,
         1.3768e-03, 9.7968e-02, 1.8147e-02, 2.3989e-01, 1.2506e-02, 4.8872e-03,
         3.6175e-02, 1.4511e-02, 4.7271e-02, 1.4450e-01, 7.2526e-03, 6.6747e-01,
         4.4479e-01, 1.1462e+00, 6.3257e-02, 1.6807e-02, 9.6797e-02, 1.3395e-01,
         2.6604e-02, 6.1508e-03, 9.7722e-02, 8.4947e-02, 4.0284e-02, 2.7049e-02,
         1.3109e-02, 7.0623e-03, 6.8406e-03, 2.5879e-03, 1.9220e-02, 4.6278e-02,
         1.2286e-01, 3.5393e-02, 2.0366e-01, 4.9215e-03, 1.3224e-02, 1.0408e-01,
         3.7810e-02, 6.8069e-02, 1.6409e-01, 1.1154e-02, 1.0686e-02, 6.0723e-02,
         1.0820e-02, 1.0611e-02, 2.1055e-02, 1.1505e-01, 1.0570e-02, 1.2012e-01,
         3.1564e-01, 3.0809e-01, 1.7868e-01, 2.7979e+00, 1.0732e+00, 1.3092e+00,
         5.0585e-01, 3.6209e-02, 4.2233e-01, 2.1880e-02, 6.6865e-03, 8.2816e-01,
         2.4508e-01, 1.6776e-01, 8.4871e-02, 1.7050e-01, 1.7417e-03, 3.8669e-02,
         2.1646e-02, 6.5109e-03, 6.1485e-04, 3.1873e-02, 9.3309e-01, 1.1979e+00,
         1.4494e+00, 1.0786e+00, 1.5596e+00, 1.9429e-02, 1.5903e-01, 1.8219e+00,
         8.8001e-01, 4.6803e-02, 6.4416e-02, 4.2281e-03, 1.3909e-01, 2.5197e-02,
         1.0400e+00, 1.0572e+00, 6.3493e-01, 2.0618e+00, 1.2835e+00, 2.0668e+00,
         4.5865e-01, 1.7047e-01, 2.8061e+00, 2.3945e+00, 9.7968e-03, 4.3799e-01,
         2.8750e-01, 1.9288e+00, 1.3081e-01, 7.7630e-02, 6.4621e-02, 8.8270e-02,
         2.3892e-02, 2.1309e-02, 4.8245e-02, 3.3568e-03, 5.1736e-02, 2.0948e-01,
         6.1820e-01, 2.3720e-01, 5.5725e-01, 2.2894e-01, 9.9477e-03, 2.4422e-01,
         1.9409e+00, 1.8763e+00, 6.9105e-01, 9.1217e-01, 3.1357e+00, 1.1179e+00,
         1.6046e+00, 7.5675e-02, 1.1855e+00, 4.0561e-02, 5.2014e-03, 1.2588e+00,
         1.3045e+00, 2.5276e-01, 5.0003e-02, 4.2414e-01, 1.7932e-01, 7.2137e-01,
         4.8578e-03, 2.4026e-01, 4.8343e-02, 1.7864e-02, 7.8924e-04, 1.1537e-01,
         1.7162e+00, 1.1897e+00, 2.5232e-08],
        [6.2651e+00, 5.0770e+00, 4.4337e+00, 2.8952e+00, 3.0272e+00, 5.3170e+00,
         2.4795e+00, 3.8771e+00, 3.3226e+00, 2.9189e+00, 3.4693e+00, 4.1637e-01,
         2.1313e+00, 3.4468e+00, 1.9966e+00, 3.6558e+00, 2.1170e+00, 2.3563e+00,
         1.8005e+00, 2.0123e+00, 3.6027e+00, 2.9818e+00, 3.0912e+00, 1.9115e+00,
         3.0392e+00, 3.3994e+00, 4.4070e+00, 1.9427e-01, 3.2244e+00, 2.8657e-02,
         1.6248e+00, 1.0581e+00, 1.4899e+00, 1.9054e+00, 1.9988e+00, 1.8871e-01,
         3.5470e-03, 1.5206e+00, 3.5253e+00, 3.1223e+00, 1.4398e+00, 1.6423e+00,
         2.0374e+00, 2.0463e+00, 1.6647e+00, 4.1298e+00, 8.1298e-02, 2.3595e+00,
         3.6780e+00, 5.3524e-01, 1.9928e+00, 2.1402e+00, 1.7608e+00, 1.4053e+00,
         3.5115e+00, 4.2262e+00, 3.3201e+00, 2.6964e-02, 3.0577e+00, 8.6002e-01,
         4.0159e+00, 1.5836e-03, 2.4875e+00, 1.9516e+00, 1.1139e+00, 1.9659e+00,
         1.8451e+00, 1.4328e+00, 1.6645e+00, 3.6735e+00, 4.3806e+00, 1.4663e+00,
         8.1383e-01, 2.0477e+00, 2.2044e+00, 1.6106e+00, 2.0304e+00, 3.7887e+00,
         3.7681e-01, 1.2768e+00, 1.2246e+00, 1.6334e+00, 1.3561e+00, 5.8874e-01,
         2.2160e-03, 1.2443e+00, 1.0885e+00, 8.6825e-01, 1.8319e-03, 6.3374e-01,
         1.0170e+00, 1.7940e+00, 1.7012e+00, 3.0279e+00, 3.3891e+00, 6.1129e-01,
         7.3968e-01, 4.1365e+00, 3.2655e+00, 2.4572e-02, 4.1863e+00, 2.5992e+00,
         3.7412e+00, 1.9966e+00, 3.3486e-01, 1.4200e-02, 3.1742e+00, 1.2188e+00,
         8.4771e-01, 5.8230e-03, 3.3001e+00, 2.9399e+00, 1.3638e+00, 3.4068e+00,
         3.9902e+00, 4.2507e+00, 3.2497e+00, 7.2398e-03, 2.9908e+00, 2.4555e+00,
         1.1338e+00, 2.7958e+00, 2.4123e+00, 3.3606e+00, 7.5065e-01, 1.7839e+00,
         1.4517e+00, 1.3810e-03, 2.7505e+00, 9.7589e-01, 1.4531e+00, 1.2582e-01,
         9.8385e-01, 1.1536e+00, 5.4479e-02, 1.4063e+00, 4.7007e-01, 1.3961e-02,
         1.8464e+00, 2.2446e+00, 6.7498e-01, 9.3289e-01, 9.8386e-02, 4.0117e-01,
         3.1939e-01, 3.6427e-03, 1.0090e+00, 2.5034e+00, 2.4789e+00, 2.1788e+00,
         2.6083e+00, 1.9254e+00, 1.3902e+00, 1.3483e-01, 4.6001e-02, 1.3019e-02,
         2.2189e+00, 1.2256e+00, 1.1424e+00, 2.3516e+00, 1.4482e+00, 1.4579e-01,
         1.0059e+00, 2.4793e+00, 2.7268e+00, 3.3504e-01, 7.4426e-01, 9.4545e-01,
         1.6948e-02, 6.3230e-01, 1.6131e-01, 4.4109e-03, 1.7396e+00, 9.3812e-01,
         7.2759e-01, 4.5796e-02, 1.2377e+00, 1.5082e+00, 8.3990e-01, 3.0550e-01,
         3.9226e-02, 1.7929e-01, 1.2299e-01, 4.5354e-03, 9.8514e-02, 1.1065e-01,
         3.1222e-02, 2.6947e-02, 5.4271e-03, 4.6999e-01, 2.7377e+00, 1.0181e+00,
         3.3357e+00, 2.5352e+00, 3.0053e+00, 2.0883e+00, 2.2693e-01, 4.2540e-01,
         4.5045e-01, 1.7794e-04, 1.7999e+00, 2.1706e+00, 1.2505e+00, 4.6698e-01,
         2.9731e-02, 1.3186e+00, 3.8459e+00, 1.2727e+00, 1.4699e+00, 1.0618e-01,
         5.8409e-01, 9.7371e-01, 6.5867e-01, 7.9958e-01, 3.8349e-03, 7.6863e-01,
         1.0551e+00, 7.1163e-02, 6.6684e-01, 3.4925e-02, 3.7285e-03, 5.2058e-01,
         5.7871e-02, 1.5917e-01, 1.7310e-02, 1.0135e-02, 4.7886e-03, 9.2485e-02,
         4.2149e-01, 6.8063e-02, 1.2048e-03, 1.1925e+00, 1.0492e+00, 5.8003e-01,
         2.0641e+00, 1.7919e+00, 2.5306e+00, 2.3933e+00, 8.6898e-01, 3.1993e+00,
         2.3917e+00, 2.4764e+00, 5.5521e-01, 1.6225e-02, 6.5521e-03, 6.1115e-03,
         2.8587e-01, 8.6663e-01, 2.5515e-01, 1.4460e-03, 7.4016e-01, 2.3889e+00,
         2.1154e+00, 2.7228e+00, 1.2867e+00, 1.4106e+00, 5.5903e-01, 6.2230e-02,
         3.1727e-03, 1.7518e+00, 6.6126e-01, 6.7501e-02, 1.2754e+00, 1.1875e+00,
         7.7399e-01, 1.0934e+00, 6.8003e-01, 2.6445e-01, 5.5379e-04, 1.0266e+00,
         7.5507e-01, 1.2677e+00, 7.8069e-02, 2.8899e-01, 2.0346e+00, 1.4621e+00,
         1.2098e+00, 2.7248e+00, 8.5895e-01, 2.1791e-01, 4.6501e-03, 9.0195e-01,
         1.5885e+00, 3.5807e-01, 2.6986e-01, 1.5266e+00, 6.4076e-01, 8.0659e-01,
         7.1261e-01, 7.0830e-04, 1.1997e+00, 1.7691e+00, 2.3823e-02, 2.2612e-01,
         1.8211e+00, 9.7710e-01, 1.8344e-01, 1.2174e+00, 3.0384e-01, 1.0378e-01,
         7.0970e-01, 1.4350e-01, 1.8956e-02, 1.5505e-02, 1.4678e-02, 1.2641e+00,
         1.0201e+00, 7.1946e-01, 8.2288e-02, 5.6602e-01, 2.0239e-01, 5.1118e-03,
         3.5672e-02, 1.5388e-02, 1.3711e+00, 5.6572e-02, 6.9856e-03, 1.2394e+00,
         1.3158e+00, 2.9912e+00, 1.4491e-01, 2.5832e-01, 1.6415e+00, 9.0566e-01,
         3.2059e-01, 8.0714e-02, 1.6853e-03, 5.0862e-03, 3.4201e-03, 1.6062e-01,
         1.5396e-02, 4.0347e-03, 5.7929e-01, 3.9074e-01, 1.9053e-01, 2.1224e-04,
         2.8683e-01, 9.6647e-01, 6.0886e-09]], device='cuda:0')
torch.Size([2, 339])
tensor([6.2651e+00, 5.0770e+00, 4.4337e+00, 2.8952e+00, 3.0272e+00, 1.3339e+00,
        3.2072e+00, 2.1036e+00, 5.6732e+00, 1.6338e-01, 6.6122e+00, 2.1985e+00,
        3.5838e+00, 2.4795e+00, 1.6721e+00, 7.1178e-01, 7.4313e-02, 1.9560e+00,
        6.9544e-01, 2.0172e+00, 2.4215e+00, 4.4070e+00, 1.1690e-02, 1.3636e+00,
        4.0892e+00, 2.9519e+00, 5.9615e-01, 2.6670e+00, 2.0388e+00, 3.4913e+00,
        1.7541e-01, 4.3771e+00, 2.1685e+00, 9.6750e-01, 1.4360e+00, 4.3753e-01,
        5.1750e-02, 3.6603e+00, 1.3646e-01, 1.4487e-01, 1.4587e+00, 3.1711e+00,
        1.5791e+00, 3.6638e+00, 1.5643e-01, 3.0394e+00, 2.9458e+00, 3.0967e+00,
        1.1848e+00, 3.0756e+00, 1.5530e+00, 2.8099e+00, 1.2597e+00, 7.3260e-01,
        4.5646e-01, 2.0499e+00, 2.6711e+00, 2.4169e+00, 1.3795e+00, 7.8474e-01,
        1.7587e+00, 3.0499e+00, 3.0986e+00, 4.4493e-01, 6.4524e-01, 4.3940e+00,
        4.2037e+00, 1.7396e+00, 2.3667e+00, 1.6701e+00, 2.5918e-01, 2.6035e+00,
        2.3565e+00, 1.4240e+00, 5.8918e-01, 7.2590e-01, 3.7801e-02, 2.8024e-01,
        1.8621e-01, 7.8967e-02, 3.5114e-03, 1.6237e-01, 9.0226e-01, 5.3538e+00,
        9.3395e-01, 3.2152e+00, 2.5020e+00, 3.4677e-01, 5.5726e-01, 4.1605e-02,
        1.2317e-02, 2.5702e-02, 1.5492e+00, 3.2916e+00, 2.7331e+00, 3.9574e+00,
        2.7736e+00, 3.6763e+00, 3.6004e+00, 2.2902e+00, 2.5290e+00, 1.7211e+00,
        2.8178e-01, 2.0265e+00, 1.9616e+00, 2.0189e-01, 2.7148e+00, 1.5654e+00,
        6.5115e-01, 3.2991e+00, 4.9716e-01, 1.7104e+00, 1.8473e+00, 1.7823e+00,
        9.4819e-01, 6.4214e-01, 4.0271e-01, 7.8341e-02, 1.5406e-01, 1.0290e-01,
        3.1917e-02, 5.4542e-01, 8.0862e-01, 8.1633e-01, 4.4765e-01, 1.5222e-01,
        7.8674e-01, 7.0187e-01, 1.5755e+00, 9.0335e-01, 3.2282e-02, 5.0857e-02,
        6.0011e-01, 2.2655e-01, 1.4271e-01, 4.1894e-01, 1.2798e+00, 2.1525e+00,
        2.9358e+00, 3.2728e+00, 7.9637e-01, 2.7924e+00, 8.3974e-01, 2.3957e-03,
        2.8096e+00, 7.1084e-01, 8.0478e-01, 1.8359e-01, 2.0003e-02, 1.8449e+00,
        7.4003e-01, 1.1569e+00, 2.0225e+00, 1.3270e-01, 9.0073e-01, 1.4329e+00,
        2.3097e+00, 6.0724e-01, 6.7335e-02, 6.6054e-01, 2.1421e-01, 5.4796e-01,
        4.7693e-02, 5.8083e-01, 2.5325e-01, 9.8586e-01, 9.8688e-01, 6.4786e-01,
        6.8990e-02, 1.5509e-02, 3.1702e-02, 5.4499e-02, 5.8383e-01, 1.1701e+00,
        1.3482e-01, 1.5115e+00, 5.1889e-02, 3.3053e-02, 7.6764e-01, 5.2370e-01,
        6.4120e-01, 9.3876e-01, 4.5189e-02, 4.0134e-02, 9.2721e-01, 1.0619e-01,
        1.6509e-02, 9.5935e-02, 1.3974e+00, 2.0943e-01, 5.6499e-01, 1.4116e+00,
        3.2213e+00, 8.0079e-01, 2.5356e+00, 1.7319e-02, 3.3131e-01, 2.3233e-02,
        1.3768e-03, 9.7968e-02, 1.8147e-02, 2.3989e-01, 1.2506e-02, 4.8872e-03,
        3.6175e-02, 1.4511e-02, 4.7271e-02, 1.4450e-01, 7.2526e-03, 6.6747e-01,
        4.4479e-01, 1.1462e+00, 6.3257e-02, 1.6807e-02, 9.6797e-02, 1.3395e-01,
        2.6604e-02, 6.1508e-03, 9.7722e-02, 8.4947e-02, 4.0284e-02, 2.7049e-02,
        1.3109e-02, 7.0623e-03, 6.8406e-03, 2.5879e-03, 1.9220e-02, 4.6278e-02,
        1.2286e-01, 3.5393e-02, 2.0366e-01, 4.9215e-03, 1.3224e-02, 1.0408e-01,
        3.7810e-02, 6.8069e-02, 1.6409e-01, 1.1154e-02, 1.0686e-02, 6.0723e-02,
        1.0820e-02, 1.0611e-02, 2.1055e-02, 1.1505e-01, 1.0570e-02, 1.2012e-01,
        3.1564e-01, 3.0809e-01, 1.7868e-01, 2.7979e+00, 1.0732e+00, 1.3092e+00,
        5.0585e-01, 3.6209e-02, 4.2233e-01, 2.1880e-02, 6.6865e-03, 8.2816e-01,
        2.4508e-01, 1.6776e-01, 8.4871e-02, 1.7050e-01, 1.7417e-03, 3.8669e-02,
        2.1646e-02, 6.5109e-03, 6.1485e-04, 3.1873e-02, 9.3309e-01, 1.1979e+00,
        1.4494e+00, 1.0786e+00, 1.5596e+00, 1.9429e-02, 1.5903e-01, 1.8219e+00,
        8.8001e-01, 4.6803e-02, 6.4416e-02, 4.2281e-03, 1.3909e-01, 2.5197e-02,
        1.0400e+00, 1.0572e+00, 6.3493e-01, 2.0618e+00, 1.2835e+00, 2.0668e+00,
        4.5865e-01, 1.7047e-01, 2.8061e+00, 2.3945e+00, 9.7968e-03, 4.3799e-01,
        2.8750e-01, 1.9288e+00, 1.3081e-01, 7.7630e-02, 6.4621e-02, 8.8270e-02,
        2.3892e-02, 2.1309e-02, 4.8245e-02, 3.3568e-03, 5.1736e-02, 2.0948e-01,
        6.1820e-01, 2.3720e-01, 5.5725e-01, 2.2894e-01, 9.9477e-03, 2.4422e-01,
        1.9409e+00, 1.8763e+00, 6.9105e-01, 9.1217e-01, 3.1357e+00, 1.1179e+00,
        1.6046e+00, 7.5675e-02, 1.1855e+00, 4.0561e-02, 5.2014e-03, 1.2588e+00,
        1.3045e+00, 2.5276e-01, 5.0003e-02, 4.2414e-01, 1.7932e-01, 7.2137e-01,
        4.8578e-03, 2.4026e-01, 4.8343e-02, 1.7864e-02, 7.8924e-04, 1.1537e-01,
        1.7162e+00, 1.1897e+00, 2.5232e-08], device='cuda:0')
torch.Size([339])
[[83, 65, 66], [115, 100, 97]]
"\nTo find the total number of cases where you order 1 coffee and 1 bread (either a cookie or a muffin), you can use the counting principle, which states that if there are \\( m \\) ways to do one thing and \\( n \\) ways to do another, then there are \\( m \\times n \\) ways to do both.\n\nFirst, let's find the total number of ways to order 1 coffee. Since there are 5 types of hot coffee and 4 types of cold coffee, there are a total of \\( 5 + 4 = 9 \\) types of coffee.\n\nNext, let's find the total number of ways to order 1 bread. Since there are 3 types of cookies and 5 types of muffins, there are a total of \\( 3 + 5 = 8 \\) types of bread.\n\nNow, to find the total number of cases where you order 1 coffee and 1 bread, you multiply the number of coffee choices by the number of bread choices:\n\n\\( 9 \\) (types of coffee) \\( \\times 8 \\) (types of bread) \\( = 72 \\) cases.\n\nSo, there are 72 different cases where you can order 1 coffee and 1 bread.</s>"
'<s> ###Input:\nThere are 5 types of hot coffee and 4 types of cold coffee in the cafe, and there are 3 types of cookies and 5 types of muffins. Find the number of cases where you order 1 coffee and 1 bread each. \n###Output:'
'<s> ###Input:\nThere are 5 types of hot coffee and 4 types of cold coffee in the cafe, and there are 3 types of cookies and 5 types of muffins. Find the number of cases where you order 1 coffee and 1 bread each. \n###Output:\n'
'<s> ###Input:\nThere are 5 types of hot coffee and 4 types of cold coffee in the cafe, and there are 3 types of cookies and 5 types of muffins. Find the number of cases where you order 1 coffee and 1 bread each. \n###Output:\nTo find the total number of cases where you order 1 coffee and 1 bread'
'<s> ###Input:\nThere are 5 types of hot coffee and 4 types of cold coffee in the cafe, and there are 3 types of cookies and 5 types of muffins. Find the number of cases where you order 1 coffee and 1 bread each. \n###Output:\nTo find the total number of cases where you order 1 coffee and 1 bread ('
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 877, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 861, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 399, in train
    outputs = think_tuner_step(batch, model=model, tokenizer=tokenizer, train_config=train_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 495, in think_tuner_step
    total_gate_loss, total_reinforce_loss, total_nll_thought, logy = start_thinking(batch["input_ids"], batch["prompt_length"], batch["total_length"], outputs.last_hidden_state, logits,  batch["labels"], unreduced_loss, model, tokenizer, use_reward_decay = train_config.use_reward_decay, use_best_reward_signal=train_config.use_best_reward_signal,reward_based_optimization=train_config.reward_based_optimization, sample_thought_by_prompt=train_config.sample_thought_by_prompt)
                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 288, in start_thinking
    for i, indices in enumerate(selected_indices):
                      ^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit

If you suspect this is an IPython 8.28.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True
