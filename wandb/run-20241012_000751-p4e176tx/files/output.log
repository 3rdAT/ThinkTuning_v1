Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.10s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                    [0m| 0/225 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/modeling_llama.py:1373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
Preparing 4D causal attention mask with cache position
torch.Size([1064])
tensor(1.0413, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 267])
gate shape:  torch.Size([4, 267])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1472, device='cuda:1')
torch.Size([38])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4278)
idx:  37
Gate loss:  tensor(0.4278, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1248, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4479)
idx:  111
Gate loss:  tensor(0.8757, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1546, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1710)
idx:  197
Gate loss:  tensor(2.0467, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2897)
idx:  44
Gate loss:  tensor(2.3363, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4712)
idx:  150
Gate loss:  tensor(2.8075, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16429, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3270)
idx:  178
Gate loss:  tensor(3.1345, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2000)
idx:  63
Gate loss:  tensor(3.3345, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3267)
idx:  102
Gate loss:  tensor(3.6612, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6440)
idx:  243
Gate loss:  tensor(6.3052, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8095)
idx:  166
Gate loss:  tensor(7.1147, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7736, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9992)
idx:  208
Gate loss:  tensor(8.1139, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([278])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5045)
idx:  214
Gate loss:  tensor(9.6184, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Training Epoch: 1/1, step 0/225 completed (loss: 1.041335940361023):   0%|[34m                        [0m| 1/225 [00:20<1:18:02, 20.91s/it][0m
Preparing 4D causal attention mask with cache position
torch.Size([1472])
tensor(3.2953, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 369])
gate shape:  torch.Size([4, 369])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8773)
idx:  174
Gate loss:  tensor(0.8773, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7985)
idx:  293
Gate loss:  tensor(2.6758, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1320)
idx:  322
Gate loss:  tensor(5.8078, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7071)
idx:  302
Gate loss:  tensor(7.5149, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(27881, device='cuda:1')
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4290)
idx:  321
Gate loss:  tensor(10.9439, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5186, device='cuda:1')
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8298)
idx:  330
Gate loss:  tensor(15.7736, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6485)
idx:  291
Gate loss:  tensor(17.4221, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7856)
idx:  296
Gate loss:  tensor(19.2077, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.1879)
idx:  362
Gate loss:  tensor(37.3957, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4188)
idx:  112
Gate loss:  tensor(37.8144, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4306)
idx:  184
Gate loss:  tensor(38.2450, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
reasoning_path shape:  torch.Size([380])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1972)
idx:  271
Gate loss:  tensor(39.4423, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1400])
tensor(1.7304, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 351])
gate shape:  torch.Size([4, 351])
The shape of hidden_states: torch.Size([4096])
The topk tensor(2022, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2806)
idx:  6
Gate loss:  tensor(0.2806, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2606)
idx:  133
Gate loss:  tensor(0.5413, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9570)
idx:  275
Gate loss:  tensor(1.4982, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(881, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3830)
idx:  133
Gate loss:  tensor(1.8812, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3031)
idx:  153
Gate loss:  tensor(2.1843, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4950)
idx:  190
Gate loss:  tensor(2.6793, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(3719, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1700)
idx:  54
Gate loss:  tensor(2.8494, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1730)
idx:  118
Gate loss:  tensor(3.0224, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(433, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2453)
idx:  127
Gate loss:  tensor(3.2677, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1831)
idx:  127
Gate loss:  tensor(3.4508, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2618)
idx:  162
Gate loss:  tensor(3.7126, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([362])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5920)
idx:  233
Gate loss:  tensor(4.3046, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([908])
tensor(1.3275, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 228])
gate shape:  torch.Size([4, 228])
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5706)
idx:  124
Gate loss:  tensor(0.5706, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7021)
idx:  128
Gate loss:  tensor(1.2727, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8152)
idx:  194
Gate loss:  tensor(3.0880, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4805)
idx:  42
Gate loss:  tensor(3.5685, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(763, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6385)
idx:  110
Gate loss:  tensor(4.2070, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([236])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5831)
idx:  140
Gate loss:  tensor(4.7901, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2691)
idx:  9
Gate loss:  tensor(5.0592, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3558)
idx:  93
Gate loss:  tensor(5.4150, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.1505)
idx:  220
Gate loss:  tensor(11.5656, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(988, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3441)
idx:  83
Gate loss:  tensor(11.9097, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.9963)
idx:  219
Gate loss:  tensor(19.9060, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2125, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([239])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.0413)
idx:  222
Gate loss:  tensor(33.9473, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1348])
tensor(1.3227, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 338])
gate shape:  torch.Size([4, 338])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3374)
idx:  67
Gate loss:  tensor(0.3374, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26807, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6286)
idx:  175
Gate loss:  tensor(0.9661, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2041, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6133)
idx:  199
Gate loss:  tensor(1.5793, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4493)
idx:  159
Gate loss:  tensor(2.0286, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29918, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0698)
idx:  226
Gate loss:  tensor(3.0984, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9475)
idx:  256
Gate loss:  tensor(4.0459, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2914)
idx:  68
Gate loss:  tensor(4.3373, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6295, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4073)
idx:  165
Gate loss:  tensor(4.7446, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4710)
idx:  225
Gate loss:  tensor(5.2156, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1644, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2207)
idx:  44
Gate loss:  tensor(5.4362, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2960)
idx:  96
Gate loss:  tensor(5.7323, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2, device='cuda:1')
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-21.5653)
idx:  336
Gate loss:  tensor(27.2975, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([868])
tensor(1.4784, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 218])
gate shape:  torch.Size([4, 218])
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3575)
idx:  52
Gate loss:  tensor(0.3575, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6572)
idx:  126
Gate loss:  tensor(1.0147, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(14772, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3968)
idx:  11
Gate loss:  tensor(1.4115, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5866)
idx:  110
Gate loss:  tensor(1.9981, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7628)
idx:  140
Gate loss:  tensor(2.7608, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(21811, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6572)
idx:  50
Gate loss:  tensor(3.4181, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7543)
idx:  155
Gate loss:  tensor(5.1724, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(21811, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4621)
idx:  163
Gate loss:  tensor(6.6344, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2721)
idx:  13
Gate loss:  tensor(6.9065, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5617)
idx:  90
Gate loss:  tensor(7.4682, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([229])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5681)
idx:  195
Gate loss:  tensor(10.0363, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([764])
tensor(1.2365, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 192])
gate shape:  torch.Size([4, 192])
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6229)
idx:  70
Gate loss:  tensor(0.6229, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3367)
idx:  128
Gate loss:  tensor(1.9596, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6306, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1545)
idx:  135
Gate loss:  tensor(3.1141, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4091)
idx:  160
Gate loss:  tensor(4.5232, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9135)
idx:  162
Gate loss:  tensor(6.4367, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.5409)
idx:  179
Gate loss:  tensor(11.9777, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(25402, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4919)
idx:  7
Gate loss:  tensor(12.4696, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5015)
idx:  64
Gate loss:  tensor(12.9710, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2618, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7034)
idx:  87
Gate loss:  tensor(13.6745, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4398)
idx:  18
Gate loss:  tensor(14.1143, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8485)
idx:  94
Gate loss:  tensor(14.9628, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([203])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5931)
idx:  96
Gate loss:  tensor(15.5560, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1008])
tensor(1.7208, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 253])
gate shape:  torch.Size([4, 253])
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2434)
idx:  50
Gate loss:  tensor(0.2434, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3826)
idx:  88
Gate loss:  tensor(0.6260, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8621)
idx:  183
Gate loss:  tensor(1.4881, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4290, device='cuda:1')
torch.Size([2])
torch.Size([1])
torch.Size([2])
Inside custom generate sample func
torch.Size([2])
torch.Size([1])
torch.Size([2])
Inside custom generate sample func
torch.Size([2])
torch.Size([1])
torch.Size([2])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1575)
idx:  1
Gate loss:  tensor(1.6456, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.0773)
idx:  2
Gate loss:  tensor(1.7229, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2103)
idx:  33
Gate loss:  tensor(1.9331, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3268)
idx:  32
Gate loss:  tensor(2.2599, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5083)
idx:  150
Gate loss:  tensor(2.7682, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4505, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6439)
idx:  191
Gate loss:  tensor(3.4121, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(610, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2879)
idx:  79
Gate loss:  tensor(3.7000, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3325)
idx:  210
Gate loss:  tensor(5.0324, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([264])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5078)
idx:  234
Gate loss:  tensor(8.5402, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1100])
tensor(1.3388, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 276])
gate shape:  torch.Size([4, 276])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4539)
idx:  54
Gate loss:  tensor(0.4539, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5045)
idx:  118
Gate loss:  tensor(0.9584, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9225)
idx:  186
Gate loss:  tensor(1.8809, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3452)
idx:  54
Gate loss:  tensor(2.2261, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4863)
idx:  150
Gate loss:  tensor(2.7125, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5981)
idx:  156
Gate loss:  tensor(3.3106, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4094, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3912)
idx:  15
Gate loss:  tensor(3.7018, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3835)
idx:  31
Gate loss:  tensor(4.0853, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3109, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4096)
idx:  245
Gate loss:  tensor(6.4949, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(7799, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3104)
idx:  70
Gate loss:  tensor(6.8053, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3282)
idx:  158
Gate loss:  tensor(7.1335, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8394)
idx:  228
Gate loss:  tensor(7.9729, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([712])
tensor(1.1079, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 179])
gate shape:  torch.Size([4, 179])
The shape of hidden_states: torch.Size([4096])
The topk tensor(2277, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5385)
idx:  46
Gate loss:  tensor(0.5385, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(368, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6854)
idx:  52
Gate loss:  tensor(1.2239, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(373, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.9892)
idx:  155
Gate loss:  tensor(5.2131, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3566)
idx:  136
Gate loss:  tensor(6.5697, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1741)
idx:  139
Gate loss:  tensor(7.7438, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0710)
idx:  140
Gate loss:  tensor(8.8148, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2248)
idx:  33
Gate loss:  tensor(9.0396, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4535)
idx:  73
Gate loss:  tensor(9.4931, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4524)
idx:  146
Gate loss:  tensor(10.9455, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4062)
idx:  126
Gate loss:  tensor(11.3517, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0121)
idx:  140
Gate loss:  tensor(12.3637, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24246, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([190])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2924)
idx:  162
Gate loss:  tensor(15.6561, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([932])
tensor(1.1752, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 234])
gate shape:  torch.Size([4, 234])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3705)
idx:  133
Gate loss:  tensor(0.3705, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4638)
idx:  134
Gate loss:  tensor(0.8343, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7474)
idx:  202
Gate loss:  tensor(2.5817, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29909, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2651)
idx:  38
Gate loss:  tensor(2.8468, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7936, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3198)
idx:  54
Gate loss:  tensor(3.1666, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(265, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4705)
idx:  55
Gate loss:  tensor(3.6370, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1334, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3823)
idx:  113
Gate loss:  tensor(4.0193, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20459, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9126)
idx:  151
Gate loss:  tensor(4.9319, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([244])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.7139)
idx:  214
Gate loss:  tensor(9.6459, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2744)
idx:  6
Gate loss:  tensor(9.9202, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2877)
idx:  27
Gate loss:  tensor(10.2079, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3800, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3912)
idx:  47
Gate loss:  tensor(10.5992, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1052])
tensor(1.4077, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 264])
gate shape:  torch.Size([4, 264])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2717)
idx:  27
Gate loss:  tensor(0.2717, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3897, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3589)
idx:  32
Gate loss:  tensor(0.6306, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(320, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3358)
idx:  91
Gate loss:  tensor(0.9664, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4132)
idx:  13
Gate loss:  tensor(1.3796, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7227)
idx:  139
Gate loss:  tensor(2.1023, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1070)
idx:  201
Gate loss:  tensor(3.2093, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29916, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3984)
idx:  100
Gate loss:  tensor(3.6077, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29961, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4579)
idx:  145
Gate loss:  tensor(4.0655, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5149, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8863)
idx:  187
Gate loss:  tensor(4.9518, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4358)
idx:  122
Gate loss:  tensor(5.3876, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4891)
idx:  127
Gate loss:  tensor(5.8767, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7063, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([275])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5432)
idx:  175
Gate loss:  tensor(6.4199, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([656])
tensor(1.1295, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 165])
gate shape:  torch.Size([4, 165])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6013)
idx:  82
Gate loss:  tensor(0.6013, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9924)
idx:  124
Gate loss:  tensor(1.5938, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 2
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5986)
idx:  95
Gate loss:  tensor(2.1924, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([169])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.0806)
idx:  161
Gate loss:  tensor(16.2730, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.0368)
idx:  163
Gate loss:  tensor(34.3098, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3681)
idx:  25
Gate loss:  tensor(34.6779, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10454, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7966)
idx:  107
Gate loss:  tensor(35.4745, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6347)
idx:  134
Gate loss:  tensor(37.1093, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3675)
idx:  18
Gate loss:  tensor(37.4767, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5205)
idx:  40
Gate loss:  tensor(37.9972, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-22.9927)
idx:  163
Gate loss:  tensor(60.9899, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([904])
tensor(1.1734, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 227])
gate shape:  torch.Size([4, 227])
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7096)
idx:  101
Gate loss:  tensor(0.7096, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6186)
idx:  130
Gate loss:  tensor(1.3282, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1855)
idx:  169
Gate loss:  tensor(2.5137, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1819, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0784)
idx:  165
Gate loss:  tensor(3.5921, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2701)
idx:  187
Gate loss:  tensor(4.8621, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.5761)
idx:  215
Gate loss:  tensor(12.4382, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2085)
idx:  66
Gate loss:  tensor(12.6467, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18942, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3446)
idx:  71
Gate loss:  tensor(12.9913, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6295, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9372)
idx:  165
Gate loss:  tensor(13.9285, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2818)
idx:  45
Gate loss:  tensor(14.2102, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7943)
idx:  171
Gate loss:  tensor(15.0046, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([238])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5542)
idx:  175
Gate loss:  tensor(15.5588, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1364])
tensor(1.0728, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 342])
gate shape:  torch.Size([4, 342])
The shape of hidden_states: torch.Size([4096])
The topk tensor(10501, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3650)
idx:  80
Gate loss:  tensor(0.3650, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4713)
idx:  182
Gate loss:  tensor(0.8363, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0896)
idx:  267
Gate loss:  tensor(1.9259, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(6293, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2229)
idx:  61
Gate loss:  tensor(2.1488, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29924, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2706)
idx:  115
Gate loss:  tensor(2.4194, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
reasoning_path shape:  torch.Size([352])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2329)
idx:  313
Gate loss:  tensor(5.6524, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(511, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2245)
idx:  19
Gate loss:  tensor(5.8768, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3171, device='cuda:1')
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7547)
idx:  253
Gate loss:  tensor(6.6316, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2397, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9909)
idx:  260
Gate loss:  tensor(7.6225, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3903)
idx:  185
Gate loss:  tensor(8.0129, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5159)
idx:  224
Gate loss:  tensor(8.5288, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
reasoning_path shape:  torch.Size([353])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5746)
idx:  282
Gate loss:  tensor(9.1034, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([664])
tensor(1.1939, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 167])
gate shape:  torch.Size([4, 167])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6246)
idx:  53
Gate loss:  tensor(0.6246, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9162)
idx:  139
Gate loss:  tensor(2.5408, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4312, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.6365)
idx:  153
Gate loss:  tensor(7.1773, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5288)
idx:  22
Gate loss:  tensor(7.7061, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4641)
idx:  60
Gate loss:  tensor(8.1702, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8565)
idx:  94
Gate loss:  tensor(9.0267, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6376)
idx:  46
Gate loss:  tensor(9.6643, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1363, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3417)
idx:  107
Gate loss:  tensor(11.0060, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3031)
idx:  146
Gate loss:  tensor(14.3091, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8171)
idx:  105
Gate loss:  tensor(15.1262, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(583, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5660)
idx:  124
Gate loss:  tensor(16.6921, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(21152, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([178])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4533)
idx:  133
Gate loss:  tensor(18.1455, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1340])
tensor(1.2254, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 336])
gate shape:  torch.Size([4, 336])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3815)
idx:  124
Gate loss:  tensor(0.3815, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4849)
idx:  145
Gate loss:  tensor(0.8664, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4126)
idx:  171
Gate loss:  tensor(1.2790, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6888)
idx:  209
Gate loss:  tensor(1.9678, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3620)
idx:  282
Gate loss:  tensor(3.3298, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7149)
idx:  297
Gate loss:  tensor(5.0446, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29885, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2376)
idx:  57
Gate loss:  tensor(5.2822, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2318)
idx:  69
Gate loss:  tensor(5.5140, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4029)
idx:  138
Gate loss:  tensor(5.9169, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1477)
idx:  9
Gate loss:  tensor(6.0645, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2519)
idx:  154
Gate loss:  tensor(6.3164, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3029)
idx:  200
Gate loss:  tensor(6.6193, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([324])
tensor(1.5693, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 82])
gate shape:  torch.Size([4, 82])
The shape of hidden_states: torch.Size([4096])
The topk tensor(3492, device='cuda:1')
torch.Size([5])
torch.Size([1])
torch.Size([5])
Inside custom generate sample func
torch.Size([5])
torch.Size([1])
torch.Size([5])
Inside custom generate sample func
torch.Size([5])
torch.Size([1])
torch.Size([5])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2971)
idx:  4
Gate loss:  tensor(1.2971, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4287)
idx:  21
Gate loss:  tensor(2.7257, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.6227)
idx:  71
Gate loss:  tensor(12.3484, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(6466, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4000)
idx:  40
Gate loss:  tensor(14.7485, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1979)
idx:  54
Gate loss:  tensor(17.9464, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.7203)
idx:  73
Gate loss:  tensor(31.6667, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(549, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0465)
idx:  6
Gate loss:  tensor(32.7132, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2030, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2305)
idx:  31
Gate loss:  tensor(33.9436, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2060)
idx:  36
Gate loss:  tensor(35.1496, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(289, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1217)
idx:  27
Gate loss:  tensor(36.2713, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29926, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8873)
idx:  67
Gate loss:  tensor(41.1586, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.6813)
idx:  74
Gate loss:  tensor(47.8399, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([2492])
tensor(1.2634, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 624])
gate shape:  torch.Size([4, 624])
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([408])
torch.Size([1])
torch.Size([408])
Inside custom generate sample func
torch.Size([408])
torch.Size([1])
torch.Size([408])
Inside custom generate sample func
torch.Size([408])
torch.Size([1])
torch.Size([408])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8504)
idx:  407
Gate loss:  tensor(0.8504, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([471])
torch.Size([1])
torch.Size([471])
Inside custom generate sample func
torch.Size([471])
torch.Size([1])
torch.Size([471])
Inside custom generate sample func
torch.Size([471])
torch.Size([1])
torch.Size([471])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3102)
idx:  470
Gate loss:  tensor(2.1606, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3800, device='cuda:1')
torch.Size([555])
torch.Size([1])
torch.Size([555])
Inside custom generate sample func
torch.Size([555])
torch.Size([1])
torch.Size([555])
Inside custom generate sample func
torch.Size([555])
torch.Size([1])
torch.Size([555])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 847, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 829, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 389, in train
    outputs = model(**batch, tokenizer=tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/modeling_llama.py", line 1407, in forward
    packed_reasoning_path, _, packed_reasoning_path_casual_mask, packed = get_packed_inputs(reasoning_path, max_length=4090, pad_token_id=self.config.eos_token_id, thought_index=thought_index)
                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/pack_input_ids.py", line 127, in get_packed_inputs
    casual_mask = build_batched_causal_mask_from_attention_mask(attention_mask, torch.float).to(input_ids[0].device)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/pack_input_ids.py", line 107, in build_batched_causal_mask_from_attention_mask
    masks = zero_positions & masks_per_j  # Shape: [bz, seq_len, seq_len, seq_len]
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.47 GiB. GPU 0 has a total capacity of 93.12 GiB of which 399.12 MiB is free. Including non-PyTorch memory, this process has 92.71 GiB memory in use. Of the allocated memory 79.38 GiB is allocated by PyTorch, and 12.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
