Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.29s/it]
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 9000
--> Validation Set Length = 1000
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                          [0m| 0/4500 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py:297: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
> [0;32m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py[0m(141)[0;36mthink_loss[0;34m()[0m
[0;32m    140 [0;31m            [0;31m# reward_signal = (unreduced_loss[idx:] - packed_loss[index][thought['thought_end_index']:thought['end_index']]).detach()[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 141 [0;31m            [0mnll_signal[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mmean[0m[0;34m([0m[0mnll_signal[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    142 [0;31m            [0mreward_signal[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mmean[0m[0;34m([0m[0mreward_signal[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
tensor([-4.2699e-02,  1.4790e-01, -5.6703e-02, -8.2802e-03, -3.4015e-03,
        -1.3530e+00, -6.9839e-02, -5.8441e+00,  2.2624e+00, -9.3364e-02,
         7.5843e-02, -5.3882e-02, -2.9109e-02, -6.6216e-01, -1.2552e+00,
        -1.0813e-01,  3.7979e-05, -2.8016e-01, -4.7106e-01, -6.0049e-01,
        -4.0347e-02, -2.3643e-02, -1.0963e-01,  6.4044e-03, -9.8989e-03,
        -6.8336e-02, -1.7686e-01, -2.5645e+00, -2.8385e+00, -1.2193e+00,
         1.5314e+00,  3.0465e+00, -2.1908e+00,  2.5062e+00, -2.1506e-01,
        -2.1395e-01, -1.1557e+00, -3.1889e+00,  4.5595e+00,  1.2473e-01,
         5.5035e-02, -6.8564e-01, -1.8120e+00, -1.2455e+00,  3.8300e+00,
        -6.8365e-01,  4.3365e-01,  2.0593e-01,  4.9513e-02, -1.1941e+00,
         8.1133e-01,  4.7813e-02, -5.9473e-02,  2.5792e-02, -1.1243e-03,
         2.2286e-02, -1.6988e+00,  1.6798e+00, -1.6338e+00,  1.3315e+00,
        -5.9390e-01, -4.7637e+00,  3.6586e+00,  1.3487e+00,  2.9104e-01,
        -2.3652e-01, -4.3095e+00,  3.8328e+00,  2.9326e-01,  4.0904e-01,
        -1.2670e-02,  8.2369e-03, -8.7428e-03,  9.0413e-03, -5.4754e-01,
         4.6014e-01, -8.1329e-02, -1.0343e+00,  1.1747e+00, -4.8845e-02,
         3.7451e-02, -9.4087e-02, -1.3095e+00,  1.3006e+00, -7.9184e-02,
        -1.4620e-01, -2.4452e-01,  5.4726e-01, -7.8920e-01, -1.7418e+00,
         1.5142e+00, -4.6341e+00,  3.6819e+00, -3.0371e+00,  3.8038e+00,
        -1.1398e+00,  3.2260e-01,  1.4218e-01, -1.0602e-02,  5.9286e-02,
         7.5150e-03,  1.0720e-02, -7.5549e-02,  9.1968e-02, -2.0400e+00,
         1.7235e+00, -5.8112e-02, -1.9798e-01,  6.0592e-01, -1.3060e+00,
         2.5448e-01, -1.0185e-02,  5.9122e-03, -4.7795e-03, -5.5988e-01,
        -1.0426e+00,  8.4095e-01,  5.9652e-01, -1.0313e-01,  1.7125e-01,
        -2.8369e+00,  2.3704e+00,  3.0206e-01, -2.0204e-01,  2.0534e-01,
        -1.6000e-02,  3.4120e-02, -1.9846e-02, -2.1512e-02, -4.2495e-01,
         1.2918e+00, -9.1846e-03,  6.2565e-03, -1.4550e-03, -3.8222e+00,
         3.8380e+00,  7.4813e-02, -1.6073e+00,  1.9616e+00, -9.2280e-02,
        -8.6112e-01,  8.6554e-01, -5.1605e-03,  4.4143e-02, -2.0182e-01,
        -3.7009e-01,  3.6788e-01,  3.9652e-01, -9.4952e-03,  7.8039e-03,
        -2.5627e-03, -8.2459e-02,  5.7940e-02, -2.3769e-02, -4.6451e-01,
         6.1333e-01, -5.7146e-06, -6.6841e-02, -6.2830e-02, -2.3271e-02,
        -1.0850e+00,  4.3584e-02,  9.6787e-01, -7.3332e-01,  5.9193e-01,
         1.5148e-01,  2.8821e-02, -9.9574e-03, -6.0640e-03,  7.8074e-03,
        -2.9834e-02, -8.2008e-02, -1.8902e-01, -6.2712e+00,  4.3409e+00,
         2.7058e-01,  1.2372e+00,  7.7371e-01, -1.2227e+00, -3.7538e-01,
         1.6225e+00,  1.6760e-02, -1.2183e-02, -1.3224e+00,  1.3476e+00,
         5.3185e-03, -4.2991e-02, -6.0858e+00,  5.5854e+00,  8.2457e-01,
        -7.4010e-01,  7.0435e-01, -1.4180e+00,  9.4303e-01,  3.1824e-01,
         1.6256e-01, -3.8525e-01,  1.5324e-01,  2.5183e-01, -4.1086e-02,
        -4.5456e-02,  9.7651e-02, -1.3242e-02,  1.1845e-02, -2.4091e-02,
        -4.4029e-01, -1.0954e+00,  1.6473e+00, -1.1952e-01, -1.7421e-02,
         2.0312e-01, -5.0400e-01, -4.2443e-01,  3.4812e-01,  4.9211e-01,
        -4.9260e-03,  3.1855e-03, -1.3224e-02, -2.3040e+00,  1.5872e+00,
         7.6553e-01, -3.6458e+00,  4.0536e+00,  1.0449e-01, -6.5530e-02,
        -5.1987e-02, -4.8042e-01, -1.8648e-01,  4.7784e-01,  2.9098e-01,
         5.5117e-03,  5.5899e-03, -1.1083e-03, -6.6664e-03,  3.1583e-03,
        -1.5518e-01,  7.4648e-02,  1.3806e-01, -5.7984e-03,  4.0531e-03,
        -1.7449e-03, -1.3231e-01,  1.4535e-02, -4.4001e-01,  5.0630e-01,
        -4.2325e-02, -1.3355e-02, -1.5055e-01, -2.0256e+00,  7.7048e-01,
         1.4874e-01, -6.8455e-01,  1.8644e+00, -2.6952e-01,  2.2866e-01,
        -2.7404e-03, -2.5408e-01, -4.1848e-02,  2.4887e-01, -5.2588e-02,
         8.5438e-02, -9.7972e-01,  9.5702e-01, -1.3342e-02, -8.6799e-01,
         5.9576e-01, -3.5522e-01,  1.3434e-01, -3.2240e-01,  5.8960e-01,
        -2.2945e-03, -2.6468e-01,  2.2069e-01, -5.7379e-02,  8.5807e-04,
         2.8715e-02, -8.3148e-04, -1.9387e-03,  1.3289e-03, -4.1665e-02,
        -5.0623e-01,  6.7919e-02, -1.0468e-04, -1.0827e-03, -5.2807e-01,
         4.6920e-01, -1.7156e-02, -4.2809e-02,  2.0918e-02,  2.8714e-04,
        -3.7036e-01,  2.5733e-01, -4.2841e-02,  4.1375e-02,  8.2933e-04,
        -1.2332e-04, -3.4275e-03, -1.9622e-02, -3.5000e+00, -5.1515e+00,
         8.2886e+00, -1.1065e-01, -6.1599e-01, -8.5382e-02,  4.5761e-01,
         4.2697e-01,  6.5183e-03,  5.8087e-03, -1.1451e+00, -4.3567e-01,
         1.0490e+00, -3.0823e-01,  8.9238e-01, -7.3864e-01,  3.2207e-01,
         9.8384e-02,  2.8269e-01, -2.2302e-02,  6.0781e-04,  4.2945e-03,
        -4.5251e-02,  4.0152e-02,  3.9842e-04,  1.2063e-02, -8.4407e-02,
         7.5480e-02, -9.7730e-04, -3.4783e-01,  3.3829e-01, -1.0909e-01,
         6.2921e-02, -3.6399e-02, -1.8421e-01,  9.3936e-02,  7.7670e-02,
         6.9478e-02, -1.8684e-02, -3.9520e-02,  8.3552e-03,  3.8280e-02,
         8.7152e-03, -9.1766e-04,  4.9617e-04, -2.7840e-03,  2.4736e-03,
        -4.6093e-02,  3.3616e-02, -2.4645e-02,  9.8395e-03,  8.6293e-03,
        -2.2215e-03, -1.0862e-03, -1.6353e-01,  1.5352e-01,  3.4141e-03,
        -6.8471e-02,  7.5546e-02,  1.1250e-03, -2.0036e-02, -3.6600e-01,
         3.4019e-01, -6.7970e-01,  7.0672e-01, -4.9668e-02,  6.1483e-02,
        -2.1917e-03, -9.3261e-03,  5.2341e-03,  3.2839e-03, -1.1279e-03,
         1.8051e-04, -2.4426e-01,  2.4791e-01, -4.7476e+00,  4.7219e+00,
        -2.9360e-01,  2.7781e-01, -1.2517e-02, -4.3763e-02,  3.8565e-02,
        -2.5963e-04, -7.2836e-04, -3.5650e-01, -6.4402e-02,  3.9445e-01,
        -1.1893e-02,  1.9068e-02,  6.2172e-03, -3.6795e-01,  3.0262e-01,
        -8.2782e-02, -4.2391e-01,  3.8482e-01,  4.4497e-02, -2.5835e-02,
         2.3261e-02, -3.0139e-02,  9.5392e-03, -1.9120e-01,  2.4710e-01,
        -3.4474e-04, -6.6537e-04, -7.0775e-01,  6.2082e-01,  7.6360e-02,
        -1.9178e-01,  1.7440e-01, -1.0789e-02,  9.4679e-03, -5.2677e-02,
         1.9473e-02, -1.0594e-02,  1.0600e-03,  1.9074e-02, -5.0988e-02,
        -6.7026e-02,  9.5600e-02, -3.1865e-04, -6.9576e-04, -6.9131e-02,
         4.8581e-02,  1.5376e-02, -7.7088e-02,  8.3973e-02,  1.3628e-03,
        -4.6487e-02, -5.9561e-02, -9.9165e-02, -2.3793e+00,  1.9493e+00,
         4.0861e-01, -3.6627e-02, -1.2015e-03, -5.2582e-01,  5.1692e-01,
        -3.6438e-02,  3.9864e-02, -5.4477e-02, -7.3412e-02,  1.1122e-01,
         1.2692e-02, -2.0952e-02,  2.4457e-02,  1.1902e-03, -5.5691e-02,
        -8.5444e-01, -1.0423e+00, -2.7239e+01], device='cuda:0',
       grad_fn=<SubBackward0>)
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 875, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 859, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 397, in train
    outputs = think_tuner_step(batch, model=model, tokenizer=tokenizer, train_config=train_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 405, in think_tuner_step
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 360, in start_thinking
    # packed = [{'configs': [{'rationale_no': r_no, '<s>_index': st_no, '<SoT>_index': sot_idx, '<EoT>_index': eot_idx, '</s>_index':ed_no},{...}]}]
                                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 141, in think_loss
    nll_signal = torch.mean(nll_signal)
                 ^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit

If you suspect this is an IPython 8.28.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True
