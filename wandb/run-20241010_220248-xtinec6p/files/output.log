Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.90s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                              [0m| 0/225 [00:00<?, ?it/s][0m
torch.Size([952])
tensor(1.0463, device='cuda:1', grad_fn=<MeanBackward0>)
The gate values are: tensor([[0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.9926, 0.9289, 0.5942, 0.7560,
         0.3404, 0.7251, 0.7384, 0.8450, 0.7460, 0.7719, 0.8586, 0.6299, 0.9828,
         0.2905, 0.0903, 0.9918, 0.2798, 0.9787, 0.9875, 0.6668, 0.6418, 0.9870,
         0.9626, 0.8570, 0.9639, 0.8348, 0.9138, 0.9790, 0.9621, 0.9060, 0.8976,
         0.9837, 0.5859, 0.9783, 0.3432, 0.0594, 0.9883, 0.7713, 0.9794, 0.9718,
         0.8391, 0.6033, 0.9850, 0.9121, 0.6823, 0.8562, 0.9881, 0.9637, 0.6404,
         0.1138, 0.9225, 0.9522, 0.8644, 0.9192, 0.5376, 0.2481, 0.9363, 0.6952,
         0.3954, 0.7328, 0.9230, 0.4523, 0.3782, 0.9524, 0.7054, 0.5086, 0.9791,
         0.9569, 0.9851, 0.9959, 0.9931, 0.9460, 0.6579, 0.9726, 0.7201, 0.8221,
         0.9984, 0.9808, 0.9235, 0.9862, 0.9326, 0.9914, 0.9158, 0.9816, 0.9756,
         0.4999, 0.2590, 0.3331, 0.5691, 0.6220, 0.9608, 0.7221, 0.9261, 0.9851,
         0.9649, 0.9614, 0.9090, 0.9713, 0.8002, 0.9786, 0.9678, 0.8113, 0.5550,
         0.6519, 0.6506, 0.9735, 0.9953, 0.8895, 0.9523, 0.9532, 0.7914, 0.2999,
         0.7432, 0.8640, 0.7552, 0.8002, 0.6494, 0.9817, 0.8727, 0.9563, 0.9982,
         0.9964, 0.8778, 0.8251, 0.9889, 0.9864, 0.7952, 0.9504, 0.9096, 0.8380,
         0.7536, 0.9876, 0.9803, 0.8263, 0.5637, 0.7309, 0.7652, 0.9725, 0.7667,
         0.9570, 0.9510, 0.9088, 0.9639, 0.9375, 0.9688, 0.8082, 0.9899, 0.9942,
         0.9654, 0.7445, 0.8564, 0.8042, 0.9893, 0.9970, 0.6549, 0.9770, 0.9880,
         0.9797, 0.6196, 0.6304, 0.8177, 0.7765, 0.9033, 0.7635, 0.9854, 0.9400,
         0.8595, 0.9834, 0.9929, 0.9477, 0.7570, 0.9873, 0.9940, 0.7991, 0.9729,
         0.9571, 0.4819, 0.4557, 0.9828, 0.9614, 0.6567, 0.2123, 0.9382, 0.6345,
         0.9317, 0.4526, 0.5370, 0.1250, 0.9000, 0.8068, 0.4332, 0.4254, 0.7760,
         0.9918, 0.9964, 0.9813, 0.3915, 0.9375, 0.9758, 0.9132, 0.9465, 0.9815,
         0.9498, 0.9549, 0.9416, 0.9317, 0.8662, 0.9937, 0.9959, 0.9763, 0.7049,
         0.9178, 0.9837, 0.9219, 0.8083, 0.9644, 0.9592, 0.7223, 0.4391, 0.0542,
         0.7036, 0.5315, 0.8733, 0.6658, 0.9702, 0.7769, 0.9711, 0.9772, 0.9572,
         0.8374, 0.6364, 0.8602, 0.8790, 0.6312],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.5847, 0.6144, 0.9766, 0.9945,
         0.9983, 0.8842, 0.5354, 0.9941, 0.7685, 0.9890, 0.1016, 0.1739, 0.8108,
         0.9896, 0.9433, 0.4292, 0.0600, 0.3558, 0.6000, 0.9854, 0.8856, 0.7769,
         0.9725, 0.9934, 0.4907, 0.9840, 0.7343, 0.9782, 0.9463, 0.9978, 0.8450,
         0.2214, 0.0810, 0.9728, 0.9955, 0.9768, 0.9657, 0.9827, 0.9918, 0.8809,
         0.2521, 0.3460, 0.1659, 0.9860, 0.9510, 0.6067, 0.6022, 0.3574, 0.1598,
         0.9320, 0.8264, 0.4307, 0.9363, 0.3811, 0.1897, 0.9896, 0.9962, 0.9051,
         0.8556, 0.6228, 0.1625, 0.4430, 0.9705, 0.9750, 0.9574, 0.5906, 0.9888,
         0.9107, 0.9870, 0.9935, 0.9953, 0.9250, 0.6621, 0.1260, 0.9957, 0.9852,
         0.7948, 0.9486, 0.9517, 0.8985, 0.9676, 0.9425, 0.9678, 0.9918, 0.9915,
         0.7471, 0.6096, 0.0908, 0.5299, 0.9741, 0.9039, 0.9628, 0.1854, 0.5727,
         0.4929, 0.2942, 0.2989, 0.8440, 0.9785, 0.6471, 0.1177, 0.1425, 0.9292,
         0.9957, 0.9949, 0.8245, 0.9954, 0.9882, 0.9733, 0.9868, 0.9876, 0.7906,
         0.5659, 0.0942, 0.9804, 0.9576, 0.9883, 0.9941, 0.9493, 0.9938, 0.9944,
         0.9099, 0.5689, 0.9988, 0.9961, 0.9930, 0.9993, 0.9956, 0.9843, 0.9920,
         0.9865, 0.8432, 0.6698, 0.1341, 0.4941, 0.9772, 0.7907, 0.4566, 0.1261,
         0.9364, 0.9553, 0.9393, 0.9767, 0.9896, 0.9788, 0.9906, 0.9822, 0.9897,
         0.9339, 0.6104, 0.1163, 0.7533, 0.9808, 0.9673, 0.9973, 0.9918, 0.9660,
         0.9596, 0.9581, 0.9769, 0.9904, 0.9922, 0.9009, 0.6722, 0.1557, 0.5905,
         0.9985, 0.8073, 0.2733, 0.0680, 0.2713, 0.9932, 0.9976, 0.9014, 0.9283,
         0.9581, 0.9592, 0.9130, 0.9870, 0.9194, 0.8017, 0.5097, 0.9794, 0.9968,
         0.7320, 0.9867, 0.9981, 0.9954, 0.9174, 0.9894, 0.9522, 0.9825, 0.9531,
         0.9771, 0.9869, 0.9923, 0.9947, 0.9972, 0.9418, 0.6890, 0.1975, 0.5107,
         0.9965, 0.9893, 0.7894, 0.9939, 0.9989, 0.9894, 0.9242, 0.9928, 0.9985,
         0.3465, 0.7865, 0.7351, 0.2290, 0.6320, 0.9677, 0.9964, 0.9831, 0.7173,
         0.9933, 0.3847, 0.4126, 0.1423, 0.3092, 0.9533, 0.9916, 0.9971, 0.9853,
         0.8737, 0.9974, 0.9897, 0.9156, 0.6389],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.3305, 0.0280, 0.5672, 0.8092,
         0.6215, 0.9633, 0.8799, 0.9556, 0.9916, 0.9838, 0.5355, 0.9902, 0.9912,
         0.9939, 0.9429, 0.8666, 0.0521, 0.1756, 0.6137, 0.9853, 0.9977, 0.7719,
         0.8858, 0.9569, 0.9917, 0.9836, 0.9661, 0.6498, 0.9776, 0.9486, 0.7442,
         0.9910, 0.9968, 0.9797, 0.9458, 0.9357, 0.7269, 0.9444, 0.8265, 0.6095,
         0.5857, 0.2266, 0.1936, 0.9408, 0.7750, 0.1966, 0.2736, 0.8279, 0.8801,
         0.9944, 0.9911, 0.8302, 0.9564, 0.8113, 0.6282, 0.9612, 0.9091, 0.9853,
         0.9839, 0.9789, 0.8149, 0.2396, 0.7102, 0.8958, 0.7901, 0.9658, 0.9858,
         0.6375, 0.9631, 0.7749, 0.9410, 0.9423, 0.6863, 0.1535, 0.1855, 0.6917,
         0.6439, 0.1389, 0.9094, 0.3699, 0.1247, 0.4158, 0.9811, 0.9792, 0.7531,
         0.3479, 0.7753, 0.6876, 0.9358, 0.2076, 0.3641, 0.9633, 0.9450, 0.9186,
         0.7141, 0.9796, 0.1862, 0.8510, 0.9016, 0.9674, 0.8012, 0.9749, 0.9071,
         0.7868, 0.9576, 0.9228, 0.9273, 0.7547, 0.9507, 0.3774, 0.9472, 0.9674,
         0.9760, 0.8202, 0.9572, 0.8847, 0.6011, 0.6023, 0.9376, 0.9885, 0.9145,
         0.9341, 0.8049, 0.9922, 0.3826, 0.9637, 0.9830, 0.9763, 0.8031, 0.9583,
         0.9183, 0.6710, 0.1659, 0.9907, 0.4529, 0.1816, 0.8118, 0.6230, 0.4254,
         0.0825, 0.9374, 0.9026, 0.4508, 0.6756, 0.9825, 0.9799, 0.9451, 0.6371,
         0.9034, 0.9590, 0.9576, 0.5909, 0.8218, 0.9795, 0.9729, 0.7767, 0.9852,
         0.2967, 0.9770, 0.9634, 0.9741, 0.8738, 0.9803, 0.9782, 0.6693, 0.9659,
         0.9884, 0.9678, 0.7782, 0.9531, 0.4348, 0.9022, 0.9272, 0.9575, 0.8754,
         0.9650, 0.9143, 0.4860, 0.8044, 0.9854, 0.9876, 0.9760, 0.8725, 0.9922,
         0.3984, 0.9232, 0.9295, 0.9648, 0.9284, 0.9774, 0.9528, 0.8160, 0.3754,
         0.9843, 0.4972, 0.0640, 0.4040, 0.4148, 0.4490, 0.0829, 0.9910, 0.9920,
         0.7503, 0.9802, 0.8709, 0.5967, 0.9801, 0.8815, 0.9093, 0.1641, 0.3499,
         0.8894, 0.5242, 0.9317, 0.9020, 0.9415, 0.3943, 0.6754, 0.7895, 0.9621,
         0.8247, 0.5032, 0.2262, 0.8903, 0.7156, 0.7286, 0.7693, 0.8611, 0.9320,
         0.9730, 0.7065, 0.7696, 0.7243, 0.6998],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.5675, 0.2509, 0.2103, 0.3392,
         0.6739, 0.2165, 0.3149, 0.0239, 0.1396, 0.6893, 0.6804, 0.8427, 0.9687,
         0.9633, 0.5067, 0.9810, 0.9394, 0.2906, 0.0692, 0.9823, 0.1124, 0.9661,
         0.3922, 0.9819, 0.3618, 0.1546, 0.5364, 0.3120, 0.4762, 0.9091, 0.9736,
         0.8265, 0.7247, 0.3096, 0.8037, 0.3171, 0.9420, 0.6382, 0.9679, 0.4975,
         0.8736, 0.6476, 0.8114, 0.9897, 0.7907, 0.8635, 0.9251, 0.5137, 0.9438,
         0.9299, 0.7015, 0.8057, 0.4403, 0.1470, 0.9267, 0.7426, 0.3629, 0.7282,
         0.9478, 0.5669, 0.8527, 0.7302, 0.8538, 0.9829, 0.7986, 0.7826, 0.9101,
         0.3074, 0.9683, 0.9175, 0.9751, 0.9834, 0.9892, 0.9793, 0.9015, 0.5151,
         0.2753, 0.7774, 0.3082, 0.2670, 0.9718, 0.9792, 0.9088, 0.8675, 0.6556,
         0.9247, 0.6324, 0.8862, 0.4696, 0.3703, 0.3680, 0.8206, 0.9683, 0.9840,
         0.7978, 0.9613, 0.9863, 0.8672, 0.9797, 0.9141, 0.9798, 0.9446, 0.9452,
         0.9731, 0.9887, 0.8550, 0.8789, 0.9781, 0.9727, 0.9826, 0.9907, 0.8761,
         0.7690, 0.9080, 0.5960, 0.5220, 0.9914, 0.9956, 0.6677, 0.7011, 0.5046,
         0.9761, 0.9927, 0.9043, 0.8311, 0.5968, 0.9946, 0.9897, 0.8676, 0.7513,
         0.8202, 0.9844, 0.9670, 0.8088, 0.9372, 0.9428, 0.7210, 0.9940, 0.9761,
         0.9034, 0.6771, 0.3767, 0.9679, 0.9770, 0.9909, 0.9969, 0.8739, 0.5953,
         0.1141, 0.8186, 0.4524, 0.4550, 0.9701, 0.9581, 0.6924, 0.9159, 0.8945,
         0.2973, 0.9471, 0.5390, 0.9766, 0.9469, 0.3592, 0.9590, 0.9346, 0.8907,
         0.9725, 0.9379, 0.9505, 0.9677, 0.9904, 0.8776, 0.6251, 0.5270, 0.9230,
         0.2272, 0.2994, 0.9920, 0.9630, 0.9483, 0.9858, 0.7819, 0.9946, 0.8486,
         0.8260, 0.5177, 0.9885, 0.9906, 0.9493, 0.7376, 0.8966, 0.6439, 0.8864,
         0.9832, 0.9410, 0.8761, 0.6749, 0.9939, 0.9917, 0.9604, 0.8905, 0.8347,
         0.7435, 0.9964, 0.9934, 0.9646, 0.9319, 0.9946, 0.9709, 0.9575, 0.6356,
         0.9281, 0.8706, 0.1043, 0.8401, 0.6813, 0.8374, 0.6764, 0.7232, 0.6997,
         0.8888, 0.9844, 0.7471, 0.8304, 0.9309, 0.7039, 0.9669, 0.9948, 0.9725,
         0.9836, 0.9691, 0.9697, 0.8969, 0.7029]], device='cuda:1',
       grad_fn=<SqueezeBackward1>)
The shape is: torch.Size([4, 239])
The shape of new_sequence: torch.Size([4, 239])
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([6])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3257)
idx:  5
Gate loss:  tensor(0.3233, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2933)
idx:  6
Gate loss:  tensor(0.5957, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1051, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3851)
idx:  7
Gate loss:  tensor(0.8245, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4603, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3940)
idx:  8
Gate loss:  tensor(1.1224, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3542)
idx:  10
Gate loss:  tensor(1.3792, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2811)
idx:  11
Gate loss:  tensor(1.5868, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3109)
idx:  12
Gate loss:  tensor(1.8495, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3205)
idx:  13
Gate loss:  tensor(2.0886, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3457)
idx:  14
Gate loss:  tensor(2.3554, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3146)
idx:  15
Gate loss:  tensor(2.6256, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3705)
idx:  16
Gate loss:  tensor(2.8589, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3500)
idx:  17
Gate loss:  tensor(3.2029, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3286)
idx:  20
Gate loss:  tensor(3.5287, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3063)
idx:  22
Gate loss:  tensor(3.8285, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2707)
idx:  23
Gate loss:  tensor(4.0958, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3183)
idx:  24
Gate loss:  tensor(4.3080, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3038)
idx:  25
Gate loss:  tensor(4.5030, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1884)
idx:  26
Gate loss:  tensor(4.6890, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1551)
idx:  27
Gate loss:  tensor(4.8383, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2065)
idx:  28
Gate loss:  tensor(5.0152, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2884)
idx:  29
Gate loss:  tensor(5.2932, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20970, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2493)
idx:  30
Gate loss:  tensor(5.5013, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2563)
idx:  31
Gate loss:  tensor(5.7355, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1815)
idx:  32
Gate loss:  tensor(5.9132, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2209)
idx:  33
Gate loss:  tensor(6.1257, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2110)
idx:  34
Gate loss:  tensor(6.3169, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2682)
idx:  35
Gate loss:  tensor(6.5577, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2273)
idx:  36
Gate loss:  tensor(6.7812, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2492)
idx:  37
Gate loss:  tensor(6.9273, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2100)
idx:  38
Gate loss:  tensor(7.1327, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3614)
idx:  41
Gate loss:  tensor(7.4898, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4912, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3219)
idx:  42
Gate loss:  tensor(7.7382, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2925)
idx:  43
Gate loss:  tensor(8.0246, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(373, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2512)
idx:  44
Gate loss:  tensor(8.2688, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3130)
idx:  45
Gate loss:  tensor(8.5314, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3605)
idx:  46
Gate loss:  tensor(8.7489, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2408)
idx:  47
Gate loss:  tensor(8.9861, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2182)
idx:  48
Gate loss:  tensor(9.1852, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3062)
idx:  49
Gate loss:  tensor(9.3941, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3204)
idx:  50
Gate loss:  tensor(9.6684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3860)
idx:  51
Gate loss:  tensor(10.0498, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7200, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4078)
idx:  52
Gate loss:  tensor(10.4428, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4038, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3382)
idx:  53
Gate loss:  tensor(10.6594, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2599)
idx:  55
Gate loss:  tensor(10.8991, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1794)
idx:  56
Gate loss:  tensor(11.0699, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1688)
idx:  57
Gate loss:  tensor(11.2159, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2497)
idx:  58
Gate loss:  tensor(11.4454, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3690)
idx:  59
Gate loss:  tensor(11.6438, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2397)
idx:  61
Gate loss:  tensor(11.8682, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2701)
idx:  62
Gate loss:  tensor(12.0559, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3518)
idx:  64
Gate loss:  tensor(12.3137, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3523)
idx:  65
Gate loss:  tensor(12.6389, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2684)
idx:  68
Gate loss:  tensor(12.8945, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3286)
idx:  69
Gate loss:  tensor(13.1263, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8267, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4335)
idx:  70
Gate loss:  tensor(13.3468, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4072)
idx:  71
Gate loss:  tensor(13.7455, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3014)
idx:  72
Gate loss:  tensor(14.0339, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2209)
idx:  73
Gate loss:  tensor(14.2514, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3661)
idx:  74
Gate loss:  tensor(14.6160, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3488)
idx:  75
Gate loss:  tensor(14.9624, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3337)
idx:  76
Gate loss:  tensor(15.2781, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3359)
idx:  77
Gate loss:  tensor(15.4990, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3242)
idx:  78
Gate loss:  tensor(15.8143, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1269, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3630)
idx:  79
Gate loss:  tensor(16.0757, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4021)
idx:  80
Gate loss:  tensor(16.4063, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3253)
idx:  81
Gate loss:  tensor(16.7310, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3409)
idx:  82
Gate loss:  tensor(17.0654, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4022)
idx:  83
Gate loss:  tensor(17.4369, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4978)
idx:  84
Gate loss:  tensor(17.9278, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4445)
idx:  85
Gate loss:  tensor(18.3424, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3965)
idx:  86
Gate loss:  tensor(18.7355, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8267, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4148)
idx:  87
Gate loss:  tensor(19.1154, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4532)
idx:  88
Gate loss:  tensor(19.5602, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2885)
idx:  89
Gate loss:  tensor(19.8417, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4306)
idx:  93
Gate loss:  tensor(20.0868, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4821)
idx:  94
Gate loss:  tensor(20.3866, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3991)
idx:  95
Gate loss:  tensor(20.7700, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4954)
idx:  96
Gate loss:  tensor(21.1278, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3735)
idx:  97
Gate loss:  tensor(21.4737, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4194)
idx:  98
Gate loss:  tensor(21.8869, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4523)
idx:  99
Gate loss:  tensor(22.3232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3813)
idx:  100
Gate loss:  tensor(22.6898, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4219)
idx:  101
Gate loss:  tensor(23.0733, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4283)
idx:  102
Gate loss:  tensor(23.4893, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4890)
idx:  103
Gate loss:  tensor(23.8806, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1644, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5364)
idx:  104
Gate loss:  tensor(24.4055, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3732)
idx:  105
Gate loss:  tensor(24.7667, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3514)
idx:  106
Gate loss:  tensor(25.0518, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3409)
idx:  107
Gate loss:  tensor(25.2410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4237)
idx:  108
Gate loss:  tensor(25.5172, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4467)
idx:  109
Gate loss:  tensor(25.8078, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3602)
idx:  110
Gate loss:  tensor(26.1585, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4969)
idx:  111
Gate loss:  tensor(26.6531, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4052)
idx:  112
Gate loss:  tensor(27.0135, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3879)
idx:  113
Gate loss:  tensor(27.3829, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2931)
idx:  114
Gate loss:  tensor(27.6623, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3708)
idx:  115
Gate loss:  tensor(27.9558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14772, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5253)
idx:  117
Gate loss:  tensor(28.3462, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4690)
idx:  118
Gate loss:  tensor(28.7514, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11137, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4640)
idx:  119
Gate loss:  tensor(29.1019, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11137, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3768)
idx:  120
Gate loss:  tensor(29.4034, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4831)
idx:  121
Gate loss:  tensor(29.7171, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3909)
idx:  122
Gate loss:  tensor(30.1009, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3681)
idx:  123
Gate loss:  tensor(30.4221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5249)
idx:  124
Gate loss:  tensor(30.9241, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6204)
idx:  125
Gate loss:  tensor(31.5434, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5882)
idx:  126
Gate loss:  tensor(32.1294, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6235)
idx:  127
Gate loss:  tensor(32.6767, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6449)
idx:  128
Gate loss:  tensor(33.2089, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6916)
idx:  129
Gate loss:  tensor(33.8927, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5092)
idx:  130
Gate loss:  tensor(34.3950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2975, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5837)
idx:  131
Gate loss:  tensor(34.8592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4306)
idx:  132
Gate loss:  tensor(35.2684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6392)
idx:  133
Gate loss:  tensor(35.8498, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6045)
idx:  134
Gate loss:  tensor(36.3564, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7085)
idx:  135
Gate loss:  tensor(36.8903, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5287)
idx:  136
Gate loss:  tensor(37.4125, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7569)
idx:  137
Gate loss:  tensor(38.1545, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3921)
idx:  138
Gate loss:  tensor(38.4785, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2831, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3418)
idx:  139
Gate loss:  tensor(38.6711, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4184)
idx:  140
Gate loss:  tensor(38.9769, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4018)
idx:  141
Gate loss:  tensor(39.2844, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(15090, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4127)
idx:  142
Gate loss:  tensor(39.6858, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4286)
idx:  143
Gate loss:  tensor(40.0144, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4592)
idx:  144
Gate loss:  tensor(40.4539, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4320)
idx:  145
Gate loss:  tensor(40.8647, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4485)
idx:  146
Gate loss:  tensor(41.2724, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4375)
idx:  147
Gate loss:  tensor(41.6941, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6171)
idx:  148
Gate loss:  tensor(42.2726, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7497)
idx:  149
Gate loss:  tensor(42.9989, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6506)
idx:  150
Gate loss:  tensor(43.5247, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6586)
idx:  151
Gate loss:  tensor(44.1767, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3464)
idx:  152
Gate loss:  tensor(44.5210, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3600)
idx:  153
Gate loss:  tensor(44.8685, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1576, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3529)
idx:  154
Gate loss:  tensor(45.1313, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(15090, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6041)
idx:  155
Gate loss:  tensor(45.6486, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6441)
idx:  156
Gate loss:  tensor(46.1666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4350)
idx:  157
Gate loss:  tensor(46.5970, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5040)
idx:  158
Gate loss:  tensor(47.0995, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6150)
idx:  159
Gate loss:  tensor(47.5022, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5678)
idx:  160
Gate loss:  tensor(48.0569, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4586)
idx:  161
Gate loss:  tensor(48.5100, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8521)
idx:  162
Gate loss:  tensor(49.3448, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5894, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5201)
idx:  163
Gate loss:  tensor(49.6670, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14772, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5829)
idx:  164
Gate loss:  tensor(50.0344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7187)
idx:  165
Gate loss:  tensor(50.6220, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6458)
idx:  166
Gate loss:  tensor(51.1235, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(15090, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7061)
idx:  167
Gate loss:  tensor(51.7613, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8202)
idx:  168
Gate loss:  tensor(52.3875, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6226)
idx:  169
Gate loss:  tensor(53.0010, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4723)
idx:  170
Gate loss:  tensor(53.4450, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7928)
idx:  171
Gate loss:  tensor(54.1264, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6881)
idx:  172
Gate loss:  tensor(54.8031, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5987)
idx:  173
Gate loss:  tensor(55.3975, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9101)
idx:  174
Gate loss:  tensor(56.2600, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8022)
idx:  175
Gate loss:  tensor(56.8673, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7832)
idx:  176
Gate loss:  tensor(57.6405, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8605)
idx:  177
Gate loss:  tensor(58.4958, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2975, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0280)
idx:  178
Gate loss:  tensor(59.3174, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7185)
idx:  179
Gate loss:  tensor(60.0164, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9055)
idx:  180
Gate loss:  tensor(60.8830, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6120)
idx:  183
Gate loss:  tensor(62.4673, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7349)
idx:  184
Gate loss:  tensor(63.1738, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7588)
idx:  185
Gate loss:  tensor(63.6721, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(862, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2787)
idx:  187
Gate loss:  tensor(64.8718, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(292, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0397)
idx:  188
Gate loss:  tensor(65.5315, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2553)
idx:  189
Gate loss:  tensor(66.7011, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(326, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2849)
idx:  191
Gate loss:  tensor(67.3911, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8862)
idx:  193
Gate loss:  tensor(68.1887, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9988)
idx:  194
Gate loss:  tensor(68.9946, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1566)
idx:  197
Gate loss:  tensor(69.8921, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9465)
idx:  198
Gate loss:  tensor(71.8226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6253)
idx:  199
Gate loss:  tensor(73.4420, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7621, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3104)
idx:  200
Gate loss:  tensor(74.7279, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5941)
idx:  202
Gate loss:  tensor(76.2224, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3325)
idx:  203
Gate loss:  tensor(77.5226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7513)
idx:  204
Gate loss:  tensor(79.1218, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9252)
idx:  205
Gate loss:  tensor(80.9440, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9738)
idx:  206
Gate loss:  tensor(82.8813, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5402)
idx:  207
Gate loss:  tensor(85.2941, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1550, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4719)
idx:  208
Gate loss:  tensor(86.6996, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2542)
idx:  209
Gate loss:  tensor(87.8805, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11137, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3467)
idx:  210
Gate loss:  tensor(89.1352, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7647)
idx:  211
Gate loss:  tensor(90.6638, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6020)
idx:  212
Gate loss:  tensor(92.2558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9987)
idx:  213
Gate loss:  tensor(94.2463, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1654)
idx:  214
Gate loss:  tensor(96.3603, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14772, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3778)
idx:  215
Gate loss:  tensor(98.0365, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3045)
idx:  216
Gate loss:  tensor(100.1515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1754)
idx:  217
Gate loss:  tensor(102.2914, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4654)
idx:  218
Gate loss:  tensor(104.5643, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2866)
idx:  219
Gate loss:  tensor(107.2209, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8293)
idx:  220
Gate loss:  tensor(111.8781, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5049)
idx:  221
Gate loss:  tensor(114.2807, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8803)
idx:  222
Gate loss:  tensor(115.6388, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8965)
idx:  225
Gate loss:  tensor(119.0838, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0158)
idx:  226
Gate loss:  tensor(120.6868, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7939)
idx:  227
Gate loss:  tensor(124.0001, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(15090, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.6873)
idx:  228
Gate loss:  tensor(128.4522, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(15090, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3895)
idx:  229
Gate loss:  tensor(132.7107, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.9291)
idx:  230
Gate loss:  tensor(137.3171, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.5535)
idx:  231
Gate loss:  tensor(142.7101, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.9371)
idx:  232
Gate loss:  tensor(150.4661, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7621, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.9968)
idx:  233
Gate loss:  tensor(163.8634, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.5147)
idx:  234
Gate loss:  tensor(178.5300, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14772, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.2736)
idx:  235
Gate loss:  tensor(188.8870, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-28.1738)
idx:  236
Gate loss:  tensor(213.1221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-21.0372)
idx:  237
Gate loss:  tensor(231.6146, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 209
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3352)
idx:  5
Gate loss:  tensor(231.8106, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3129)
idx:  6
Gate loss:  tensor(232.0028, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2616)
idx:  7
Gate loss:  tensor(232.2583, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16246, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2864)
idx:  8
Gate loss:  tensor(232.5431, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3213)
idx:  9
Gate loss:  tensor(232.8639, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3300)
idx:  10
Gate loss:  tensor(233.1557, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3593, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3016)
idx:  11
Gate loss:  tensor(233.3172, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2540)
idx:  12
Gate loss:  tensor(233.5697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2655)
idx:  13
Gate loss:  tensor(233.7737, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2319)
idx:  14
Gate loss:  tensor(234.0030, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2851)
idx:  17
Gate loss:  tensor(234.2342, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2577)
idx:  18
Gate loss:  tensor(234.4892, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1156, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2801)
idx:  19
Gate loss:  tensor(234.7534, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2911)
idx:  23
Gate loss:  tensor(234.9280, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2145)
idx:  24
Gate loss:  tensor(235.1394, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(769, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2045)
idx:  25
Gate loss:  tensor(235.3205, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(565, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1800)
idx:  26
Gate loss:  tensor(235.4604, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(892, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2435)
idx:  27
Gate loss:  tensor(235.6972, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2595)
idx:  28
Gate loss:  tensor(235.9550, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3522)
idx:  30
Gate loss:  tensor(236.3016, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8041, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2950)
idx:  31
Gate loss:  tensor(236.5182, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1546, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2519)
idx:  32
Gate loss:  tensor(236.7646, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3400)
idx:  33
Gate loss:  tensor(237.0863, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1434, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2773)
idx:  34
Gate loss:  tensor(237.3629, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1075, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2540)
idx:  35
Gate loss:  tensor(237.5776, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2268)
idx:  38
Gate loss:  tensor(237.7982, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(920, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2260)
idx:  39
Gate loss:  tensor(238.0233, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3431)
idx:  40
Gate loss:  tensor(238.3584, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3055)
idx:  41
Gate loss:  tensor(238.6533, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2908)
idx:  42
Gate loss:  tensor(238.9391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1434, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2686)
idx:  43
Gate loss:  tensor(239.2054, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2823)
idx:  44
Gate loss:  tensor(239.4541, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2544)
idx:  48
Gate loss:  tensor(239.7049, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2132)
idx:  49
Gate loss:  tensor(239.9077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1549)
idx:  50
Gate loss:  tensor(240.0017, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3267)
idx:  51
Gate loss:  tensor(240.1984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1793)
idx:  54
Gate loss:  tensor(240.3655, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1821)
idx:  55
Gate loss:  tensor(240.5161, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20260, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2692)
idx:  57
Gate loss:  tensor(240.7681, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2959)
idx:  60
Gate loss:  tensor(241.0609, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1492, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2180)
idx:  61
Gate loss:  tensor(241.2781, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1156, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2265)
idx:  62
Gate loss:  tensor(241.4832, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2742)
idx:  63
Gate loss:  tensor(241.7178, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4108)
idx:  64
Gate loss:  tensor(241.9736, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2966)
idx:  67
Gate loss:  tensor(242.2615, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2812)
idx:  68
Gate loss:  tensor(242.5357, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1643)
idx:  69
Gate loss:  tensor(242.6930, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2811)
idx:  70
Gate loss:  tensor(242.8590, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3264)
idx:  71
Gate loss:  tensor(243.1817, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2531)
idx:  72
Gate loss:  tensor(243.4122, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3472)
idx:  73
Gate loss:  tensor(243.7549, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3011)
idx:  74
Gate loss:  tensor(244.0540, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1434, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2685)
idx:  75
Gate loss:  tensor(244.3212, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20260, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2802)
idx:  76
Gate loss:  tensor(244.5804, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29895, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3717)
idx:  77
Gate loss:  tensor(244.8266, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2746)
idx:  79
Gate loss:  tensor(245.1000, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3154)
idx:  80
Gate loss:  tensor(245.4107, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3663)
idx:  81
Gate loss:  tensor(245.7019, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2871)
idx:  82
Gate loss:  tensor(245.9742, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2497)
idx:  83
Gate loss:  tensor(246.2118, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3537)
idx:  84
Gate loss:  tensor(246.5296, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3626)
idx:  85
Gate loss:  tensor(246.8805, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3644)
idx:  86
Gate loss:  tensor(247.2239, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3383)
idx:  87
Gate loss:  tensor(247.5513, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4050)
idx:  88
Gate loss:  tensor(247.9529, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1156, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3291)
idx:  89
Gate loss:  tensor(248.2793, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4074)
idx:  90
Gate loss:  tensor(248.5836, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4813)
idx:  91
Gate loss:  tensor(248.8770, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4380)
idx:  93
Gate loss:  tensor(249.1091, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4594)
idx:  94
Gate loss:  tensor(249.5566, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3536)
idx:  95
Gate loss:  tensor(249.8762, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3724)
idx:  96
Gate loss:  tensor(250.2348, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20260, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([248])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5754)
idx:  98
Gate loss:  tensor(250.5643, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4152)
idx:  102
Gate loss:  tensor(250.9148, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3426)
idx:  103
Gate loss:  tensor(251.2500, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2714)
idx:  104
Gate loss:  tensor(251.4256, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3157)
idx:  107
Gate loss:  tensor(251.7190, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3472)
idx:  108
Gate loss:  tensor(252.0648, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3614)
idx:  109
Gate loss:  tensor(252.4243, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3914)
idx:  110
Gate loss:  tensor(252.7470, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5433)
idx:  111
Gate loss:  tensor(253.2878, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3269)
idx:  112
Gate loss:  tensor(253.6109, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3050)
idx:  113
Gate loss:  tensor(253.9077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3195)
idx:  114
Gate loss:  tensor(254.2230, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1434, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3383)
idx:  115
Gate loss:  tensor(254.5571, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20260, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3720)
idx:  116
Gate loss:  tensor(254.8512, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29895, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5647)
idx:  117
Gate loss:  tensor(255.1708, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3404)
idx:  119
Gate loss:  tensor(255.5045, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3604)
idx:  120
Gate loss:  tensor(255.8497, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4535)
idx:  121
Gate loss:  tensor(256.2979, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4009)
idx:  122
Gate loss:  tensor(256.6964, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3545)
idx:  123
Gate loss:  tensor(257.0330, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4039)
idx:  124
Gate loss:  tensor(257.4344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3494)
idx:  125
Gate loss:  tensor(257.7818, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4685)
idx:  126
Gate loss:  tensor(258.2081, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4511)
idx:  127
Gate loss:  tensor(258.4647, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4093)
idx:  128
Gate loss:  tensor(258.8735, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5169)
idx:  129
Gate loss:  tensor(259.3884, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4960)
idx:  130
Gate loss:  tensor(259.8810, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4942)
idx:  131
Gate loss:  tensor(260.3748, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4553)
idx:  132
Gate loss:  tensor(260.8281, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3865)
idx:  133
Gate loss:  tensor(261.2085, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4447)
idx:  134
Gate loss:  tensor(261.6497, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1434, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4111)
idx:  135
Gate loss:  tensor(262.0552, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4239)
idx:  136
Gate loss:  tensor(262.4127, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9896)
idx:  137
Gate loss:  tensor(263.0755, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8524)
idx:  140
Gate loss:  tensor(263.9085, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3790)
idx:  141
Gate loss:  tensor(264.2082, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3193)
idx:  144
Gate loss:  tensor(264.5072, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4720)
idx:  145
Gate loss:  tensor(264.9581, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5805)
idx:  146
Gate loss:  tensor(265.5033, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5836)
idx:  147
Gate loss:  tensor(266.0734, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(920, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4290)
idx:  148
Gate loss:  tensor(266.4979, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6292)
idx:  149
Gate loss:  tensor(267.1137, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7048)
idx:  150
Gate loss:  tensor(267.8119, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5483)
idx:  151
Gate loss:  tensor(268.3504, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1156, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5178)
idx:  152
Gate loss:  tensor(268.8629, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6197)
idx:  153
Gate loss:  tensor(269.4416, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8914)
idx:  154
Gate loss:  tensor(269.9857, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6658)
idx:  156
Gate loss:  tensor(270.4873, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4700)
idx:  157
Gate loss:  tensor(270.9483, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7071)
idx:  158
Gate loss:  tensor(271.6322, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4605)
idx:  159
Gate loss:  tensor(272.0916, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6340)
idx:  160
Gate loss:  tensor(272.7204, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6839)
idx:  161
Gate loss:  tensor(273.3810, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6676)
idx:  162
Gate loss:  tensor(274.0216, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6205)
idx:  163
Gate loss:  tensor(274.6161, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4907)
idx:  164
Gate loss:  tensor(275.0954, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7389)
idx:  165
Gate loss:  tensor(275.8273, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1434, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5820)
idx:  166
Gate loss:  tensor(276.4047, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7613)
idx:  167
Gate loss:  tensor(277.0905, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0574)
idx:  168
Gate loss:  tensor(277.8014, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9769)
idx:  170
Gate loss:  tensor(278.3782, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7729)
idx:  171
Gate loss:  tensor(279.1501, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20260, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7358)
idx:  172
Gate loss:  tensor(279.7440, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3654, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9871)
idx:  176
Gate loss:  tensor(280.7245, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6715)
idx:  177
Gate loss:  tensor(281.3943, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6954)
idx:  178
Gate loss:  tensor(282.0212, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7451)
idx:  179
Gate loss:  tensor(282.7129, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0447)
idx:  180
Gate loss:  tensor(283.7139, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9187)
idx:  181
Gate loss:  tensor(284.5951, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9894)
idx:  182
Gate loss:  tensor(285.4983, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7873)
idx:  183
Gate loss:  tensor(286.2754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7326)
idx:  184
Gate loss:  tensor(286.9490, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6798)
idx:  185
Gate loss:  tensor(287.4940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8439, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8901)
idx:  186
Gate loss:  tensor(287.9477, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1902)
idx:  187
Gate loss:  tensor(289.1134, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9897)
idx:  188
Gate loss:  tensor(290.1000, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8851)
idx:  189
Gate loss:  tensor(290.7479, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1598)
idx:  190
Gate loss:  tensor(291.8923, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8041, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2165)
idx:  191
Gate loss:  tensor(293.1065, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8623)
idx:  192
Gate loss:  tensor(293.9648, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9208)
idx:  193
Gate loss:  tensor(294.8096, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7045)
idx:  194
Gate loss:  tensor(295.5067, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8053)
idx:  195
Gate loss:  tensor(296.2735, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0319)
idx:  196
Gate loss:  tensor(297.2874, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1002)
idx:  197
Gate loss:  tensor(298.3360, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1116)
idx:  198
Gate loss:  tensor(299.4223, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8041, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0486)
idx:  199
Gate loss:  tensor(300.4572, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2535)
idx:  200
Gate loss:  tensor(301.7011, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2996, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2732)
idx:  201
Gate loss:  tensor(302.9676, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1156, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1833)
idx:  202
Gate loss:  tensor(304.1475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4453)
idx:  203
Gate loss:  tensor(305.5087, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7109)
idx:  204
Gate loss:  tensor(306.6875, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8413)
idx:  206
Gate loss:  tensor(307.6280, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2921)
idx:  207
Gate loss:  tensor(308.9155, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6793)
idx:  208
Gate loss:  tensor(310.5769, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([242])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.6086)
idx:  209
Gate loss:  tensor(313.4254, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2826)
idx:  210
Gate loss:  tensor(316.6881, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6470)
idx:  211
Gate loss:  tensor(318.3333, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8367)
idx:  212
Gate loss:  tensor(320.1504, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([247])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3070)
idx:  213
Gate loss:  tensor(324.1309, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3730)
idx:  214
Gate loss:  tensor(327.4795, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0877)
idx:  215
Gate loss:  tensor(329.5641, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6323)
idx:  217
Gate loss:  tensor(330.8480, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1914)
idx:  218
Gate loss:  tensor(333.1939, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([249])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3654)
idx:  220
Gate loss:  tensor(335.9528, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8490)
idx:  221
Gate loss:  tensor(340.6453, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9418)
idx:  222
Gate loss:  tensor(343.5765, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7927)
idx:  223
Gate loss:  tensor(347.3051, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3227)
idx:  224
Gate loss:  tensor(349.6886, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.0321)
idx:  225
Gate loss:  tensor(354.6869, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3654, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.1920)
idx:  230
Gate loss:  tensor(362.4964, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.0571)
idx:  231
Gate loss:  tensor(373.4604, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([244])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.1011)
idx:  232
Gate loss:  tensor(381.5381, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.4917)
idx:  233
Gate loss:  tensor(397.7868, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([243])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.3506)
idx:  234
Gate loss:  tensor(408.5772, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-24.7942)
idx:  235
Gate loss:  tensor(433.3066, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-29.2545)
idx:  236
Gate loss:  tensor(462.2605, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.0227)
idx:  237
Gate loss:  tensor(487.0016, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 191
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2972)
idx:  7
Gate loss:  tensor(487.1702, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2954)
idx:  8
Gate loss:  tensor(487.4092, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3236)
idx:  9
Gate loss:  tensor(487.6104, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4134)
idx:  10
Gate loss:  tensor(488.0086, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3854)
idx:  11
Gate loss:  tensor(488.3477, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3272)
idx:  12
Gate loss:  tensor(488.6603, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2638)
idx:  13
Gate loss:  tensor(488.9219, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3445)
idx:  14
Gate loss:  tensor(489.2608, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3582)
idx:  15
Gate loss:  tensor(489.4526, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4037)
idx:  16
Gate loss:  tensor(489.8524, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6199, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3379)
idx:  17
Gate loss:  tensor(490.1873, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3423)
idx:  18
Gate loss:  tensor(490.5275, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2759)
idx:  19
Gate loss:  tensor(490.7877, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1183, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2364)
idx:  20
Gate loss:  tensor(490.9926, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(554, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3442)
idx:  23
Gate loss:  tensor(491.2038, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2607)
idx:  24
Gate loss:  tensor(491.4606, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2999)
idx:  25
Gate loss:  tensor(491.7599, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3550)
idx:  26
Gate loss:  tensor(492.0339, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3713)
idx:  27
Gate loss:  tensor(492.3628, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3757)
idx:  28
Gate loss:  tensor(492.7223, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5155, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3239)
idx:  29
Gate loss:  tensor(493.0435, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2539)
idx:  30
Gate loss:  tensor(493.2932, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3060)
idx:  31
Gate loss:  tensor(493.5889, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3087)
idx:  32
Gate loss:  tensor(493.7895, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3841, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3362)
idx:  33
Gate loss:  tensor(494.1181, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2908)
idx:  34
Gate loss:  tensor(494.3940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2405)
idx:  35
Gate loss:  tensor(494.5730, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3524)
idx:  36
Gate loss:  tensor(494.9222, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(901, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3672)
idx:  37
Gate loss:  tensor(495.2882, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5155, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3334)
idx:  38
Gate loss:  tensor(495.6149, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3025)
idx:  39
Gate loss:  tensor(495.9010, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3109, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3285)
idx:  40
Gate loss:  tensor(496.2084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3244)
idx:  41
Gate loss:  tensor(496.4442, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2275)
idx:  42
Gate loss:  tensor(496.6590, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1633)
idx:  43
Gate loss:  tensor(496.7940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2357)
idx:  44
Gate loss:  tensor(496.9377, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2575)
idx:  45
Gate loss:  tensor(497.0885, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2613)
idx:  48
Gate loss:  tensor(497.3343, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1733)
idx:  49
Gate loss:  tensor(497.4685, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3290)
idx:  52
Gate loss:  tensor(497.7409, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1058, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2718)
idx:  53
Gate loss:  tensor(497.9801, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3501)
idx:  54
Gate loss:  tensor(498.3283, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(901, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3353)
idx:  55
Gate loss:  tensor(498.6606, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3503)
idx:  56
Gate loss:  tensor(498.9514, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5960)
idx:  57
Gate loss:  tensor(499.5214, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2815)
idx:  58
Gate loss:  tensor(499.7498, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3602)
idx:  59
Gate loss:  tensor(499.9760, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3129)
idx:  60
Gate loss:  tensor(500.2768, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2663)
idx:  61
Gate loss:  tensor(500.5190, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2901)
idx:  62
Gate loss:  tensor(500.8048, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2139)
idx:  63
Gate loss:  tensor(501.0153, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3157)
idx:  64
Gate loss:  tensor(501.3243, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2823)
idx:  65
Gate loss:  tensor(501.5544, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3497)
idx:  67
Gate loss:  tensor(501.8027, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3616)
idx:  68
Gate loss:  tensor(502.1266, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5155, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3549)
idx:  69
Gate loss:  tensor(502.4070, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3168)
idx:  70
Gate loss:  tensor(502.7130, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2774)
idx:  71
Gate loss:  tensor(502.9865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4265)
idx:  72
Gate loss:  tensor(503.2584, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3506)
idx:  73
Gate loss:  tensor(503.5960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3504)
idx:  74
Gate loss:  tensor(503.8676, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2022, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3214)
idx:  75
Gate loss:  tensor(504.1700, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2629)
idx:  76
Gate loss:  tensor(504.4177, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1730)
idx:  77
Gate loss:  tensor(504.5364, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(382, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3239)
idx:  80
Gate loss:  tensor(504.7604, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(348, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5199)
idx:  81
Gate loss:  tensor(505.0952, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2904)
idx:  83
Gate loss:  tensor(505.3593, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4569)
idx:  87
Gate loss:  tensor(505.8076, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3970)
idx:  88
Gate loss:  tensor(506.1963, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3358)
idx:  89
Gate loss:  tensor(506.4491, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4366)
idx:  91
Gate loss:  tensor(506.7876, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4297)
idx:  92
Gate loss:  tensor(507.0830, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3030)
idx:  93
Gate loss:  tensor(507.3666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3353)
idx:  96
Gate loss:  tensor(507.6896, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2927)
idx:  97
Gate loss:  tensor(507.9663, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4122)
idx:  98
Gate loss:  tensor(508.3449, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3888)
idx:  99
Gate loss:  tensor(508.6225, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3198)
idx:  100
Gate loss:  tensor(508.9358, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6446)
idx:  102
Gate loss:  tensor(509.4843, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5516)
idx:  103
Gate loss:  tensor(509.9817, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4413)
idx:  104
Gate loss:  tensor(510.4086, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4909)
idx:  105
Gate loss:  tensor(510.8019, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5096)
idx:  106
Gate loss:  tensor(511.2987, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4641)
idx:  107
Gate loss:  tensor(511.7197, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5695)
idx:  108
Gate loss:  tensor(512.1678, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6120)
idx:  109
Gate loss:  tensor(512.7538, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3917)
idx:  110
Gate loss:  tensor(513.1154, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14990, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4179)
idx:  111
Gate loss:  tensor(513.5029, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3841, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3929)
idx:  112
Gate loss:  tensor(513.7994, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4827)
idx:  113
Gate loss:  tensor(514.2582, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5732)
idx:  115
Gate loss:  tensor(514.8011, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5896)
idx:  116
Gate loss:  tensor(515.3715, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5759)
idx:  117
Gate loss:  tensor(515.9337, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5266)
idx:  118
Gate loss:  tensor(516.3656, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4160)
idx:  119
Gate loss:  tensor(516.7639, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5115)
idx:  120
Gate loss:  tensor(517.2164, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5803)
idx:  121
Gate loss:  tensor(517.5652, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7667)
idx:  122
Gate loss:  tensor(518.0270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6909)
idx:  123
Gate loss:  tensor(518.6748, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4189)
idx:  124
Gate loss:  tensor(519.0889, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4761)
idx:  125
Gate loss:  tensor(519.5243, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4357)
idx:  126
Gate loss:  tensor(519.9313, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6996)
idx:  127
Gate loss:  tensor(520.4944, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3823)
idx:  128
Gate loss:  tensor(520.8737, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7316)
idx:  130
Gate loss:  tensor(521.5787, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6168)
idx:  131
Gate loss:  tensor(522.1850, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4444)
idx:  132
Gate loss:  tensor(522.6188, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7078)
idx:  133
Gate loss:  tensor(523.1873, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5545)
idx:  134
Gate loss:  tensor(523.7186, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5869)
idx:  135
Gate loss:  tensor(524.2576, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7082)
idx:  136
Gate loss:  tensor(524.7328, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9574)
idx:  138
Gate loss:  tensor(525.6813, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4800)
idx:  141
Gate loss:  tensor(526.0709, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29882, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5971)
idx:  142
Gate loss:  tensor(526.4429, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4369)
idx:  145
Gate loss:  tensor(526.8524, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3703)
idx:  146
Gate loss:  tensor(527.1866, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4458)
idx:  148
Gate loss:  tensor(527.4879, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5481)
idx:  149
Gate loss:  tensor(528.0264, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5384)
idx:  150
Gate loss:  tensor(528.5540, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5210)
idx:  151
Gate loss:  tensor(529.0464, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7428)
idx:  152
Gate loss:  tensor(529.5196, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8565)
idx:  153
Gate loss:  tensor(530.2933, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7466)
idx:  154
Gate loss:  tensor(531.0093, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0577)
idx:  155
Gate loss:  tensor(532.0222, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11536, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6856)
idx:  156
Gate loss:  tensor(532.4272, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3841, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5773)
idx:  157
Gate loss:  tensor(532.9016, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5621)
idx:  158
Gate loss:  tensor(533.4521, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6309)
idx:  159
Gate loss:  tensor(534.0660, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7153)
idx:  160
Gate loss:  tensor(534.6216, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4673)
idx:  161
Gate loss:  tensor(535.0820, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9737)
idx:  163
Gate loss:  tensor(536.0333, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5268)
idx:  164
Gate loss:  tensor(536.5409, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5351)
idx:  165
Gate loss:  tensor(537.0621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6069)
idx:  166
Gate loss:  tensor(537.5925, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5357)
idx:  167
Gate loss:  tensor(538.1176, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14990, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7438)
idx:  168
Gate loss:  tensor(538.8452, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5913)
idx:  169
Gate loss:  tensor(539.2410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6560)
idx:  170
Gate loss:  tensor(539.8745, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6214)
idx:  171
Gate loss:  tensor(540.4888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14990, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6846)
idx:  172
Gate loss:  tensor(541.1513, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3841, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6353)
idx:  173
Gate loss:  tensor(541.6457, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5752)
idx:  174
Gate loss:  tensor(542.1939, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9563)
idx:  176
Gate loss:  tensor(543.0566, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9011)
idx:  177
Gate loss:  tensor(543.8922, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8141)
idx:  178
Gate loss:  tensor(544.6717, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7790)
idx:  179
Gate loss:  tensor(545.3536, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7349)
idx:  180
Gate loss:  tensor(546.0627, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6567)
idx:  181
Gate loss:  tensor(546.6631, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9426)
idx:  183
Gate loss:  tensor(547.4214, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1164)
idx:  184
Gate loss:  tensor(548.5215, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7884)
idx:  185
Gate loss:  tensor(549.3002, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0570)
idx:  186
Gate loss:  tensor(550.3318, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8005)
idx:  187
Gate loss:  tensor(551.0302, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6666)
idx:  188
Gate loss:  tensor(551.6916, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0220)
idx:  190
Gate loss:  tensor(553.5582, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1517)
idx:  191
Gate loss:  tensor(554.6287, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9206)
idx:  192
Gate loss:  tensor(555.5168, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2707)
idx:  193
Gate loss:  tensor(556.6966, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9455)
idx:  194
Gate loss:  tensor(557.6207, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3392)
idx:  195
Gate loss:  tensor(558.8967, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5732)
idx:  196
Gate loss:  tensor(560.1804, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9447)
idx:  198
Gate loss:  tensor(561.1103, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3915)
idx:  205
Gate loss:  tensor(562.4892, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(901, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6634)
idx:  206
Gate loss:  tensor(564.1393, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1119)
idx:  207
Gate loss:  tensor(565.7239, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7540)
idx:  208
Gate loss:  tensor(567.4431, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.2324)
idx:  209
Gate loss:  tensor(571.1290, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5760)
idx:  210
Gate loss:  tensor(573.2628, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1204)
idx:  211
Gate loss:  tensor(576.3210, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.6819)
idx:  212
Gate loss:  tensor(580.4481, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(385, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1955)
idx:  213
Gate loss:  tensor(582.4445, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5207)
idx:  216
Gate loss:  tensor(584.6865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([248])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7246)
idx:  217
Gate loss:  tensor(586.6391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5664)
idx:  218
Gate loss:  tensor(589.9619, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8984)
idx:  219
Gate loss:  tensor(592.5764, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7484, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4016)
idx:  220
Gate loss:  tensor(595.7787, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.1091)
idx:  222
Gate loss:  tensor(599.2294, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9401, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.9930)
idx:  223
Gate loss:  tensor(603.1713, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1966)
idx:  224
Gate loss:  tensor(606.2467, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(382, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0003)
idx:  225
Gate loss:  tensor(609.5460, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(348, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.3904)
idx:  226
Gate loss:  tensor(613.2649, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.3622)
idx:  228
Gate loss:  tensor(619.8195, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.1384)
idx:  229
Gate loss:  tensor(625.6433, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6588, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.8580)
idx:  230
Gate loss:  tensor(629.9113, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([243])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.0646)
idx:  231
Gate loss:  tensor(636.8849, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.5795)
idx:  232
Gate loss:  tensor(652.0226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4828, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([245])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.0172)
idx:  233
Gate loss:  tensor(662.2911, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.0193)
idx:  234
Gate loss:  tensor(678.8516, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2462, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([244])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.3791)
idx:  235
Gate loss:  tensor(690.4238, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-19.8818)
idx:  236
Gate loss:  tensor(705.7249, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-25.0215)
idx:  237
Gate loss:  tensor(723.8488, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 192
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3188)
idx:  5
Gate loss:  tensor(724.0297, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(674, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3201)
idx:  9
Gate loss:  tensor(724.2454, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2898)
idx:  14
Gate loss:  tensor(724.4452, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2837)
idx:  15
Gate loss:  tensor(724.6382, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11855, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3284)
idx:  16
Gate loss:  tensor(724.9149, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2859)
idx:  17
Gate loss:  tensor(725.1918, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3252)
idx:  18
Gate loss:  tensor(725.5051, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3255)
idx:  19
Gate loss:  tensor(725.6700, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2910)
idx:  20
Gate loss:  tensor(725.9554, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2253)
idx:  21
Gate loss:  tensor(726.1671, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3409)
idx:  24
Gate loss:  tensor(726.5020, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2740)
idx:  26
Gate loss:  tensor(726.7668, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(769, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2717)
idx:  28
Gate loss:  tensor(727.0336, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2748)
idx:  31
Gate loss:  tensor(727.1810, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1983, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3282)
idx:  34
Gate loss:  tensor(727.4793, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1909)
idx:  35
Gate loss:  tensor(727.6652, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1207, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2364)
idx:  36
Gate loss:  tensor(727.8606, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2611)
idx:  37
Gate loss:  tensor(728.0498, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11137, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2741)
idx:  39
Gate loss:  tensor(728.2701, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2126)
idx:  41
Gate loss:  tensor(728.4704, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2309)
idx:  42
Gate loss:  tensor(728.6178, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2826)
idx:  43
Gate loss:  tensor(728.8913, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3250)
idx:  45
Gate loss:  tensor(729.1752, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2838)
idx:  46
Gate loss:  tensor(729.3589, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3098)
idx:  47
Gate loss:  tensor(729.6104, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2485)
idx:  48
Gate loss:  tensor(729.8563, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2940)
idx:  49
Gate loss:  tensor(730.0888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2910)
idx:  50
Gate loss:  tensor(730.3401, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2318)
idx:  51
Gate loss:  tensor(730.5545, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3503)
idx:  52
Gate loss:  tensor(730.7344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2140)
idx:  53
Gate loss:  tensor(730.9364, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1790)
idx:  54
Gate loss:  tensor(731.1028, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1717)
idx:  55
Gate loss:  tensor(731.2233, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2596)
idx:  56
Gate loss:  tensor(731.4325, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2165)
idx:  59
Gate loss:  tensor(731.6331, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2069)
idx:  60
Gate loss:  tensor(731.7867, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2842)
idx:  62
Gate loss:  tensor(731.9937, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2586)
idx:  63
Gate loss:  tensor(732.2388, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2705)
idx:  64
Gate loss:  tensor(732.3922, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3161)
idx:  65
Gate loss:  tensor(732.6616, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3075)
idx:  66
Gate loss:  tensor(732.8862, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3516)
idx:  67
Gate loss:  tensor(733.1863, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3384)
idx:  68
Gate loss:  tensor(733.5189, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3910)
idx:  69
Gate loss:  tensor(733.8312, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3294)
idx:  70
Gate loss:  tensor(734.0890, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3631)
idx:  71
Gate loss:  tensor(734.4194, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2880)
idx:  73
Gate loss:  tensor(734.6983, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(306, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3504)
idx:  74
Gate loss:  tensor(735.0198, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3447)
idx:  75
Gate loss:  tensor(735.3558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3475)
idx:  76
Gate loss:  tensor(735.6976, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3599)
idx:  77
Gate loss:  tensor(736.0536, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3486)
idx:  78
Gate loss:  tensor(736.3950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3692)
idx:  79
Gate loss:  tensor(736.7278, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3179)
idx:  80
Gate loss:  tensor(736.8916, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3608)
idx:  82
Gate loss:  tensor(737.1721, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1304, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3365)
idx:  85
Gate loss:  tensor(737.4991, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3443)
idx:  86
Gate loss:  tensor(737.8363, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1207, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3829)
idx:  87
Gate loss:  tensor(738.1843, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3298)
idx:  88
Gate loss:  tensor(738.4704, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3651)
idx:  89
Gate loss:  tensor(738.7097, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3431)
idx:  90
Gate loss:  tensor(739.0270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4367)
idx:  91
Gate loss:  tensor(739.3031, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3221)
idx:  92
Gate loss:  tensor(739.5885, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3968)
idx:  96
Gate loss:  tensor(739.9141, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6135)
idx:  97
Gate loss:  tensor(740.5082, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11137, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4748)
idx:  98
Gate loss:  tensor(740.9753, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5142)
idx:  99
Gate loss:  tensor(741.3856, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5644)
idx:  100
Gate loss:  tensor(741.9281, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4471)
idx:  101
Gate loss:  tensor(742.3691, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4477)
idx:  102
Gate loss:  tensor(742.7573, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4016)
idx:  103
Gate loss:  tensor(743.1508, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4068)
idx:  104
Gate loss:  tensor(743.5226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3315)
idx:  105
Gate loss:  tensor(743.8475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3380)
idx:  106
Gate loss:  tensor(744.1667, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3507)
idx:  107
Gate loss:  tensor(744.4982, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4356)
idx:  108
Gate loss:  tensor(744.9220, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3495)
idx:  109
Gate loss:  tensor(745.2675, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4395)
idx:  110
Gate loss:  tensor(745.6433, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4465)
idx:  111
Gate loss:  tensor(746.0357, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4289)
idx:  112
Gate loss:  tensor(746.4552, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3821)
idx:  113
Gate loss:  tensor(746.8269, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(769, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3852)
idx:  114
Gate loss:  tensor(747.2054, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3690)
idx:  115
Gate loss:  tensor(747.5710, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(916, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4206)
idx:  116
Gate loss:  tensor(747.9395, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4482)
idx:  117
Gate loss:  tensor(748.2841, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5511)
idx:  118
Gate loss:  tensor(748.7845, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4811)
idx:  119
Gate loss:  tensor(749.0712, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2753, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9660)
idx:  120
Gate loss:  tensor(749.5754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1304, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5390)
idx:  121
Gate loss:  tensor(750.1097, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5002)
idx:  122
Gate loss:  tensor(750.6077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4776)
idx:  123
Gate loss:  tensor(750.9266, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5584)
idx:  124
Gate loss:  tensor(751.3181, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5858)
idx:  125
Gate loss:  tensor(751.6136, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4591)
idx:  126
Gate loss:  tensor(752.0618, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([247])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6537)
idx:  127
Gate loss:  tensor(752.7108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5655)
idx:  128
Gate loss:  tensor(753.2221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6815)
idx:  129
Gate loss:  tensor(753.7885, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3997, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5501)
idx:  130
Gate loss:  tensor(754.1168, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5552)
idx:  131
Gate loss:  tensor(754.6689, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4478)
idx:  132
Gate loss:  tensor(755.1122, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4968)
idx:  133
Gate loss:  tensor(755.5432, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4451)
idx:  134
Gate loss:  tensor(755.8776, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6537)
idx:  135
Gate loss:  tensor(756.4137, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6893)
idx:  136
Gate loss:  tensor(757.0923, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4758)
idx:  137
Gate loss:  tensor(757.5524, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2975, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4947)
idx:  138
Gate loss:  tensor(757.9525, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5112)
idx:  139
Gate loss:  tensor(758.4316, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5677)
idx:  140
Gate loss:  tensor(758.9669, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6461)
idx:  141
Gate loss:  tensor(759.4327, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7884)
idx:  142
Gate loss:  tensor(760.2164, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6516)
idx:  143
Gate loss:  tensor(760.8524, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5547)
idx:  144
Gate loss:  tensor(761.3535, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5490)
idx:  145
Gate loss:  tensor(761.7253, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5320)
idx:  147
Gate loss:  tensor(762.2402, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4742)
idx:  148
Gate loss:  tensor(762.7034, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5490)
idx:  149
Gate loss:  tensor(763.2474, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(671, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6488)
idx:  150
Gate loss:  tensor(763.8942, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5184)
idx:  151
Gate loss:  tensor(764.3472, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6300)
idx:  152
Gate loss:  tensor(764.7223, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5917)
idx:  154
Gate loss:  tensor(765.2067, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5225)
idx:  157
Gate loss:  tensor(765.7136, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1207, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5898)
idx:  158
Gate loss:  tensor(766.2786, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5440)
idx:  159
Gate loss:  tensor(766.6553, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3827)
idx:  160
Gate loss:  tensor(767.0059, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6148)
idx:  161
Gate loss:  tensor(767.5557, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5733)
idx:  163
Gate loss:  tensor(768.0987, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5270)
idx:  164
Gate loss:  tensor(768.3827, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7143)
idx:  165
Gate loss:  tensor(769.0803, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7889)
idx:  166
Gate loss:  tensor(769.8273, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6916)
idx:  168
Gate loss:  tensor(770.4906, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9475, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7475)
idx:  169
Gate loss:  tensor(771.1893, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5186, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8071)
idx:  170
Gate loss:  tensor(771.9081, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7988)
idx:  171
Gate loss:  tensor(772.6849, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7173)
idx:  172
Gate loss:  tensor(773.3576, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6343)
idx:  173
Gate loss:  tensor(773.9605, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(565, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7721)
idx:  174
Gate loss:  tensor(774.7076, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(674, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6536)
idx:  175
Gate loss:  tensor(775.3550, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9168)
idx:  176
Gate loss:  tensor(776.1596, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8799)
idx:  177
Gate loss:  tensor(776.7096, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9787)
idx:  178
Gate loss:  tensor(777.2254, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1476)
idx:  179
Gate loss:  tensor(778.2846, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1304, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7026)
idx:  182
Gate loss:  tensor(778.9816, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9281)
idx:  183
Gate loss:  tensor(779.8754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0027)
idx:  184
Gate loss:  tensor(780.8262, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2245)
idx:  185
Gate loss:  tensor(782.0332, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1946)
idx:  186
Gate loss:  tensor(782.9673, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8528)
idx:  187
Gate loss:  tensor(783.8156, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0455)
idx:  188
Gate loss:  tensor(784.7028, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1037)
idx:  189
Gate loss:  tensor(785.6146, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1588)
idx:  190
Gate loss:  tensor(786.2145, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9062)
idx:  191
Gate loss:  tensor(787.1104, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8430)
idx:  192
Gate loss:  tensor(787.9454, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1793)
idx:  193
Gate loss:  tensor(789.0649, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8519)
idx:  194
Gate loss:  tensor(789.6933, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0883)
idx:  195
Gate loss:  tensor(790.6691, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0335)
idx:  196
Gate loss:  tensor(791.3345, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9131)
idx:  197
Gate loss:  tensor(792.1438, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9827)
idx:  198
Gate loss:  tensor(793.1100, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0800)
idx:  199
Gate loss:  tensor(794.1263, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0335)
idx:  200
Gate loss:  tensor(795.0317, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0426)
idx:  201
Gate loss:  tensor(795.7354, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9235)
idx:  202
Gate loss:  tensor(797.6472, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7086)
idx:  203
Gate loss:  tensor(799.3416, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6562)
idx:  204
Gate loss:  tensor(800.9323, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30646, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3076)
idx:  205
Gate loss:  tensor(802.0967, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3045)
idx:  206
Gate loss:  tensor(803.1855, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8782)
idx:  207
Gate loss:  tensor(804.5820, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9789)
idx:  208
Gate loss:  tensor(806.5538, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5407)
idx:  209
Gate loss:  tensor(808.0843, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4006)
idx:  210
Gate loss:  tensor(809.4354, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2826)
idx:  211
Gate loss:  tensor(811.5626, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([244])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2546)
idx:  212
Gate loss:  tensor(814.7997, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3510)
idx:  213
Gate loss:  tensor(817.0823, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4128)
idx:  214
Gate loss:  tensor(819.3926, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2975, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0016)
idx:  215
Gate loss:  tensor(820.6648, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1640)
idx:  216
Gate loss:  tensor(821.7451, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9241)
idx:  217
Gate loss:  tensor(823.4201, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([249])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3387)
idx:  219
Gate loss:  tensor(826.2249, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([249])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0327)
idx:  220
Gate loss:  tensor(828.2911, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2854)
idx:  221
Gate loss:  tensor(830.2049, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8145)
idx:  222
Gate loss:  tensor(832.1086, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7828)
idx:  223
Gate loss:  tensor(834.8444, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9865)
idx:  224
Gate loss:  tensor(836.9341, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4925)
idx:  225
Gate loss:  tensor(840.0383, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.2594)
idx:  226
Gate loss:  tensor(846.1999, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.0557)
idx:  227
Gate loss:  tensor(851.4710, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4943, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.0495)
idx:  228
Gate loss:  tensor(855.6641, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4725, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.4558)
idx:  229
Gate loss:  tensor(860.7427, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12841, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.4061)
idx:  230
Gate loss:  tensor(865.9557, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.6893)
idx:  231
Gate loss:  tensor(871.4567, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([249])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.0261)
idx:  232
Gate loss:  tensor(885.4097, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([249])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-19.4992)
idx:  233
Gate loss:  tensor(904.3732, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.9329)
idx:  234
Gate loss:  tensor(916.1108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-23.3551)
idx:  235
Gate loss:  tensor(938.7441, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([243])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-30.3054)
idx:  236
Gate loss:  tensor(968.1315, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-25.4804)
idx:  237
Gate loss:  tensor(990.9843, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 197
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 848, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 830, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 397, in train
    with open(f'samples.json', 'r') as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'samples.json'
