_wandb:
    value:
        cli_version: 0.18.3
        m: []
        python_version: 3.12.7
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 98
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 98
            "3":
                - 23
                - 55
            "4": 3.12.7
            "5": 0.18.3
            "6": 4.45.2
            "8":
                - 5
            "12": 0.18.3
            "13": linux-x86_64
batch_size_training:
    value: 4
batching_strategy:
    value: padding
context_length:
    value: 4096
flop_counter:
    value: false
flop_counter_start:
    value: 3
gamma:
    value: 0.85
gradient_accumulation_steps:
    value: 1
gradient_clipping:
    value: true
gradient_clipping_threshold:
    value: 1
lr:
    value: 0.0001
max_eval_step:
    value: 0
max_train_step:
    value: 0
mixed_precision:
    value: true
model_name:
    value: meta-llama/Llama-2-7b-hf
num_epochs:
    value: 1
num_workers_dataloader:
    value: 1
output_dir:
    value: /data/data/arrv/models/test1
profiler_dir:
    value: /scratch/ssaeidi1/aswin/model
run_validation:
    value: false
save_metrics:
    value: true
save_model:
    value: true
seed:
    value: 0
tokenizer_name:
    value: null
use_profiler:
    value: false
use_wandb:
    value: false
val_batch_size:
    value: 1
weight_decay:
    value: 0
