Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.08s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/225 [00:00<?, ?it/s]/data/data/arrv/ThinkTuning_v1/modeling_llama.py:1368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
Preparing 4D causal attention mask with cache position
torch.Size([1456])
tensor(1.0055, device='cuda:1', grad_fn=<MeanBackward0>)
The gate values are: tensor([[0.1958, 0.2726, 0.1058,  ..., 0.5566, 0.5289, 0.5058],
        [0.1958, 0.2726, 0.1058,  ..., 0.9099, 0.6731, 0.6622],
        [0.1958, 0.2726, 0.1058,  ..., 0.9837, 0.9624, 0.6133],
        [0.1958, 0.2726, 0.1058,  ..., 0.9944, 0.9566, 0.7390]],
       device='cuda:1', grad_fn=<SqueezeBackward1>)
The shape is: torch.Size([4, 365])
The shape of new_sequence: torch.Size([4, 365])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([73])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4185)
idx:  72
Gate loss:  tensor(0.3999, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4540)
idx:  73
Gate loss:  tensor(0.6978, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5437)
idx:  74
Gate loss:  tensor(1.2020, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5133)
idx:  75
Gate loss:  tensor(1.5877, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24231, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6327)
idx:  76
Gate loss:  tensor(2.2203, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5106)
idx:  77
Gate loss:  tensor(2.7270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5395)
idx:  78
Gate loss:  tensor(3.2641, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5194)
idx:  79
Gate loss:  tensor(3.7365, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4875)
idx:  80
Gate loss:  tensor(4.2048, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4588)
idx:  81
Gate loss:  tensor(4.6583, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5004)
idx:  82
Gate loss:  tensor(5.0954, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5229)
idx:  84
Gate loss:  tensor(5.6071, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4979)
idx:  85
Gate loss:  tensor(6.0960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4911)
idx:  86
Gate loss:  tensor(6.5660, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1827, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4687)
idx:  87
Gate loss:  tensor(7.0299, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5019)
idx:  88
Gate loss:  tensor(7.5297, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5127)
idx:  89
Gate loss:  tensor(8.0178, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5085)
idx:  90
Gate loss:  tensor(8.5209, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5213)
idx:  91
Gate loss:  tensor(9.0360, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5500)
idx:  92
Gate loss:  tensor(9.5744, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5514)
idx:  93
Gate loss:  tensor(10.0492, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5480)
idx:  95
Gate loss:  tensor(10.5951, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6293, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5276)
idx:  96
Gate loss:  tensor(10.8690, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5853)
idx:  97
Gate loss:  tensor(11.2809, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5771)
idx:  98
Gate loss:  tensor(11.8571, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4823)
idx:  99
Gate loss:  tensor(12.3390, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6674, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5148)
idx:  100
Gate loss:  tensor(12.8302, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5890, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5535)
idx:  101
Gate loss:  tensor(13.2584, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5744)
idx:  102
Gate loss:  tensor(13.8305, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5488)
idx:  103
Gate loss:  tensor(14.3529, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5783)
idx:  104
Gate loss:  tensor(14.9292, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4707)
idx:  105
Gate loss:  tensor(15.3996, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5355)
idx:  106
Gate loss:  tensor(15.9279, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5054)
idx:  107
Gate loss:  tensor(16.4199, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5172)
idx:  108
Gate loss:  tensor(16.9321, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5302)
idx:  109
Gate loss:  tensor(17.4616, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4777)
idx:  110
Gate loss:  tensor(17.8099, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6365)
idx:  113
Gate loss:  tensor(18.4458, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5630)
idx:  114
Gate loss:  tensor(19.0085, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5685)
idx:  115
Gate loss:  tensor(19.5736, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6118)
idx:  116
Gate loss:  tensor(20.1499, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5492)
idx:  118
Gate loss:  tensor(20.6963, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5455)
idx:  119
Gate loss:  tensor(21.2200, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6109)
idx:  120
Gate loss:  tensor(21.7775, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5435)
idx:  121
Gate loss:  tensor(22.3162, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7919)
idx:  122
Gate loss:  tensor(23.1067, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6826)
idx:  123
Gate loss:  tensor(23.7357, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(639, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6091)
idx:  125
Gate loss:  tensor(24.3418, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1867, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6110)
idx:  126
Gate loss:  tensor(24.9501, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2256, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6220)
idx:  127
Gate loss:  tensor(25.5044, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5224)
idx:  128
Gate loss:  tensor(26.0225, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5430)
idx:  129
Gate loss:  tensor(26.5548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6087)
idx:  130
Gate loss:  tensor(27.1196, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5916)
idx:  131
Gate loss:  tensor(27.6928, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6642)
idx:  132
Gate loss:  tensor(28.2803, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7285)
idx:  133
Gate loss:  tensor(29.0036, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6322)
idx:  134
Gate loss:  tensor(29.5281, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([374])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7136)
idx:  136
Gate loss:  tensor(30.2380, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5398)
idx:  137
Gate loss:  tensor(30.5489, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6956)
idx:  141
Gate loss:  tensor(30.9472, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4846, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6058)
idx:  142
Gate loss:  tensor(31.5496, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6878)
idx:  143
Gate loss:  tensor(32.2329, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6031)
idx:  144
Gate loss:  tensor(32.8336, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5896)
idx:  145
Gate loss:  tensor(33.3692, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24231, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5976)
idx:  146
Gate loss:  tensor(33.9663, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5990)
idx:  147
Gate loss:  tensor(34.5595, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6641)
idx:  148
Gate loss:  tensor(34.9978, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5978)
idx:  150
Gate loss:  tensor(35.5922, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10523, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6994)
idx:  151
Gate loss:  tensor(36.2643, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6439)
idx:  154
Gate loss:  tensor(36.9020, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4001, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6088)
idx:  155
Gate loss:  tensor(37.3168, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6435)
idx:  156
Gate loss:  tensor(37.9500, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(714, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6818)
idx:  157
Gate loss:  tensor(38.6264, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(920, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6229)
idx:  158
Gate loss:  tensor(39.2475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7046)
idx:  159
Gate loss:  tensor(39.9468, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6232)
idx:  160
Gate loss:  tensor(40.5659, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7322)
idx:  161
Gate loss:  tensor(41.2438, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10523, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6584)
idx:  163
Gate loss:  tensor(41.8887, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6533)
idx:  164
Gate loss:  tensor(42.5150, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5797)
idx:  165
Gate loss:  tensor(43.0617, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5906)
idx:  166
Gate loss:  tensor(43.6429, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5893)
idx:  167
Gate loss:  tensor(44.2312, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6291)
idx:  168
Gate loss:  tensor(44.8569, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7083)
idx:  169
Gate loss:  tensor(45.4628, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6107)
idx:  170
Gate loss:  tensor(46.0733, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7195)
idx:  171
Gate loss:  tensor(46.7809, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6460)
idx:  172
Gate loss:  tensor(47.3671, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6740)
idx:  173
Gate loss:  tensor(48.0241, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5308)
idx:  174
Gate loss:  tensor(48.5546, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5672)
idx:  175
Gate loss:  tensor(49.0606, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6425)
idx:  176
Gate loss:  tensor(49.5298, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24231, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6781)
idx:  178
Gate loss:  tensor(50.2078, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6592)
idx:  179
Gate loss:  tensor(50.8655, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7730)
idx:  180
Gate loss:  tensor(51.5439, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5949)
idx:  182
Gate loss:  tensor(52.1381, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7134)
idx:  183
Gate loss:  tensor(52.8384, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6882)
idx:  184
Gate loss:  tensor(53.4582, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7486)
idx:  185
Gate loss:  tensor(54.1928, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7995)
idx:  186
Gate loss:  tensor(54.9914, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8014)
idx:  187
Gate loss:  tensor(55.6958, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6747)
idx:  189
Gate loss:  tensor(56.3700, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1867, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7205)
idx:  190
Gate loss:  tensor(57.0844, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2256, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8206)
idx:  191
Gate loss:  tensor(57.8838, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7018)
idx:  192
Gate loss:  tensor(58.5814, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7776)
idx:  193
Gate loss:  tensor(59.2815, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7953)
idx:  194
Gate loss:  tensor(59.7393, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8358)
idx:  195
Gate loss:  tensor(60.4290, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8222)
idx:  196
Gate loss:  tensor(61.2476, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8394)
idx:  197
Gate loss:  tensor(61.9074, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9914)
idx:  199
Gate loss:  tensor(62.8960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7987)
idx:  200
Gate loss:  tensor(63.5039, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8848)
idx:  204
Gate loss:  tensor(64.0423, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4846, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8401)
idx:  205
Gate loss:  tensor(64.8806, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4846, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7712)
idx:  206
Gate loss:  tensor(65.6499, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3448, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7948)
idx:  207
Gate loss:  tensor(66.4342, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7712)
idx:  208
Gate loss:  tensor(67.2015, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7977)
idx:  209
Gate loss:  tensor(67.9201, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7850)
idx:  210
Gate loss:  tensor(68.7037, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9433)
idx:  211
Gate loss:  tensor(69.3584, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8297)
idx:  213
Gate loss:  tensor(70.1865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(435, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8083)
idx:  214
Gate loss:  tensor(70.9592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(578, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0222)
idx:  216
Gate loss:  tensor(71.4967, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9202)
idx:  217
Gate loss:  tensor(72.0574, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7735)
idx:  218
Gate loss:  tensor(72.8183, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1763, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7992)
idx:  219
Gate loss:  tensor(73.5254, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8348)
idx:  220
Gate loss:  tensor(74.2900, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8681)
idx:  221
Gate loss:  tensor(74.9994, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8870)
idx:  222
Gate loss:  tensor(75.8527, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8947)
idx:  223
Gate loss:  tensor(76.7372, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(23197, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9795)
idx:  224
Gate loss:  tensor(77.7084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8466)
idx:  225
Gate loss:  tensor(78.5510, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9231)
idx:  226
Gate loss:  tensor(79.4457, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9561)
idx:  227
Gate loss:  tensor(80.3763, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8677)
idx:  228
Gate loss:  tensor(81.2291, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9365)
idx:  229
Gate loss:  tensor(82.1299, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0303)
idx:  230
Gate loss:  tensor(82.8111, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0122)
idx:  232
Gate loss:  tensor(83.8180, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3448, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0448)
idx:  233
Gate loss:  tensor(84.8593, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8886)
idx:  234
Gate loss:  tensor(85.7457, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1658)
idx:  235
Gate loss:  tensor(86.8367, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0434)
idx:  236
Gate loss:  tensor(87.8377, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0825)
idx:  237
Gate loss:  tensor(88.9146, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0726)
idx:  238
Gate loss:  tensor(89.9822, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2648)
idx:  239
Gate loss:  tensor(91.2385, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1680)
idx:  240
Gate loss:  tensor(92.2813, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9921)
idx:  242
Gate loss:  tensor(93.2674, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0596)
idx:  243
Gate loss:  tensor(94.2341, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(714, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2601)
idx:  244
Gate loss:  tensor(95.4744, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(920, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0083)
idx:  245
Gate loss:  tensor(96.4789, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1506)
idx:  246
Gate loss:  tensor(97.6247, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0878)
idx:  247
Gate loss:  tensor(98.6895, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2235)
idx:  248
Gate loss:  tensor(99.8084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1954)
idx:  250
Gate loss:  tensor(100.9816, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2175, device='cuda:1')
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1399)
idx:  251
Gate loss:  tensor(102.1190, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1754)
idx:  252
Gate loss:  tensor(103.2901, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5056)
idx:  253
Gate loss:  tensor(104.7397, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1305)
idx:  254
Gate loss:  tensor(105.8202, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1841)
idx:  255
Gate loss:  tensor(106.8324, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1522)
idx:  256
Gate loss:  tensor(107.9414, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2924)
idx:  257
Gate loss:  tensor(109.0998, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2730)
idx:  259
Gate loss:  tensor(110.3695, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3448, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3264)
idx:  260
Gate loss:  tensor(111.6937, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3162)
idx:  261
Gate loss:  tensor(113.0074, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10523, device='cuda:1')
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3420)
idx:  262
Gate loss:  tensor(114.3303, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3240)
idx:  265
Gate loss:  tensor(115.6504, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(435, device='cuda:1')
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3281)
idx:  266
Gate loss:  tensor(116.9703, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(578, device='cuda:1')
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4961)
idx:  268
Gate loss:  tensor(118.1228, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3666)
idx:  269
Gate loss:  tensor(119.4183, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2901)
idx:  270
Gate loss:  tensor(120.7042, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2608)
idx:  271
Gate loss:  tensor(121.9167, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3588)
idx:  272
Gate loss:  tensor(123.1643, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5709)
idx:  273
Gate loss:  tensor(124.7231, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6079)
idx:  274
Gate loss:  tensor(126.3260, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6212)
idx:  275
Gate loss:  tensor(127.8031, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6180)
idx:  277
Gate loss:  tensor(129.4129, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4414)
idx:  278
Gate loss:  tensor(130.8424, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5458)
idx:  279
Gate loss:  tensor(132.2312, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5020)
idx:  280
Gate loss:  tensor(133.7301, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6299)
idx:  281
Gate loss:  tensor(135.1097, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5346)
idx:  283
Gate loss:  tensor(136.6389, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4846)
idx:  284
Gate loss:  tensor(138.1034, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7645)
idx:  285
Gate loss:  tensor(139.6840, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7384)
idx:  286
Gate loss:  tensor(141.0675, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6146)
idx:  287
Gate loss:  tensor(142.6752, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8308)
idx:  288
Gate loss:  tensor(144.0133, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2719, device='cuda:1')
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8663)
idx:  289
Gate loss:  tensor(144.9837, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6906)
idx:  290
Gate loss:  tensor(146.6704, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6180)
idx:  291
Gate loss:  tensor(148.0130, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0034)
idx:  293
Gate loss:  tensor(149.1910, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2175, device='cuda:1')
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9412)
idx:  295
Gate loss:  tensor(151.1286, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9546)
idx:  296
Gate loss:  tensor(153.0816, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1632)
idx:  297
Gate loss:  tensor(155.1997, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4614)
idx:  299
Gate loss:  tensor(157.2256, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0026)
idx:  300
Gate loss:  tensor(159.2262, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0567)
idx:  301
Gate loss:  tensor(161.2162, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2094)
idx:  302
Gate loss:  tensor(163.3564, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0357)
idx:  303
Gate loss:  tensor(165.3869, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2483)
idx:  304
Gate loss:  tensor(167.2621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0603)
idx:  306
Gate loss:  tensor(169.3189, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14990, device='cuda:1')
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3283)
idx:  307
Gate loss:  tensor(171.5748, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4751)
idx:  308
Gate loss:  tensor(173.9568, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1541)
idx:  309
Gate loss:  tensor(175.8728, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3340)
idx:  311
Gate loss:  tensor(178.2014, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3448, device='cuda:1')
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4989)
idx:  312
Gate loss:  tensor(180.6903, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2861)
idx:  313
Gate loss:  tensor(182.9737, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3662)
idx:  314
Gate loss:  tensor(185.0691, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6122)
idx:  315
Gate loss:  tensor(187.3535, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2175, device='cuda:1')
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4835)
idx:  317
Gate loss:  tensor(189.8321, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0195)
idx:  318
Gate loss:  tensor(192.8490, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9834)
idx:  319
Gate loss:  tensor(195.7865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0498)
idx:  321
Gate loss:  tensor(197.9620, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8265)
idx:  322
Gate loss:  tensor(200.7831, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8664)
idx:  323
Gate loss:  tensor(203.5691, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3801)
idx:  324
Gate loss:  tensor(206.6702, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8913)
idx:  325
Gate loss:  tensor(210.5117, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0053)
idx:  326
Gate loss:  tensor(214.1077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0583)
idx:  327
Gate loss:  tensor(218.1592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.1214)
idx:  328
Gate loss:  tensor(221.7067, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8985)
idx:  330
Gate loss:  tensor(225.6024, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8009)
idx:  331
Gate loss:  tensor(229.3520, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.5782)
idx:  332
Gate loss:  tensor(233.3268, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8087)
idx:  333
Gate loss:  tensor(237.9506, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
reasoning_path shape:  torch.Size([375])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.2878)
idx:  334
Gate loss:  tensor(244.2232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.4924)
idx:  335
Gate loss:  tensor(248.5922, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.7610)
idx:  337
Gate loss:  tensor(254.3453, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.4870)
idx:  338
Gate loss:  tensor(259.7418, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
reasoning_path shape:  torch.Size([372])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.4607)
idx:  339
Gate loss:  tensor(266.6969, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.1058)
idx:  341
Gate loss:  tensor(272.7770, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.5898)
idx:  342
Gate loss:  tensor(280.3948, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.5834)
idx:  344
Gate loss:  tensor(290.9431, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.5148)
idx:  345
Gate loss:  tensor(296.9828, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.9026)
idx:  347
Gate loss:  tensor(304.4939, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8378, device='cuda:1')
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.6295)
idx:  348
Gate loss:  tensor(313.0002, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(865, device='cuda:1')
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.2483)
idx:  350
Gate loss:  tensor(322.1972, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.0730)
idx:  351
Gate loss:  tensor(334.2349, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.4074)
idx:  352
Gate loss:  tensor(346.6322, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
reasoning_path shape:  torch.Size([374])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-19.0913)
idx:  353
Gate loss:  tensor(365.3028, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.5100)
idx:  354
Gate loss:  tensor(383.6837, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(282, device='cuda:1')
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
reasoning_path shape:  torch.Size([371])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-19.7009)
idx:  355
Gate loss:  tensor(403.3313, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3977, device='cuda:1')
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-22.9415)
idx:  356
Gate loss:  tensor(418.9471, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2175, device='cuda:1')
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-33.4656)
idx:  358
Gate loss:  tensor(452.3182, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
reasoning_path shape:  torch.Size([372])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-38.8078)
idx:  359
Gate loss:  tensor(490.7380, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-43.8452)
idx:  360
Gate loss:  tensor(529.7996, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-40.3069)
idx:  361
Gate loss:  tensor(552.9424, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-31.3602)
idx:  362
Gate loss:  tensor(570.3961, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-22.3876)
idx:  363
Gate loss:  tensor(582.2375, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 293
The shape of hidden_states: torch.Size([4096])
The topk tensor(17205, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2623)
idx:  71
Gate loss:  tensor(582.4822, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2167)
idx:  72
Gate loss:  tensor(582.6703, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2140)
idx:  73
Gate loss:  tensor(582.8703, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2618)
idx:  74
Gate loss:  tensor(583.1216, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2886)
idx:  75
Gate loss:  tensor(583.4018, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2588)
idx:  76
Gate loss:  tensor(583.5933, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1663, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2318)
idx:  77
Gate loss:  tensor(583.7444, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1791)
idx:  79
Gate loss:  tensor(583.9087, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(450, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2280)
idx:  80
Gate loss:  tensor(584.0336, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11855, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2521)
idx:  81
Gate loss:  tensor(584.2645, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3190)
idx:  82
Gate loss:  tensor(584.5820, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2737)
idx:  83
Gate loss:  tensor(584.8359, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8607, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3521)
idx:  84
Gate loss:  tensor(585.1827, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4275)
idx:  85
Gate loss:  tensor(585.6064, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2560)
idx:  86
Gate loss:  tensor(585.8616, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3081)
idx:  87
Gate loss:  tensor(586.1320, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9942, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2850)
idx:  88
Gate loss:  tensor(586.3039, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13471, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3111)
idx:  89
Gate loss:  tensor(586.5525, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2515)
idx:  90
Gate loss:  tensor(586.7997, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(450, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2439)
idx:  91
Gate loss:  tensor(586.9636, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2687)
idx:  92
Gate loss:  tensor(587.2069, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1492, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2817)
idx:  93
Gate loss:  tensor(587.4880, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17205, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([367])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3261)
idx:  94
Gate loss:  tensor(587.8065, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2889)
idx:  95
Gate loss:  tensor(588.0421, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2745)
idx:  96
Gate loss:  tensor(588.3093, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9942, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2454)
idx:  97
Gate loss:  tensor(588.5457, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13471, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2971)
idx:  98
Gate loss:  tensor(588.8144, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3226)
idx:  99
Gate loss:  tensor(589.1366, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3284)
idx:  101
Gate loss:  tensor(589.3822, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3043)
idx:  102
Gate loss:  tensor(589.6846, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3104)
idx:  103
Gate loss:  tensor(589.9947, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12833, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3100)
idx:  104
Gate loss:  tensor(590.3036, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2938)
idx:  105
Gate loss:  tensor(590.5968, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2933)
idx:  106
Gate loss:  tensor(590.8690, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1494, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2878)
idx:  107
Gate loss:  tensor(591.1335, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3506)
idx:  108
Gate loss:  tensor(591.4679, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2973)
idx:  109
Gate loss:  tensor(591.7507, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29934, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3717)
idx:  111
Gate loss:  tensor(591.9564, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3116)
idx:  112
Gate loss:  tensor(592.2563, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3204)
idx:  113
Gate loss:  tensor(592.5667, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4237)
idx:  114
Gate loss:  tensor(592.9479, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3422)
idx:  115
Gate loss:  tensor(593.1466, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2622)
idx:  116
Gate loss:  tensor(593.3994, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3168)
idx:  117
Gate loss:  tensor(593.6964, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3188)
idx:  118
Gate loss:  tensor(593.9337, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2661)
idx:  119
Gate loss:  tensor(594.1941, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2458)
idx:  120
Gate loss:  tensor(594.4068, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3179)
idx:  122
Gate loss:  tensor(594.6821, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1492, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3320)
idx:  123
Gate loss:  tensor(595.0069, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17205, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3646)
idx:  124
Gate loss:  tensor(595.3589, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3050)
idx:  126
Gate loss:  tensor(595.6309, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10163, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3121)
idx:  127
Gate loss:  tensor(595.9205, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3700)
idx:  128
Gate loss:  tensor(596.2771, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3699)
idx:  129
Gate loss:  tensor(596.5679, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3282)
idx:  130
Gate loss:  tensor(596.8937, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3052)
idx:  131
Gate loss:  tensor(597.1960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(27217, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3033)
idx:  132
Gate loss:  tensor(597.4885, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4048)
idx:  133
Gate loss:  tensor(597.8766, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3762)
idx:  134
Gate loss:  tensor(598.2505, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3587)
idx:  135
Gate loss:  tensor(598.5919, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3764)
idx:  136
Gate loss:  tensor(598.9639, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2896)
idx:  137
Gate loss:  tensor(599.2515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3578)
idx:  138
Gate loss:  tensor(599.5865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1492, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3145)
idx:  139
Gate loss:  tensor(599.8879, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10696, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3362)
idx:  140
Gate loss:  tensor(600.2159, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3235)
idx:  141
Gate loss:  tensor(600.5320, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3051)
idx:  142
Gate loss:  tensor(600.7723, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3937)
idx:  143
Gate loss:  tensor(601.1037, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1206, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3940)
idx:  144
Gate loss:  tensor(601.4191, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3777)
idx:  145
Gate loss:  tensor(601.7570, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2916)
idx:  146
Gate loss:  tensor(602.0170, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(505, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3834)
idx:  147
Gate loss:  tensor(602.3695, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3473)
idx:  148
Gate loss:  tensor(602.7090, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3273)
idx:  149
Gate loss:  tensor(602.9847, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3191)
idx:  150
Gate loss:  tensor(603.2856, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3295)
idx:  151
Gate loss:  tensor(603.5732, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3095)
idx:  152
Gate loss:  tensor(603.8467, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3474)
idx:  153
Gate loss:  tensor(604.1864, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4015)
idx:  154
Gate loss:  tensor(604.5825, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3684)
idx:  155
Gate loss:  tensor(604.9381, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3396)
idx:  156
Gate loss:  tensor(605.2591, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3730)
idx:  157
Gate loss:  tensor(605.4771, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3776)
idx:  158
Gate loss:  tensor(605.8527, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4200)
idx:  159
Gate loss:  tensor(606.2663, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4219)
idx:  160
Gate loss:  tensor(606.6785, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3892)
idx:  161
Gate loss:  tensor(607.0580, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4333)
idx:  162
Gate loss:  tensor(607.3729, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3639)
idx:  163
Gate loss:  tensor(607.7307, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3721)
idx:  164
Gate loss:  tensor(608.0951, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(450, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3365)
idx:  165
Gate loss:  tensor(608.3268, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4026)
idx:  166
Gate loss:  tensor(608.7252, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4515)
idx:  167
Gate loss:  tensor(609.0563, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4243)
idx:  168
Gate loss:  tensor(609.4064, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3864)
idx:  169
Gate loss:  tensor(609.7410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4575)
idx:  170
Gate loss:  tensor(610.0755, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10163, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4495)
idx:  171
Gate loss:  tensor(610.4807, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5342)
idx:  172
Gate loss:  tensor(610.9979, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4583)
idx:  173
Gate loss:  tensor(611.3076, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4264)
idx:  174
Gate loss:  tensor(611.7230, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4325)
idx:  175
Gate loss:  tensor(612.1353, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4236)
idx:  176
Gate loss:  tensor(612.5517, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3710)
idx:  177
Gate loss:  tensor(612.9180, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3964)
idx:  178
Gate loss:  tensor(613.2971, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(607, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3866)
idx:  179
Gate loss:  tensor(613.6555, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3943)
idx:  180
Gate loss:  tensor(613.9907, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3728)
idx:  181
Gate loss:  tensor(614.3143, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3253)
idx:  182
Gate loss:  tensor(614.6221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3886)
idx:  183
Gate loss:  tensor(614.8803, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(349, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3277)
idx:  184
Gate loss:  tensor(615.1493, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1541, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4820)
idx:  185
Gate loss:  tensor(615.5980, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(273, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6600)
idx:  188
Gate loss:  tensor(616.0369, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9185, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3720)
idx:  189
Gate loss:  tensor(616.3232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2886)
idx:  190
Gate loss:  tensor(616.5981, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5188)
idx:  192
Gate loss:  tensor(617.0893, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5209)
idx:  193
Gate loss:  tensor(617.5721, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4078)
idx:  194
Gate loss:  tensor(617.9780, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(289, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4673)
idx:  195
Gate loss:  tensor(618.4189, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4808)
idx:  196
Gate loss:  tensor(618.8634, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4341)
idx:  197
Gate loss:  tensor(619.2878, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4655)
idx:  198
Gate loss:  tensor(619.7501, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(274, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4711)
idx:  199
Gate loss:  tensor(620.1945, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4821)
idx:  200
Gate loss:  tensor(620.5773, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4696)
idx:  201
Gate loss:  tensor(620.9581, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3996)
idx:  202
Gate loss:  tensor(621.3445, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(988, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4444)
idx:  203
Gate loss:  tensor(621.7502, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4720)
idx:  204
Gate loss:  tensor(622.2065, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4548)
idx:  205
Gate loss:  tensor(622.6544, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5177)
idx:  206
Gate loss:  tensor(623.1442, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10163, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4740)
idx:  207
Gate loss:  tensor(623.6068, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5886)
idx:  208
Gate loss:  tensor(624.1893, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6431)
idx:  209
Gate loss:  tensor(624.6172, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8632)
idx:  210
Gate loss:  tensor(625.4607, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4151)
idx:  211
Gate loss:  tensor(625.8682, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5333)
idx:  212
Gate loss:  tensor(626.3669, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(289, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4911)
idx:  213
Gate loss:  tensor(626.8505, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5343)
idx:  214
Gate loss:  tensor(627.3779, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5035)
idx:  215
Gate loss:  tensor(627.7173, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(916, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5341)
idx:  216
Gate loss:  tensor(628.0602, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4864)
idx:  217
Gate loss:  tensor(628.5112, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5094)
idx:  218
Gate loss:  tensor(629.0078, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4084)
idx:  219
Gate loss:  tensor(629.4089, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4440)
idx:  220
Gate loss:  tensor(629.6452, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7046)
idx:  221
Gate loss:  tensor(630.0339, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5737)
idx:  223
Gate loss:  tensor(630.5449, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8232)
idx:  224
Gate loss:  tensor(631.0517, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1423, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4675)
idx:  225
Gate loss:  tensor(631.3849, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(565, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4238)
idx:  226
Gate loss:  tensor(631.7449, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5427)
idx:  227
Gate loss:  tensor(632.2391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6789)
idx:  228
Gate loss:  tensor(632.6685, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5534)
idx:  229
Gate loss:  tensor(633.1768, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6128)
idx:  230
Gate loss:  tensor(633.7788, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5044)
idx:  231
Gate loss:  tensor(634.2676, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4762)
idx:  232
Gate loss:  tensor(634.6953, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10163, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4259)
idx:  233
Gate loss:  tensor(635.0661, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5419)
idx:  234
Gate loss:  tensor(635.5977, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5179)
idx:  235
Gate loss:  tensor(635.9853, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4729)
idx:  236
Gate loss:  tensor(636.4498, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4329)
idx:  237
Gate loss:  tensor(636.7994, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4017)
idx:  238
Gate loss:  tensor(637.0532, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4523)
idx:  239
Gate loss:  tensor(637.3049, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5384)
idx:  240
Gate loss:  tensor(637.8402, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5295)
idx:  241
Gate loss:  tensor(638.3479, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6028)
idx:  242
Gate loss:  tensor(638.9495, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6211)
idx:  243
Gate loss:  tensor(639.5614, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5500)
idx:  244
Gate loss:  tensor(640.0390, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4350)
idx:  245
Gate loss:  tensor(640.4485, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5046)
idx:  246
Gate loss:  tensor(640.9406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5291)
idx:  247
Gate loss:  tensor(641.4672, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5946)
idx:  248
Gate loss:  tensor(642.0527, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5869)
idx:  249
Gate loss:  tensor(642.4822, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6365)
idx:  250
Gate loss:  tensor(643.0425, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7365)
idx:  251
Gate loss:  tensor(643.7147, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6412)
idx:  252
Gate loss:  tensor(644.3434, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6766)
idx:  253
Gate loss:  tensor(644.9473, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7157)
idx:  254
Gate loss:  tensor(645.5100, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5804)
idx:  255
Gate loss:  tensor(646.0784, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6299)
idx:  256
Gate loss:  tensor(646.7009, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6214)
idx:  257
Gate loss:  tensor(647.3006, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7846)
idx:  258
Gate loss:  tensor(648.0782, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6746)
idx:  259
Gate loss:  tensor(648.6243, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5606)
idx:  260
Gate loss:  tensor(649.1084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7068)
idx:  262
Gate loss:  tensor(649.7596, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7852)
idx:  263
Gate loss:  tensor(650.5276, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6362)
idx:  264
Gate loss:  tensor(651.1085, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6346)
idx:  265
Gate loss:  tensor(651.7407, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5877)
idx:  266
Gate loss:  tensor(652.2863, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7383)
idx:  267
Gate loss:  tensor(652.7227, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7696)
idx:  268
Gate loss:  tensor(653.4597, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7352)
idx:  269
Gate loss:  tensor(654.0132, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6868)
idx:  270
Gate loss:  tensor(654.6742, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6036)
idx:  273
Gate loss:  tensor(655.1293, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6867)
idx:  274
Gate loss:  tensor(655.6314, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8987)
idx:  275
Gate loss:  tensor(656.4829, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6816)
idx:  276
Gate loss:  tensor(657.1526, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7350)
idx:  277
Gate loss:  tensor(657.8646, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0563)
idx:  278
Gate loss:  tensor(658.8713, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8900)
idx:  279
Gate loss:  tensor(659.4622, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9638)
idx:  280
Gate loss:  tensor(660.3366, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29985, device='cuda:1')
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9073)
idx:  281
Gate loss:  tensor(661.2028, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7704)
idx:  282
Gate loss:  tensor(661.9147, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17205, device='cuda:1')
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9986)
idx:  284
Gate loss:  tensor(662.5582, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9134)
idx:  285
Gate loss:  tensor(663.4344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7323)
idx:  287
Gate loss:  tensor(664.1556, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8968)
idx:  288
Gate loss:  tensor(664.9800, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9542)
idx:  289
Gate loss:  tensor(665.9016, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1064)
idx:  290
Gate loss:  tensor(667.0007, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9214)
idx:  291
Gate loss:  tensor(667.8977, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0100)
idx:  292
Gate loss:  tensor(668.8705, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8950)
idx:  293
Gate loss:  tensor(669.6340, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10163, device='cuda:1')
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7747)
idx:  294
Gate loss:  tensor(670.3514, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1158)
idx:  295
Gate loss:  tensor(671.4296, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1790)
idx:  296
Gate loss:  tensor(672.1439, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9786)
idx:  297
Gate loss:  tensor(672.8660, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9393)
idx:  298
Gate loss:  tensor(673.3700, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0429)
idx:  299
Gate loss:  tensor(674.0391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9505)
idx:  301
Gate loss:  tensor(674.8741, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1505)
idx:  302
Gate loss:  tensor(675.9943, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(671, device='cuda:1')
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1013)
idx:  303
Gate loss:  tensor(677.0916, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1191)
idx:  304
Gate loss:  tensor(678.1639, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7063, device='cuda:1')
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1049)
idx:  305
Gate loss:  tensor(679.1522, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9689)
idx:  306
Gate loss:  tensor(680.0894, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1411)
idx:  307
Gate loss:  tensor(681.2153, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1628)
idx:  308
Gate loss:  tensor(682.2042, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9942, device='cuda:1')
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3234)
idx:  309
Gate loss:  tensor(683.3439, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13471, device='cuda:1')
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4057)
idx:  310
Gate loss:  tensor(684.6169, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1209)
idx:  311
Gate loss:  tensor(685.7310, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1457)
idx:  312
Gate loss:  tensor(686.7754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6417)
idx:  313
Gate loss:  tensor(688.2030, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29934, device='cuda:1')
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4359)
idx:  314
Gate loss:  tensor(689.4585, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4272)
idx:  315
Gate loss:  tensor(690.8525, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0451)
idx:  316
Gate loss:  tensor(691.8684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16368, device='cuda:1')
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4866)
idx:  317
Gate loss:  tensor(693.3517, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1509, device='cuda:1')
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6106)
idx:  318
Gate loss:  tensor(694.4730, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2092)
idx:  319
Gate loss:  tensor(695.6554, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4128)
idx:  320
Gate loss:  tensor(696.9601, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5035)
idx:  321
Gate loss:  tensor(697.9517, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3380)
idx:  322
Gate loss:  tensor(699.2751, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6674)
idx:  323
Gate loss:  tensor(700.5950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4188)
idx:  324
Gate loss:  tensor(701.9880, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3733)
idx:  325
Gate loss:  tensor(703.3259, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1713)
idx:  326
Gate loss:  tensor(705.3754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
reasoning_path shape:  torch.Size([372])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.9939)
idx:  327
Gate loss:  tensor(708.9518, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6368)
idx:  328
Gate loss:  tensor(710.5726, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3947)
idx:  329
Gate loss:  tensor(711.9635, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9306)
idx:  330
Gate loss:  tensor(713.8032, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4721)
idx:  331
Gate loss:  tensor(715.8746, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1083)
idx:  332
Gate loss:  tensor(717.9797, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29934, device='cuda:1')
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1385)
idx:  333
Gate loss:  tensor(719.7224, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9138)
idx:  334
Gate loss:  tensor(721.6196, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.9463)
idx:  335
Gate loss:  tensor(725.7399, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8159)
idx:  336
Gate loss:  tensor(728.0903, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2266)
idx:  337
Gate loss:  tensor(731.2917, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.7347)
idx:  338
Gate loss:  tensor(733.9969, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2241)
idx:  339
Gate loss:  tensor(735.7883, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9942, device='cuda:1')
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0189)
idx:  341
Gate loss:  tensor(738.3409, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0910)
idx:  342
Gate loss:  tensor(740.9405, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8590)
idx:  343
Gate loss:  tensor(743.8637, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11855, device='cuda:1')
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.7843)
idx:  344
Gate loss:  tensor(748.3625, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.4360)
idx:  345
Gate loss:  tensor(752.7906, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7952)
idx:  346
Gate loss:  tensor(756.0136, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3449, device='cuda:1')
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.6550)
idx:  347
Gate loss:  tensor(760.5840, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6762, device='cuda:1')
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.7374)
idx:  348
Gate loss:  tensor(765.0984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8607, device='cuda:1')
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.3169)
idx:  350
Gate loss:  tensor(770.3615, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.9240)
idx:  351
Gate loss:  tensor(776.1991, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.9159)
idx:  352
Gate loss:  tensor(782.8961, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1492, device='cuda:1')
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.6630)
idx:  353
Gate loss:  tensor(790.3824, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17205, device='cuda:1')
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.1201)
idx:  354
Gate loss:  tensor(799.3777, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.4947)
idx:  355
Gate loss:  tensor(807.7716, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.5423)
idx:  356
Gate loss:  tensor(822.1589, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.8728)
idx:  357
Gate loss:  tensor(837.5642, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.5105)
idx:  358
Gate loss:  tensor(853.6176, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
reasoning_path shape:  torch.Size([368])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-38.1743)
idx:  361
Gate loss:  tensor(890.0024, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-31.1472)
idx:  362
Gate loss:  tensor(918.3419, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-24.1615)
idx:  363
Gate loss:  tensor(934.6053, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 330
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1256)
idx:  71
Gate loss:  tensor(934.7226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1576, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2357)
idx:  72
Gate loss:  tensor(934.9024, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1674)
idx:  74
Gate loss:  tensor(935.0363, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2202)
idx:  75
Gate loss:  tensor(935.2509, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2327)
idx:  76
Gate loss:  tensor(935.4724, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1594)
idx:  77
Gate loss:  tensor(935.6276, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1485)
idx:  78
Gate loss:  tensor(935.7715, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1929)
idx:  79
Gate loss:  tensor(935.9310, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2485)
idx:  80
Gate loss:  tensor(936.1708, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2027)
idx:  81
Gate loss:  tensor(936.3568, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1956)
idx:  82
Gate loss:  tensor(936.5015, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3398, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2150)
idx:  83
Gate loss:  tensor(936.6344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2004)
idx:  84
Gate loss:  tensor(936.8337, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1340)
idx:  85
Gate loss:  tensor(936.9378, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2350)
idx:  86
Gate loss:  tensor(937.1168, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1963)
idx:  87
Gate loss:  tensor(937.3080, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2115)
idx:  88
Gate loss:  tensor(937.4911, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2344)
idx:  89
Gate loss:  tensor(937.7069, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3398, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2694)
idx:  90
Gate loss:  tensor(937.8578, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2280)
idx:  91
Gate loss:  tensor(938.0846, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1982)
idx:  92
Gate loss:  tensor(938.2762, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2259)
idx:  93
Gate loss:  tensor(938.5008, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6674, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1681)
idx:  94
Gate loss:  tensor(938.6525, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5890, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2644)
idx:  95
Gate loss:  tensor(938.8321, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2767)
idx:  96
Gate loss:  tensor(939.0931, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2548)
idx:  97
Gate loss:  tensor(939.2770, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2268)
idx:  98
Gate loss:  tensor(939.4959, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2159)
idx:  99
Gate loss:  tensor(939.6063, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1862)
idx:  101
Gate loss:  tensor(939.7910, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1676)
idx:  102
Gate loss:  tensor(939.9569, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3171, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2693)
idx:  103
Gate loss:  tensor(940.2200, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1417)
idx:  104
Gate loss:  tensor(940.3605, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(450, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1492)
idx:  105
Gate loss:  tensor(940.4885, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2395)
idx:  106
Gate loss:  tensor(940.6809, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1206, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2462)
idx:  107
Gate loss:  tensor(940.8497, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2201)
idx:  108
Gate loss:  tensor(941.0630, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1978)
idx:  109
Gate loss:  tensor(941.2548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2967, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1669)
idx:  110
Gate loss:  tensor(941.4019, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2506)
idx:  111
Gate loss:  tensor(941.6506, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2297)
idx:  112
Gate loss:  tensor(941.8796, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2165)
idx:  113
Gate loss:  tensor(942.0956, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3029)
idx:  114
Gate loss:  tensor(942.3731, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2977)
idx:  115
Gate loss:  tensor(942.6685, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6862, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2426)
idx:  116
Gate loss:  tensor(942.9091, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2372)
idx:  117
Gate loss:  tensor(943.1337, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1763)
idx:  118
Gate loss:  tensor(943.3091, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1799)
idx:  119
Gate loss:  tensor(943.4824, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3171, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2165)
idx:  120
Gate loss:  tensor(943.6881, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2159)
idx:  121
Gate loss:  tensor(943.9033, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2474)
idx:  122
Gate loss:  tensor(944.1499, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2289)
idx:  123
Gate loss:  tensor(944.3629, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3115)
idx:  124
Gate loss:  tensor(944.6312, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2933)
idx:  125
Gate loss:  tensor(944.9214, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1965)
idx:  126
Gate loss:  tensor(945.1133, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(450, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2191)
idx:  127
Gate loss:  tensor(945.2247, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2142)
idx:  128
Gate loss:  tensor(945.4053, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2405)
idx:  129
Gate loss:  tensor(945.5563, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2815)
idx:  130
Gate loss:  tensor(945.8300, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2525)
idx:  131
Gate loss:  tensor(946.0576, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2931)
idx:  132
Gate loss:  tensor(946.3314, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3398, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3095)
idx:  133
Gate loss:  tensor(946.5256, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2518)
idx:  134
Gate loss:  tensor(946.7740, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2859)
idx:  135
Gate loss:  tensor(947.0479, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2611)
idx:  136
Gate loss:  tensor(947.2799, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2977)
idx:  137
Gate loss:  tensor(947.5221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29963, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3165)
idx:  138
Gate loss:  tensor(947.7936, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3852)
idx:  139
Gate loss:  tensor(948.1410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3067)
idx:  140
Gate loss:  tensor(948.3776, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3398, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3875)
idx:  141
Gate loss:  tensor(948.5830, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3338)
idx:  142
Gate loss:  tensor(948.9138, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2757)
idx:  143
Gate loss:  tensor(949.1611, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4038, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3259)
idx:  144
Gate loss:  tensor(949.4606, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2353)
idx:  145
Gate loss:  tensor(949.6948, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3171, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1861)
idx:  146
Gate loss:  tensor(949.8749, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2196)
idx:  147
Gate loss:  tensor(950.0886, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2416)
idx:  148
Gate loss:  tensor(950.3037, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2727)
idx:  149
Gate loss:  tensor(950.5514, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2387)
idx:  150
Gate loss:  tensor(950.7391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3398, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3042)
idx:  151
Gate loss:  tensor(950.9543, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2414)
idx:  152
Gate loss:  tensor(951.1915, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2379)
idx:  153
Gate loss:  tensor(951.4214, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3259)
idx:  154
Gate loss:  tensor(951.7249, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3467)
idx:  155
Gate loss:  tensor(952.0579, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3235)
idx:  156
Gate loss:  tensor(952.3741, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30088, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2259)
idx:  157
Gate loss:  tensor(952.5621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13105, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1946)
idx:  158
Gate loss:  tensor(952.7551, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2613)
idx:  159
Gate loss:  tensor(953.0074, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3003)
idx:  160
Gate loss:  tensor(953.2869, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2945)
idx:  161
Gate loss:  tensor(953.5446, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2593)
idx:  162
Gate loss:  tensor(953.8022, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2369)
idx:  163
Gate loss:  tensor(954.0380, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24679, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2558)
idx:  164
Gate loss:  tensor(954.2375, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2519)
idx:  165
Gate loss:  tensor(954.4208, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2108)
idx:  166
Gate loss:  tensor(954.5662, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2548)
idx:  168
Gate loss:  tensor(954.8180, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3275)
idx:  169
Gate loss:  tensor(955.1325, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3760)
idx:  170
Gate loss:  tensor(955.3870, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3998)
idx:  171
Gate loss:  tensor(955.6337, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4069)
idx:  172
Gate loss:  tensor(956.0391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2957)
idx:  173
Gate loss:  tensor(956.3312, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3491)
idx:  174
Gate loss:  tensor(956.6237, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3857)
idx:  175
Gate loss:  tensor(957.0010, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2125)
idx:  176
Gate loss:  tensor(957.1840, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10454, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2379)
idx:  177
Gate loss:  tensor(957.3180, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2139)
idx:  178
Gate loss:  tensor(957.5215, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1235, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2360)
idx:  179
Gate loss:  tensor(957.7493, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2285)
idx:  180
Gate loss:  tensor(957.9359, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([367])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4515)
idx:  181
Gate loss:  tensor(958.3619, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2799)
idx:  182
Gate loss:  tensor(958.5850, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2415)
idx:  183
Gate loss:  tensor(958.7859, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2247)
idx:  184
Gate loss:  tensor(959.0099, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1476, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2273)
idx:  185
Gate loss:  tensor(959.2365, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2666)
idx:  186
Gate loss:  tensor(959.4999, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2780)
idx:  187
Gate loss:  tensor(959.7774, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3681)
idx:  188
Gate loss:  tensor(960.1155, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3109, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2157)
idx:  189
Gate loss:  tensor(960.3306, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3116)
idx:  190
Gate loss:  tensor(960.6295, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2210)
idx:  191
Gate loss:  tensor(960.8343, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2717)
idx:  192
Gate loss:  tensor(961.0775, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2518)
idx:  193
Gate loss:  tensor(961.3110, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2883)
idx:  194
Gate loss:  tensor(961.5515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2410)
idx:  195
Gate loss:  tensor(961.7672, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2086)
idx:  197
Gate loss:  tensor(961.9732, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1105, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2152)
idx:  198
Gate loss:  tensor(962.1629, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2194)
idx:  199
Gate loss:  tensor(962.3752, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2822)
idx:  200
Gate loss:  tensor(962.6298, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3748)
idx:  201
Gate loss:  tensor(962.9725, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2792)
idx:  202
Gate loss:  tensor(963.1983, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3721)
idx:  203
Gate loss:  tensor(963.5340, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2354)
idx:  204
Gate loss:  tensor(963.7688, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1521)
idx:  205
Gate loss:  tensor(963.9208, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1934)
idx:  206
Gate loss:  tensor(964.1057, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2067)
idx:  207
Gate loss:  tensor(964.2916, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24679, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2415)
idx:  208
Gate loss:  tensor(964.5141, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3726)
idx:  209
Gate loss:  tensor(964.7484, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2953)
idx:  210
Gate loss:  tensor(964.9651, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2335)
idx:  211
Gate loss:  tensor(965.1980, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16934, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2238)
idx:  212
Gate loss:  tensor(965.4097, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2903)
idx:  213
Gate loss:  tensor(965.6442, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13630, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1989)
idx:  214
Gate loss:  tensor(965.8000, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3398, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3528)
idx:  215
Gate loss:  tensor(965.9769, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2370)
idx:  216
Gate loss:  tensor(966.2132, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3064)
idx:  217
Gate loss:  tensor(966.4940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3170)
idx:  218
Gate loss:  tensor(966.7193, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2854)
idx:  219
Gate loss:  tensor(967.0032, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2943)
idx:  220
Gate loss:  tensor(967.2444, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5127)
idx:  221
Gate loss:  tensor(967.7527, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24679, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3105)
idx:  222
Gate loss:  tensor(968.0230, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3425)
idx:  223
Gate loss:  tensor(968.2204, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3094)
idx:  224
Gate loss:  tensor(968.4505, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2769)
idx:  225
Gate loss:  tensor(968.7250, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3865)
idx:  226
Gate loss:  tensor(969.0960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4343)
idx:  227
Gate loss:  tensor(969.4886, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5657)
idx:  228
Gate loss:  tensor(970.0209, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5477)
idx:  229
Gate loss:  tensor(970.5664, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3744)
idx:  230
Gate loss:  tensor(970.9381, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4392)
idx:  231
Gate loss:  tensor(971.3323, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2755)
idx:  232
Gate loss:  tensor(971.6068, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5403)
idx:  233
Gate loss:  tensor(972.1359, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3201)
idx:  234
Gate loss:  tensor(972.4315, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4539)
idx:  235
Gate loss:  tensor(972.8845, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2553)
idx:  236
Gate loss:  tensor(973.1191, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3044)
idx:  237
Gate loss:  tensor(973.4228, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24679, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3945)
idx:  238
Gate loss:  tensor(973.7584, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3985)
idx:  239
Gate loss:  tensor(973.9982, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3310)
idx:  240
Gate loss:  tensor(974.2657, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3982)
idx:  241
Gate loss:  tensor(974.6617, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5108)
idx:  242
Gate loss:  tensor(975.1697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5842)
idx:  243
Gate loss:  tensor(975.7059, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6447)
idx:  244
Gate loss:  tensor(976.3053, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6560)
idx:  245
Gate loss:  tensor(976.9451, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6151)
idx:  246
Gate loss:  tensor(977.5521, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5047)
idx:  247
Gate loss:  tensor(977.9503, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
torch.Size([249])
torch.Size([1])
torch.Size([249])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3382)
idx:  248
Gate loss:  tensor(978.2853, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5870)
idx:  249
Gate loss:  tensor(978.8100, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5946)
idx:  251
Gate loss:  tensor(979.2831, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4717)
idx:  252
Gate loss:  tensor(979.7172, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4245)
idx:  253
Gate loss:  tensor(979.9847, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6305)
idx:  254
Gate loss:  tensor(980.5676, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5121)
idx:  255
Gate loss:  tensor(981.0794, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1476, device='cuda:1')
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5370)
idx:  256
Gate loss:  tensor(981.6144, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1476, device='cuda:1')
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5382)
idx:  257
Gate loss:  tensor(982.1385, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6362)
idx:  258
Gate loss:  tensor(982.5107, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4665)
idx:  259
Gate loss:  tensor(982.9598, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4044)
idx:  260
Gate loss:  tensor(983.3453, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5231)
idx:  261
Gate loss:  tensor(983.8171, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6067)
idx:  262
Gate loss:  tensor(984.4095, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6238)
idx:  263
Gate loss:  tensor(984.9423, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6343)
idx:  264
Gate loss:  tensor(985.5259, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(967, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6020)
idx:  265
Gate loss:  tensor(986.0980, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6112)
idx:  266
Gate loss:  tensor(986.6865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5008)
idx:  267
Gate loss:  tensor(987.1844, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1105, device='cuda:1')
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4097)
idx:  268
Gate loss:  tensor(987.5072, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4901)
idx:  269
Gate loss:  tensor(987.9791, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2125, device='cuda:1')
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5445)
idx:  270
Gate loss:  tensor(988.4697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5053)
idx:  271
Gate loss:  tensor(988.9601, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4350)
idx:  272
Gate loss:  tensor(989.3889, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4816)
idx:  273
Gate loss:  tensor(989.8564, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5251)
idx:  274
Gate loss:  tensor(990.3600, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6386)
idx:  275
Gate loss:  tensor(990.9621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5395)
idx:  276
Gate loss:  tensor(991.4625, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6399)
idx:  277
Gate loss:  tensor(992.0474, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5795)
idx:  278
Gate loss:  tensor(992.6231, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4461)
idx:  279
Gate loss:  tensor(993.0252, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6742)
idx:  280
Gate loss:  tensor(993.6163, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3375)
idx:  281
Gate loss:  tensor(993.9503, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(769, device='cuda:1')
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5337)
idx:  282
Gate loss:  tensor(994.4205, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3113)
idx:  283
Gate loss:  tensor(994.7228, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4165)
idx:  284
Gate loss:  tensor(995.1103, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5317)
idx:  285
Gate loss:  tensor(995.5884, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24679, device='cuda:1')
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7201)
idx:  286
Gate loss:  tensor(996.2027, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5608)
idx:  287
Gate loss:  tensor(996.7565, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6082)
idx:  288
Gate loss:  tensor(997.3572, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5202)
idx:  289
Gate loss:  tensor(997.8619, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6850)
idx:  290
Gate loss:  tensor(998.3746, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4930)
idx:  291
Gate loss:  tensor(998.7416, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5481)
idx:  292
Gate loss:  tensor(999.2748, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5425)
idx:  293
Gate loss:  tensor(999.7933, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6795)
idx:  294
Gate loss:  tensor(1000.4579, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6197)
idx:  295
Gate loss:  tensor(1001.0704, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6163)
idx:  296
Gate loss:  tensor(1001.6666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6815)
idx:  297
Gate loss:  tensor(1002.2602, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7545)
idx:  298
Gate loss:  tensor(1002.9313, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8599)
idx:  299
Gate loss:  tensor(1003.7706, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5711)
idx:  300
Gate loss:  tensor(1004.3319, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6547)
idx:  301
Gate loss:  tensor(1004.8134, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8340)
idx:  302
Gate loss:  tensor(1005.6395, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
torch.Size([304])
torch.Size([1])
torch.Size([304])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7109)
idx:  303
Gate loss:  tensor(1006.3309, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13296, device='cuda:1')
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5737)
idx:  304
Gate loss:  tensor(1006.7994, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9017)
idx:  305
Gate loss:  tensor(1007.6849, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8953)
idx:  306
Gate loss:  tensor(1008.5646, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8913)
idx:  307
Gate loss:  tensor(1009.3121, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1787)
idx:  308
Gate loss:  tensor(1010.4661, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0876)
idx:  309
Gate loss:  tensor(1011.4728, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2625, device='cuda:1')
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3277)
idx:  310
Gate loss:  tensor(1012.6275, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1850)
idx:  311
Gate loss:  tensor(1013.8103, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3420)
idx:  312
Gate loss:  tensor(1015.0412, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1214)
idx:  313
Gate loss:  tensor(1016.0150, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1377)
idx:  314
Gate loss:  tensor(1017.1462, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3755)
idx:  315
Gate loss:  tensor(1018.4161, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(511, device='cuda:1')
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9910)
idx:  316
Gate loss:  tensor(1019.3797, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9895)
idx:  317
Gate loss:  tensor(1020.3289, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0551)
idx:  318
Gate loss:  tensor(1021.3660, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2581)
idx:  319
Gate loss:  tensor(1022.6057, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4520)
idx:  320
Gate loss:  tensor(1024.0031, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3876, device='cuda:1')
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2265)
idx:  321
Gate loss:  tensor(1025.2159, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1637)
idx:  322
Gate loss:  tensor(1026.3625, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5949)
idx:  323
Gate loss:  tensor(1027.7946, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7977, device='cuda:1')
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2743)
idx:  324
Gate loss:  tensor(1028.7072, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3746)
idx:  325
Gate loss:  tensor(1030.0535, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8150)
idx:  326
Gate loss:  tensor(1030.8579, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1196)
idx:  327
Gate loss:  tensor(1031.9584, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5835)
idx:  328
Gate loss:  tensor(1033.4956, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1384)
idx:  329
Gate loss:  tensor(1034.6174, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9849)
idx:  330
Gate loss:  tensor(1035.5952, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30562, device='cuda:1')
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9090)
idx:  331
Gate loss:  tensor(1037.4791, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(139, device='cuda:1')
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0625)
idx:  332
Gate loss:  tensor(1038.7949, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(158, device='cuda:1')
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1459)
idx:  333
Gate loss:  tensor(1040.5980, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9052)
idx:  334
Gate loss:  tensor(1042.1129, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0660)
idx:  335
Gate loss:  tensor(1042.7435, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4542)
idx:  336
Gate loss:  tensor(1044.9668, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2439)
idx:  337
Gate loss:  tensor(1047.1621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4883)
idx:  338
Gate loss:  tensor(1049.6232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30168, device='cuda:1')
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1883)
idx:  339
Gate loss:  tensor(1051.2599, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([341])
torch.Size([1])
torch.Size([341])
Inside custom generate sample func
torch.Size([341])
torch.Size([1])
torch.Size([341])
Inside custom generate sample func
torch.Size([341])
torch.Size([1])
torch.Size([341])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1545)
idx:  340
Gate loss:  tensor(1053.3799, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0656)
idx:  341
Gate loss:  tensor(1055.4314, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3812)
idx:  342
Gate loss:  tensor(1057.7087, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7433)
idx:  343
Gate loss:  tensor(1059.4392, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
torch.Size([345])
torch.Size([1])
torch.Size([345])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4727)
idx:  344
Gate loss:  tensor(1061.8978, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3915)
idx:  345
Gate loss:  tensor(1064.0482, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8685)
idx:  346
Gate loss:  tensor(1066.9080, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1777)
idx:  347
Gate loss:  tensor(1069.0671, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5468)
idx:  348
Gate loss:  tensor(1071.4995, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6511)
idx:  350
Gate loss:  tensor(1074.0363, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9119)
idx:  351
Gate loss:  tensor(1076.5806, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1663)
idx:  352
Gate loss:  tensor(1079.7091, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(697, device='cuda:1')
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3942)
idx:  353
Gate loss:  tensor(1082.9344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11155, device='cuda:1')
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.7099)
idx:  354
Gate loss:  tensor(1089.1106, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.0401)
idx:  355
Gate loss:  tensor(1099.1305, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.9038)
idx:  356
Gate loss:  tensor(1107.5214, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28704, device='cuda:1')
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
reasoning_path shape:  torch.Size([373])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.9888)
idx:  357
Gate loss:  tensor(1117.3427, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.5462)
idx:  358
Gate loss:  tensor(1123.8773, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.5193)
idx:  359
Gate loss:  tensor(1132.3906, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.8701)
idx:  360
Gate loss:  tensor(1148.2930, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-15.0708)
idx:  361
Gate loss:  tensor(1163.3123, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
reasoning_path shape:  torch.Size([368])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-37.8023)
idx:  362
Gate loss:  tensor(1200.4984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
reasoning_path shape:  torch.Size([370])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.3476)
idx:  363
Gate loss:  tensor(1226.8188, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 345
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1466)
idx:  71
Gate loss:  tensor(1226.9591, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1461)
idx:  72
Gate loss:  tensor(1227.0786, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2082)
idx:  73
Gate loss:  tensor(1227.2667, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2257)
idx:  74
Gate loss:  tensor(1227.3942, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1279)
idx:  76
Gate loss:  tensor(1227.5151, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1369)
idx:  77
Gate loss:  tensor(1227.6370, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28956, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2478)
idx:  78
Gate loss:  tensor(1227.7686, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(502, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2456)
idx:  79
Gate loss:  tensor(1227.9658, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2839)
idx:  80
Gate loss:  tensor(1228.1406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2125, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2538)
idx:  81
Gate loss:  tensor(1228.3564, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1960)
idx:  82
Gate loss:  tensor(1228.4932, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3116)
idx:  84
Gate loss:  tensor(1228.7765, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26204, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2610)
idx:  85
Gate loss:  tensor(1228.9968, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2786)
idx:  86
Gate loss:  tensor(1229.2529, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2158)
idx:  87
Gate loss:  tensor(1229.4639, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2715, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2417)
idx:  88
Gate loss:  tensor(1229.7041, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2016)
idx:  89
Gate loss:  tensor(1229.9032, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1779)
idx:  90
Gate loss:  tensor(1230.0735, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2125)
idx:  91
Gate loss:  tensor(1230.2633, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2178)
idx:  92
Gate loss:  tensor(1230.4749, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(988, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1774)
idx:  93
Gate loss:  tensor(1230.5968, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1060, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2030)
idx:  94
Gate loss:  tensor(1230.7751, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2271)
idx:  95
Gate loss:  tensor(1230.9846, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2490)
idx:  96
Gate loss:  tensor(1231.2162, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2179)
idx:  97
Gate loss:  tensor(1231.3892, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2645)
idx:  98
Gate loss:  tensor(1231.6398, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2000)
idx:  99
Gate loss:  tensor(1231.8313, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1958)
idx:  100
Gate loss:  tensor(1232.0127, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2022)
idx:  101
Gate loss:  tensor(1232.2120, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2796)
idx:  102
Gate loss:  tensor(1232.4888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2394)
idx:  103
Gate loss:  tensor(1232.7234, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2777)
idx:  104
Gate loss:  tensor(1232.9998, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1849)
idx:  105
Gate loss:  tensor(1233.1730, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13944, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2738)
idx:  108
Gate loss:  tensor(1233.3533, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2801)
idx:  109
Gate loss:  tensor(1233.6021, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2715, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2630)
idx:  110
Gate loss:  tensor(1233.8619, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2295)
idx:  111
Gate loss:  tensor(1234.0856, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1060, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2706)
idx:  112
Gate loss:  tensor(1234.3453, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1060, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2321)
idx:  113
Gate loss:  tensor(1234.5630, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4132)
idx:  114
Gate loss:  tensor(1234.9142, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18454, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2445)
idx:  115
Gate loss:  tensor(1235.1549, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2004)
idx:  116
Gate loss:  tensor(1235.3516, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(988, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1966)
idx:  117
Gate loss:  tensor(1235.5215, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2086)
idx:  118
Gate loss:  tensor(1235.6995, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25257, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2024)
idx:  119
Gate loss:  tensor(1235.8912, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1689)
idx:  120
Gate loss:  tensor(1236.0203, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1908)
idx:  121
Gate loss:  tensor(1236.1300, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2268)
idx:  122
Gate loss:  tensor(1236.3412, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2158)
idx:  123
Gate loss:  tensor(1236.5466, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1971)
idx:  124
Gate loss:  tensor(1236.7424, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2150)
idx:  125
Gate loss:  tensor(1236.9396, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2205)
idx:  126
Gate loss:  tensor(1237.1565, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3278, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2958)
idx:  130
Gate loss:  tensor(1237.4445, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2884)
idx:  131
Gate loss:  tensor(1237.7177, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2933)
idx:  132
Gate loss:  tensor(1237.9233, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3229, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2565)
idx:  134
Gate loss:  tensor(1238.1542, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2110)
idx:  135
Gate loss:  tensor(1238.3341, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2342)
idx:  136
Gate loss:  tensor(1238.4923, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2633)
idx:  137
Gate loss:  tensor(1238.7471, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2312)
idx:  138
Gate loss:  tensor(1238.9595, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(471, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2242)
idx:  139
Gate loss:  tensor(1239.1788, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25257, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2331)
idx:  140
Gate loss:  tensor(1239.4108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(368, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2634)
idx:  141
Gate loss:  tensor(1239.6729, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2669)
idx:  142
Gate loss:  tensor(1239.9326, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2620)
idx:  143
Gate loss:  tensor(1240.1371, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2645)
idx:  144
Gate loss:  tensor(1240.3986, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2704)
idx:  145
Gate loss:  tensor(1240.6259, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2686)
idx:  146
Gate loss:  tensor(1240.8282, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25187, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2621)
idx:  147
Gate loss:  tensor(1240.9750, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3028)
idx:  148
Gate loss:  tensor(1241.2362, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(471, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2029)
idx:  149
Gate loss:  tensor(1241.4338, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2828)
idx:  150
Gate loss:  tensor(1241.6505, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(471, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2374)
idx:  151
Gate loss:  tensor(1241.8828, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25257, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3232)
idx:  152
Gate loss:  tensor(1242.2043, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3112)
idx:  153
Gate loss:  tensor(1242.5110, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2428)
idx:  154
Gate loss:  tensor(1242.7396, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2779)
idx:  155
Gate loss:  tensor(1242.9785, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2730)
idx:  156
Gate loss:  tensor(1243.2412, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2479)
idx:  158
Gate loss:  tensor(1243.4476, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3020)
idx:  159
Gate loss:  tensor(1243.6152, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2682)
idx:  160
Gate loss:  tensor(1243.8319, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1641, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2939)
idx:  161
Gate loss:  tensor(1244.0930, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2715, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3089)
idx:  162
Gate loss:  tensor(1244.4008, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2844)
idx:  163
Gate loss:  tensor(1244.6775, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3097)
idx:  164
Gate loss:  tensor(1244.9786, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3412)
idx:  165
Gate loss:  tensor(1245.2455, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2983)
idx:  166
Gate loss:  tensor(1245.4983, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2746)
idx:  167
Gate loss:  tensor(1245.7616, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2851)
idx:  172
Gate loss:  tensor(1245.9753, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3113)
idx:  173
Gate loss:  tensor(1246.2159, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2533)
idx:  174
Gate loss:  tensor(1246.4119, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2897)
idx:  175
Gate loss:  tensor(1246.6942, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3234)
idx:  176
Gate loss:  tensor(1246.9907, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3085)
idx:  177
Gate loss:  tensor(1247.2814, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2828)
idx:  178
Gate loss:  tensor(1247.5439, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3130)
idx:  179
Gate loss:  tensor(1247.8524, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2794, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3255)
idx:  181
Gate loss:  tensor(1248.0717, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2478)
idx:  182
Gate loss:  tensor(1248.2528, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2844)
idx:  183
Gate loss:  tensor(1248.4531, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2262)
idx:  185
Gate loss:  tensor(1248.6055, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1641, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3011)
idx:  186
Gate loss:  tensor(1248.8955, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2591)
idx:  187
Gate loss:  tensor(1249.1157, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3128)
idx:  188
Gate loss:  tensor(1249.4172, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2576)
idx:  189
Gate loss:  tensor(1249.6735, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3308)
idx:  191
Gate loss:  tensor(1249.9968, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3356)
idx:  192
Gate loss:  tensor(1250.2881, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3698)
idx:  193
Gate loss:  tensor(1250.5541, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3986)
idx:  194
Gate loss:  tensor(1250.9050, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2917)
idx:  195
Gate loss:  tensor(1251.1761, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3978)
idx:  196
Gate loss:  tensor(1251.5614, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3435)
idx:  197
Gate loss:  tensor(1251.9023, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3521)
idx:  198
Gate loss:  tensor(1252.2521, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(23197, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3077)
idx:  199
Gate loss:  tensor(1252.5546, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3466)
idx:  200
Gate loss:  tensor(1252.8948, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10240, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2926)
idx:  201
Gate loss:  tensor(1253.0486, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3261)
idx:  202
Gate loss:  tensor(1253.3547, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3479)
idx:  203
Gate loss:  tensor(1253.6667, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2781)
idx:  205
Gate loss:  tensor(1253.9353, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2715, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4048)
idx:  206
Gate loss:  tensor(1254.3391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2918)
idx:  207
Gate loss:  tensor(1254.6240, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3823)
idx:  209
Gate loss:  tensor(1254.9281, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4009)
idx:  212
Gate loss:  tensor(1255.2286, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4262)
idx:  213
Gate loss:  tensor(1255.6368, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3575)
idx:  214
Gate loss:  tensor(1255.9893, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3559)
idx:  215
Gate loss:  tensor(1256.2349, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4468)
idx:  216
Gate loss:  tensor(1256.6730, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3790)
idx:  217
Gate loss:  tensor(1257.0391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3913)
idx:  219
Gate loss:  tensor(1257.2528, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4277)
idx:  220
Gate loss:  tensor(1257.6409, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3445)
idx:  221
Gate loss:  tensor(1257.9773, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3336)
idx:  222
Gate loss:  tensor(1258.2750, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3787)
idx:  223
Gate loss:  tensor(1258.6503, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4847)
idx:  224
Gate loss:  tensor(1259.0856, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4742)
idx:  225
Gate loss:  tensor(1259.5570, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5069)
idx:  226
Gate loss:  tensor(1260.0355, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4710)
idx:  227
Gate loss:  tensor(1260.4722, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(881, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4341)
idx:  228
Gate loss:  tensor(1260.9005, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4979)
idx:  229
Gate loss:  tensor(1261.3979, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1063, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5262)
idx:  230
Gate loss:  tensor(1261.9227, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3501)
idx:  231
Gate loss:  tensor(1262.2688, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5066)
idx:  232
Gate loss:  tensor(1262.7117, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4387)
idx:  233
Gate loss:  tensor(1263.1481, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4808)
idx:  235
Gate loss:  tensor(1263.5789, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4556)
idx:  236
Gate loss:  tensor(1264.0226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5153)
idx:  237
Gate loss:  tensor(1264.5359, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6205)
idx:  239
Gate loss:  tensor(1265.1542, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1552, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5550)
idx:  240
Gate loss:  tensor(1265.4587, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4119)
idx:  241
Gate loss:  tensor(1265.8488, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6150)
idx:  242
Gate loss:  tensor(1266.2587, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7778)
idx:  243
Gate loss:  tensor(1267.0347, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7729)
idx:  244
Gate loss:  tensor(1267.8054, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5204)
idx:  245
Gate loss:  tensor(1268.1788, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5111)
idx:  246
Gate loss:  tensor(1268.6841, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6887)
idx:  247
Gate loss:  tensor(1269.3588, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5923)
idx:  249
Gate loss:  tensor(1269.9192, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5387)
idx:  250
Gate loss:  tensor(1270.4550, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
torch.Size([252])
torch.Size([1])
torch.Size([252])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4951)
idx:  251
Gate loss:  tensor(1270.8898, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10240, device='cuda:1')
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
torch.Size([253])
torch.Size([1])
torch.Size([253])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6037)
idx:  252
Gate loss:  tensor(1271.2203, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5171)
idx:  253
Gate loss:  tensor(1271.6847, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
torch.Size([255])
torch.Size([1])
torch.Size([255])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4625)
idx:  254
Gate loss:  tensor(1272.1246, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
torch.Size([256])
torch.Size([1])
torch.Size([256])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4097)
idx:  255
Gate loss:  tensor(1272.5184, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
torch.Size([257])
torch.Size([1])
torch.Size([257])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5767)
idx:  256
Gate loss:  tensor(1273.0891, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10340, device='cuda:1')
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
torch.Size([258])
torch.Size([1])
torch.Size([258])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5433)
idx:  257
Gate loss:  tensor(1273.5986, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
torch.Size([259])
torch.Size([1])
torch.Size([259])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5986)
idx:  258
Gate loss:  tensor(1274.1901, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4652)
idx:  259
Gate loss:  tensor(1274.6235, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
torch.Size([261])
torch.Size([1])
torch.Size([261])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3313)
idx:  260
Gate loss:  tensor(1274.9280, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3636)
idx:  261
Gate loss:  tensor(1275.2645, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3997, device='cuda:1')
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
torch.Size([263])
torch.Size([1])
torch.Size([263])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4702)
idx:  262
Gate loss:  tensor(1275.5885, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
torch.Size([264])
torch.Size([1])
torch.Size([264])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6832)
idx:  263
Gate loss:  tensor(1276.2690, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6112)
idx:  264
Gate loss:  tensor(1276.8777, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4971)
idx:  265
Gate loss:  tensor(1277.3733, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
torch.Size([267])
torch.Size([1])
torch.Size([267])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5623)
idx:  266
Gate loss:  tensor(1277.9257, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
torch.Size([268])
torch.Size([1])
torch.Size([268])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5221)
idx:  267
Gate loss:  tensor(1278.3865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
torch.Size([269])
torch.Size([1])
torch.Size([269])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3703)
idx:  268
Gate loss:  tensor(1278.7539, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
torch.Size([270])
torch.Size([1])
torch.Size([270])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5797)
idx:  269
Gate loss:  tensor(1279.3280, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
torch.Size([271])
torch.Size([1])
torch.Size([271])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6299)
idx:  270
Gate loss:  tensor(1279.9508, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
torch.Size([272])
torch.Size([1])
torch.Size([272])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6601)
idx:  271
Gate loss:  tensor(1280.6052, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
torch.Size([273])
torch.Size([1])
torch.Size([273])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6749)
idx:  272
Gate loss:  tensor(1281.0706, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
torch.Size([274])
torch.Size([1])
torch.Size([274])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4585)
idx:  273
Gate loss:  tensor(1281.5162, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
torch.Size([275])
torch.Size([1])
torch.Size([275])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4615)
idx:  274
Gate loss:  tensor(1281.9330, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9190, device='cuda:1')
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
torch.Size([276])
torch.Size([1])
torch.Size([276])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4727)
idx:  275
Gate loss:  tensor(1282.2350, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
torch.Size([277])
torch.Size([1])
torch.Size([277])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5688)
idx:  276
Gate loss:  tensor(1282.7454, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
torch.Size([278])
torch.Size([1])
torch.Size([278])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5032)
idx:  277
Gate loss:  tensor(1283.2196, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
torch.Size([279])
torch.Size([1])
torch.Size([279])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5000)
idx:  278
Gate loss:  tensor(1283.7004, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
torch.Size([280])
torch.Size([1])
torch.Size([280])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5032)
idx:  279
Gate loss:  tensor(1284.1980, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25187, device='cuda:1')
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
torch.Size([281])
torch.Size([1])
torch.Size([281])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6466)
idx:  280
Gate loss:  tensor(1284.7877, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
torch.Size([282])
torch.Size([1])
torch.Size([282])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7031)
idx:  281
Gate loss:  tensor(1285.4702, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5177)
idx:  282
Gate loss:  tensor(1285.9717, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10240, device='cuda:1')
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
torch.Size([285])
torch.Size([1])
torch.Size([285])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5277)
idx:  284
Gate loss:  tensor(1286.4111, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25187, device='cuda:1')
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7487)
idx:  285
Gate loss:  tensor(1287.1338, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6378)
idx:  286
Gate loss:  tensor(1287.7233, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(471, device='cuda:1')
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
torch.Size([288])
torch.Size([1])
torch.Size([288])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5832)
idx:  287
Gate loss:  tensor(1288.2854, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7567)
idx:  288
Gate loss:  tensor(1289.0387, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
torch.Size([290])
torch.Size([1])
torch.Size([290])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9104)
idx:  289
Gate loss:  tensor(1289.7814, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
torch.Size([291])
torch.Size([1])
torch.Size([291])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8010)
idx:  290
Gate loss:  tensor(1290.5775, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(541, device='cuda:1')
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
torch.Size([292])
torch.Size([1])
torch.Size([292])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5798)
idx:  291
Gate loss:  tensor(1291.1443, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(372, device='cuda:1')
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6382)
idx:  292
Gate loss:  tensor(1291.7592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(881, device='cuda:1')
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
torch.Size([294])
torch.Size([1])
torch.Size([294])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6589)
idx:  293
Gate loss:  tensor(1292.4135, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(505, device='cuda:1')
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9485)
idx:  294
Gate loss:  tensor(1293.3615, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1063, device='cuda:1')
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
torch.Size([296])
torch.Size([1])
torch.Size([296])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7962)
idx:  295
Gate loss:  tensor(1294.1561, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7006)
idx:  296
Gate loss:  tensor(1294.8546, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
torch.Size([298])
torch.Size([1])
torch.Size([298])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9602)
idx:  297
Gate loss:  tensor(1295.6477, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
torch.Size([299])
torch.Size([1])
torch.Size([299])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8691)
idx:  298
Gate loss:  tensor(1296.5128, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1105, device='cuda:1')
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7482)
idx:  299
Gate loss:  tensor(1297.0474, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6979)
idx:  300
Gate loss:  tensor(1297.7325, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(23197, device='cuda:1')
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
torch.Size([302])
torch.Size([1])
torch.Size([302])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9769)
idx:  301
Gate loss:  tensor(1298.6979, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
torch.Size([303])
torch.Size([1])
torch.Size([303])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7907)
idx:  302
Gate loss:  tensor(1299.4788, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
torch.Size([305])
torch.Size([1])
torch.Size([305])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0139)
idx:  304
Gate loss:  tensor(1300.4883, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7597)
idx:  305
Gate loss:  tensor(1301.2460, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18103, device='cuda:1')
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
torch.Size([307])
torch.Size([1])
torch.Size([307])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8499)
idx:  306
Gate loss:  tensor(1301.9188, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4959)
idx:  307
Gate loss:  tensor(1302.4111, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
torch.Size([309])
torch.Size([1])
torch.Size([309])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2351)
idx:  308
Gate loss:  tensor(1303.2994, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
torch.Size([310])
torch.Size([1])
torch.Size([310])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3299)
idx:  309
Gate loss:  tensor(1304.6241, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
torch.Size([311])
torch.Size([1])
torch.Size([311])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2267)
idx:  310
Gate loss:  tensor(1305.8455, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
torch.Size([312])
torch.Size([1])
torch.Size([312])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0688)
idx:  311
Gate loss:  tensor(1306.6088, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4055)
idx:  312
Gate loss:  tensor(1308.0093, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
torch.Size([314])
torch.Size([1])
torch.Size([314])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2496)
idx:  313
Gate loss:  tensor(1309.2498, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
torch.Size([315])
torch.Size([1])
torch.Size([315])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2304)
idx:  314
Gate loss:  tensor(1310.4413, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
torch.Size([316])
torch.Size([1])
torch.Size([316])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8304)
idx:  315
Gate loss:  tensor(1311.1681, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
torch.Size([317])
torch.Size([1])
torch.Size([317])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3996)
idx:  316
Gate loss:  tensor(1312.5621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
torch.Size([318])
torch.Size([1])
torch.Size([318])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2293)
idx:  317
Gate loss:  tensor(1313.7760, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
torch.Size([319])
torch.Size([1])
torch.Size([319])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2141)
idx:  318
Gate loss:  tensor(1314.9812, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
torch.Size([320])
torch.Size([1])
torch.Size([320])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0485)
idx:  319
Gate loss:  tensor(1315.6548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8324)
idx:  320
Gate loss:  tensor(1316.4547, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
torch.Size([322])
torch.Size([1])
torch.Size([322])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8565)
idx:  321
Gate loss:  tensor(1317.3051, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
torch.Size([323])
torch.Size([1])
torch.Size([323])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2328)
idx:  322
Gate loss:  tensor(1318.5288, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4504)
idx:  323
Gate loss:  tensor(1319.6785, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10240, device='cuda:1')
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
torch.Size([325])
torch.Size([1])
torch.Size([325])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4537)
idx:  324
Gate loss:  tensor(1320.8066, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3886)
idx:  325
Gate loss:  tensor(1322.1338, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
torch.Size([327])
torch.Size([1])
torch.Size([327])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7190)
idx:  326
Gate loss:  tensor(1322.8441, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
torch.Size([328])
torch.Size([1])
torch.Size([328])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3248)
idx:  327
Gate loss:  tensor(1324.0992, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
torch.Size([329])
torch.Size([1])
torch.Size([329])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6010)
idx:  328
Gate loss:  tensor(1325.6940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25187, device='cuda:1')
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
torch.Size([330])
torch.Size([1])
torch.Size([330])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5495)
idx:  329
Gate loss:  tensor(1327.2052, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6774)
idx:  330
Gate loss:  tensor(1328.8575, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
torch.Size([332])
torch.Size([1])
torch.Size([332])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3565)
idx:  331
Gate loss:  tensor(1330.2025, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0328)
idx:  332
Gate loss:  tensor(1331.1720, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
torch.Size([334])
torch.Size([1])
torch.Size([334])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3833)
idx:  333
Gate loss:  tensor(1333.4004, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8172)
idx:  334
Gate loss:  tensor(1335.0109, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
torch.Size([336])
torch.Size([1])
torch.Size([336])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2077)
idx:  335
Gate loss:  tensor(1337.1857, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9791)
idx:  336
Gate loss:  tensor(1338.9547, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
torch.Size([338])
torch.Size([1])
torch.Size([338])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8475)
idx:  337
Gate loss:  tensor(1340.7961, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
torch.Size([339])
torch.Size([1])
torch.Size([339])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0573)
idx:  338
Gate loss:  tensor(1342.8304, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
torch.Size([340])
torch.Size([1])
torch.Size([340])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2009)
idx:  339
Gate loss:  tensor(1344.7500, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([341])
torch.Size([1])
torch.Size([341])
Inside custom generate sample func
torch.Size([341])
torch.Size([1])
torch.Size([341])
Inside custom generate sample func
torch.Size([341])
torch.Size([1])
torch.Size([341])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0413)
idx:  340
Gate loss:  tensor(1347.7529, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
torch.Size([342])
torch.Size([1])
torch.Size([342])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0671)
idx:  341
Gate loss:  tensor(1349.8051, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
torch.Size([343])
torch.Size([1])
torch.Size([343])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0991)
idx:  342
Gate loss:  tensor(1351.8705, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
torch.Size([346])
torch.Size([1])
torch.Size([346])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2959)
idx:  345
Gate loss:  tensor(1353.9736, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
torch.Size([347])
torch.Size([1])
torch.Size([347])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.9551)
idx:  346
Gate loss:  tensor(1359.8895, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
torch.Size([348])
torch.Size([1])
torch.Size([348])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7095)
idx:  347
Gate loss:  tensor(1361.2223, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([350])
torch.Size([1])
torch.Size([350])
Inside custom generate sample func
torch.Size([350])
torch.Size([1])
torch.Size([350])
Inside custom generate sample func
torch.Size([350])
torch.Size([1])
torch.Size([350])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7254)
idx:  349
Gate loss:  tensor(1364.4948, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
torch.Size([351])
torch.Size([1])
torch.Size([351])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3259)
idx:  350
Gate loss:  tensor(1367.1053, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
torch.Size([352])
torch.Size([1])
torch.Size([352])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3129)
idx:  351
Gate loss:  tensor(1370.2290, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
torch.Size([353])
torch.Size([1])
torch.Size([353])
Inside custom generate sample func
reasoning_path shape:  torch.Size([374])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.8754)
idx:  352
Gate loss:  tensor(1375.9827, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
torch.Size([354])
torch.Size([1])
torch.Size([354])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.9843)
idx:  353
Gate loss:  tensor(1383.4691, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
torch.Size([355])
torch.Size([1])
torch.Size([355])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.5316)
idx:  354
Gate loss:  tensor(1390.7125, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1959, device='cuda:1')
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
torch.Size([356])
torch.Size([1])
torch.Size([356])
Inside custom generate sample func
reasoning_path shape:  torch.Size([373])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.8298)
idx:  355
Gate loss:  tensor(1395.6973, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13944, device='cuda:1')
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.6140)
idx:  356
Gate loss:  tensor(1403.3992, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
torch.Size([358])
torch.Size([1])
torch.Size([358])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.7962)
idx:  357
Gate loss:  tensor(1412.0540, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.1277)
idx:  358
Gate loss:  tensor(1420.0919, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
torch.Size([360])
torch.Size([1])
torch.Size([360])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.0833)
idx:  359
Gate loss:  tensor(1437.6823, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
torch.Size([361])
torch.Size([1])
torch.Size([361])
Inside custom generate sample func
reasoning_path shape:  torch.Size([370])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-21.1418)
idx:  360
Gate loss:  tensor(1457.9431, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
torch.Size([362])
torch.Size([1])
torch.Size([362])
Inside custom generate sample func
reasoning_path shape:  torch.Size([369])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-21.4002)
idx:  361
Gate loss:  tensor(1477.3250, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
torch.Size([363])
torch.Size([1])
torch.Size([363])
Inside custom generate sample func
reasoning_path shape:  torch.Size([368])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-20.7448)
idx:  362
Gate loss:  tensor(1497.9540, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
torch.Size([364])
torch.Size([1])
torch.Size([364])
Inside custom generate sample func
reasoning_path shape:  torch.Size([376])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-28.7574)
idx:  363
Gate loss:  tensor(1525.4629, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 327
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/./train.py", line 845, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/./train.py", line 827, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/./train.py", line 418, in train
    model_loss.backward()
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
