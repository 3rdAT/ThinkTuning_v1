Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.99s/it]
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                              [0m| 0/450 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py:259: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
> [0;32m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py[0m(170)[0;36mthink_loss[0;34m()[0m
[0;32m    169 [0;31m[0;34m[0m[0m
[0m[0;32m--> 170 [0;31m    [0;32mreturn[0m [0mgate_loss[0m[0;34m,[0m [0mreinforce_loss[0m[0;34m,[0m [0mnll_loss_thought[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    171 [0;31m[0;34m[0m[0m
[0m
tensor(-0.2714, device='cuda:1')
tensor(0.0568, device='cuda:1', grad_fn=<AddBackward0>)
tensor(0.0018, device='cuda:1', grad_fn=<DivBackward0>)
[tensor(0.3142, device='cuda:1'), tensor(0.2797, device='cuda:1'), tensor(0.2466, device='cuda:1'), tensor(0.2594, device='cuda:1'), tensor(0.2571, device='cuda:1')]
tensor([[    1,   835,  4290,  ..., 29889,     2,     2]], device='cuda:1')
"<s> ###Input:\nThere are four numbers A, B, C, and D. A is 3 times 3. B is 15 less than C. C is a number four times A. D is 19 greater than A. Which of A, B, C, and D is the 2nd largest number? \n###Output:\nLet's calculate each number step by step:\n\nA is 3 times 3, so A = 3 * 3 = 9.\n\nC is four * A, so C = 4 * 9 times A, so C = 4 * A = 4 * 9 = 36.\n\nB is 15 less than C, so B = C - 15 = 36 - 15 = 21.\n\nD is 19 greater than A, so D = A + 19 = 9 + 19 = 28.\n\nNow we have all the numbers:\nA = 9\nB = 21\nC = 36\nD = 28\n\nTo find the second largest number, we order them from largest to smallest:\nC (36), D (28), B (21), A (9)\n\nThe second largest number is D, which is 28.</s></s><s> ###Input:\nThere are four numbers A, B, C, and D. A is 3 times 3. B is 15 less than C. C is a number four times A. D is 19 greater than A. Which of A, B, C, and D is the 2nd largest number? \n###Output:\nLet's calculate each number step by step:\n\nA is 3 times 3, so A = 3 * 3 = 9.\n\nC is four (4) times A, so C = 4 times A, so C = 4 * A = 4 * 9 = 36.\n\nB is 15 less than C, so B = C - 15 = 36 - 15 = 21.\n\nD is 19 greater than A, so D = A + 19 = 9 + 19 = 28.\n\nNow we have all the numbers:\nA = 9\nB = 21\nC = 36\nD = 28\n\nTo find the second largest number, we order them from largest to smallest:\nC (36), D (28), B (21), A (9)\n\nThe second largest number is D, which is 28.</s></s><s> ###Input:\nThere are four numbers A, B, C, and D. A is 3 times 3. B is 15 less than C. C is a number four times A. D is 19 greater than A. Which of A, B, C, and D is the 2nd largest number? \n###Output:\nLet's calculate each number step by step:\n\nA is 3 times 3, so A = 3 * 3 = 9.\n\nC is fourtimes A, so C = 4*A = times A, so C = 4 * A = 4 * 9 = 36.\n\nB is 15 less than C, so B = C - 15 = 36 - 15 = 21.\n\nD is 19 greater than A, so D = A + 19 = 9 + 19 = 28.\n\nNow we have all the numbers:\nA = 9\nB = 21\nC = 36\nD = 28\n\nTo find the second largest number, we order them from largest to smallest:\nC (36), D (28), B (21), A (9)\n\nThe second largest number is D, which is 28.</s></s><s> ###Input:\nThere are four numbers A, B, C, and D. A is 3 times 3. B is 15 less than C. C is a number four times A. D is 19 greater than A. Which of A, B, C, and D is the 2nd largest number? \n###Output:\nLet's calculate each number step by step:\n\nA is 3 times 3, so A = 3 * 3 = 9.\n\nC is four(n) times A, so C = n * times A, so C = 4 * A = 4 * 9 = 36.\n\nB is 15 less than C, so B = C - 15 = 36 - 15 = 21.\n\nD is 19 greater than A, so D = A + 19 = 9 + 19 = 28.\n\nNow we have all the numbers:\nA = 9\nB = 21\nC = 36\nD = 28\n\nTo find the second largest number, we order them from largest to smallest:\nC (36), D (28), B (21), A (9)\n\nThe second largest number is D, which is 28.</s></s><s> ###Input:\nThere are four numbers A, B, C, and D. A is 3 times 3. B is 15 less than C. C is a number four times A. D is 19 greater than A. Which of A, B, C, and D is the 2nd largest number? \n###Output:\nLet's calculate each number step by step:\n\nA is 3 times 3, so A = 3 * 3 = 9.\n\nC is four number times A, so C = 4 *  times A, so C = 4 * A = 4 * 9 = 36.\n\nB is 15 less than C, so B = C - 15 = 36 - 15 = 21.\n\nD is 19 greater than A, so D = A + 19 = 9 + 19 = 28.\n\nNow we have all the numbers:\nA = 9\nB = 21\nC = 36\nD = 28\n\nTo find the second largest number, we order them from largest to smallest:\nC (36), D (28), B (21), A (9)\n\nThe second largest number is D, which is 28.</s></s>"
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 863, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 847, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 395, in train
    outputs = think_tuner_step(batch, model=model, tokenizer=tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 362, in think_tuner_step
    total_gate_loss, total_reinforce_loss, total_nll_thought, logy = start_thinking(batch["input_ids"], batch["prompt_length"], batch["total_length"], outputs.last_hidden_state, logits,  batch["labels"], unreduced_loss, model, tokenizer)
                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 322, in start_thinking
    gate_loss_i, reinforce_loss_i, nll_loss_thought_i = think_loss(zz, idx, new_hidden_states, labels, packed_reasoning_path, model, gate_values, packed, unreduced_loss)
                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 170, in think_loss
    return gate_loss, reinforce_loss, nll_loss_thought
           ^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit

If you suspect this is an IPython 8.28.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True
