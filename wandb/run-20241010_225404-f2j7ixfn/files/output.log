Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.73s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/225 [00:00<?, ?it/s]
Preparing 4D causal attention mask with cache position
torch.Size([988])
tensor(1.1235, device='cuda:1', grad_fn=<MeanBackward0>)
The gate values are: tensor([[0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.9926, 0.9909, 0.8944, 0.9536,
         0.9416, 0.9719, 0.9055, 0.9409, 0.9443, 0.9775, 0.9149, 0.9524, 0.9652,
         0.9901, 0.9758, 0.6998, 0.9954, 0.9969, 0.9756, 0.9897, 0.9972, 0.8539,
         0.9967, 0.9865, 0.9062, 0.9927, 0.9234, 0.9982, 0.9972, 0.8550, 0.9963,
         0.9403, 0.9462, 0.9814, 0.9601, 0.9850, 0.9952, 0.8322, 0.8837, 0.9064,
         0.9123, 0.9958, 0.9815, 0.6064, 0.9178, 0.9408, 0.7958, 0.9848, 0.9831,
         0.7942, 0.8729, 0.5617, 0.2990, 0.9233, 0.8862, 0.4140, 0.7827, 0.3650,
         0.8453, 0.8865, 0.5122, 0.9367, 0.9674, 0.8888, 0.9660, 0.9751, 0.9775,
         0.9512, 0.9897, 0.9539, 0.6059, 0.9681, 0.4875, 0.1942, 0.1764, 0.9418,
         0.9838, 0.9628, 0.8721, 0.6583, 0.8737, 0.9687, 0.6227, 0.2832, 0.1916,
         0.9225, 0.9939, 0.9249, 0.9916, 0.9645, 0.9290, 0.9630, 0.9575, 0.9808,
         0.8663, 0.7539, 0.9766, 0.8695, 0.9830, 0.8483, 0.4943, 0.9692, 0.5359,
         0.9433, 0.9353, 0.3759, 0.8683, 0.9638, 0.9635, 0.7428, 0.9780, 0.6113,
         0.9630, 0.8983, 0.4766, 0.4100, 0.9775, 0.4919, 0.2818, 0.9761, 0.9893,
         0.9557, 0.9944, 0.9130, 0.9695, 0.9806, 0.9347, 0.9707, 0.8766, 0.7883,
         0.9601, 0.9762, 0.9816, 0.9871, 0.8730, 0.9868, 0.6681, 0.9526, 0.9728,
         0.7086, 0.9003, 0.9964, 0.9945, 0.8642, 0.9921, 0.6558, 0.9791, 0.9635,
         0.5275, 0.5380, 0.9734, 0.7830, 0.6393, 0.9815, 0.9930, 0.8944, 0.9793,
         0.9735, 0.9520, 0.9865, 0.9573, 0.9816, 0.8943, 0.8306, 0.9547, 0.9606,
         0.8273, 0.9970, 0.9694, 0.9767, 0.7002, 0.8846, 0.9393, 0.3106, 0.9947,
         0.9119, 0.5888, 0.9581, 0.9860, 0.6934, 0.9598, 0.8337, 0.6216, 0.9558,
         0.9766, 0.6882, 0.2052, 0.8476, 0.9292, 0.9516, 0.6765, 0.7543, 0.9681,
         0.9166, 0.7920, 0.9690, 0.7839, 0.4349, 0.8735, 0.9842, 0.8008, 0.9445,
         0.9692, 0.7681, 0.8038, 0.9954, 0.5333, 0.9928, 0.9640, 0.6680, 0.8897,
         0.9932, 0.7782, 0.9509, 0.9872, 0.8679, 0.9608, 0.9680, 0.8519, 0.3998,
         0.8005, 0.9850, 0.9764, 0.9911, 0.9880, 0.9860, 0.9543, 0.9648, 0.9792,
         0.9783, 0.9824, 0.9945, 0.9926, 0.9774, 0.9814, 0.9839, 0.9872, 0.9501,
         0.9780, 0.9963, 0.8802, 0.5607, 0.5571],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.7204, 0.3684, 0.7048, 0.6879,
         0.6074, 0.8019, 0.8872, 0.6792, 0.8183, 0.9902, 0.1940, 0.0696, 0.9569,
         0.9966, 0.9976, 0.9915, 0.8131, 0.9849, 0.9985, 0.9977, 0.9912, 0.9674,
         0.9669, 0.9153, 0.9832, 0.9875, 0.9040, 0.9604, 0.9823, 0.9666, 0.8090,
         0.2072, 0.9980, 0.9995, 0.9945, 0.9647, 0.6144, 0.6344, 0.3775, 0.1900,
         0.9314, 0.9394, 0.5690, 0.9250, 0.9563, 0.9991, 0.9961, 0.9927, 0.9914,
         0.8875, 0.9319, 0.9885, 0.9648, 0.9698, 0.9738, 0.9987, 0.9940, 0.5807,
         0.9795, 0.9116, 0.8878, 0.9897, 0.9840, 0.9868, 0.9912, 0.9849, 0.8631,
         0.9629, 0.9536, 0.9959, 0.9934, 0.9297, 0.9882, 0.8908, 0.6487, 0.4343,
         0.8388, 0.9544, 0.9765, 0.9556, 0.9797, 0.9885, 0.6489, 0.5883, 0.6513,
         0.9465, 0.9246, 0.9967, 0.9930, 0.9967, 0.9815, 0.9907, 0.7909, 0.9643,
         0.9599, 0.9967, 0.9955, 0.9941, 0.9966, 0.9985, 0.9922, 0.7009, 0.9990,
         0.7294, 0.6600, 0.3928, 0.9142, 0.8239, 0.9890, 0.9864, 0.9896, 0.9848,
         0.9848, 0.9962, 0.9929, 0.9568, 0.9945, 0.9824, 0.6236, 0.9540, 0.9892,
         0.9840, 0.8885, 0.9970, 0.9449, 0.9863, 0.8052, 0.4497, 0.5068, 0.8751,
         0.9891, 0.9764, 0.9389, 0.9363, 0.9624, 0.9371, 0.9865, 0.9709, 0.9991,
         0.9966, 0.9873, 0.6387, 0.9929, 0.9906, 0.9477, 0.9869, 0.9120, 0.9826,
         0.7644, 0.5793, 0.6468, 0.9520, 0.8438, 0.9898, 0.9888, 0.6055, 0.9965,
         0.9110, 0.8905, 0.9466, 0.9322, 0.9588, 0.9912, 0.9456, 0.9992, 0.9968,
         0.9941, 0.9569, 0.9492, 0.8518, 0.9840, 0.7879, 0.6568, 0.3619, 0.6373,
         0.7117, 0.7485, 0.8888, 0.9750, 0.9010, 0.7612, 0.9973, 0.9505, 0.9820,
         0.9931, 0.9937, 0.8973, 0.4409, 0.7623, 0.9927, 0.8725, 0.5816, 0.2959,
         0.9874, 0.9930, 0.9937, 0.9885, 0.9934, 0.9926, 0.8507, 0.9384, 0.9944,
         0.9644, 0.9696, 0.9941, 0.9962, 0.9971, 0.9959, 0.9897, 0.8488, 0.9759,
         0.9984, 0.9803, 0.5287, 0.9960, 0.9944, 0.9358, 0.9946, 0.9955, 0.9600,
         0.7677, 0.1727, 0.9707, 0.9643, 0.9902, 0.9952, 0.9942, 0.9960, 0.9782,
         0.9909, 0.9915, 0.9914, 0.8606, 0.3182, 0.9979, 0.9994, 0.9990, 0.9978,
         0.9482, 0.9212, 0.9971, 0.9196, 0.5943],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.8080, 0.1377, 0.9623, 0.9885,
         0.9293, 0.9831, 0.9254, 0.9983, 0.5610, 0.9658, 0.9970, 0.7952, 0.9660,
         0.9851, 0.8212, 0.7055, 0.8999, 0.9308, 0.7538, 0.9561, 0.9660, 0.9868,
         0.9759, 0.9347, 0.8530, 0.9826, 0.9819, 0.7393, 0.8939, 0.7791, 0.6971,
         0.8449, 0.8398, 0.7868, 0.9786, 0.6276, 0.9470, 0.9723, 0.9006, 0.7362,
         0.9667, 0.7593, 0.8899, 0.9817, 0.9943, 0.9768, 0.9293, 0.8522, 0.6924,
         0.3466, 0.2023, 0.9505, 0.8301, 0.4445, 0.6328, 0.9413, 0.6416, 0.9723,
         0.8437, 0.9793, 0.9960, 0.9714, 0.7362, 0.1673, 0.9693, 0.9370, 0.7873,
         0.7815, 0.6193, 0.8554, 0.9599, 0.9645, 0.6899, 0.9931, 0.9132, 0.9981,
         0.9807, 0.9081, 0.9879, 0.9961, 0.9930, 0.9371, 0.9943, 0.8666, 0.4801,
         0.4803, 0.8884, 0.9821, 0.9470, 0.8242, 0.9613, 0.9088, 0.8806, 0.9942,
         0.8476, 0.9563, 0.9632, 0.9777, 0.9922, 0.8746, 0.8387, 0.9735, 0.9794,
         0.9067, 0.9763, 0.9983, 0.9988, 0.9020, 0.9057, 0.9746, 0.9736, 0.9698,
         0.8147, 0.8100, 0.9996, 0.7811, 0.7231, 0.9851, 0.9852, 0.8378, 0.9776,
         0.9892, 0.9495, 0.9724, 0.9169, 0.9352, 0.9772, 0.9945, 0.9791, 0.9295,
         0.9936, 0.9819, 0.9989, 0.9573, 0.8729, 0.7777, 0.7006, 0.9669, 0.9365,
         0.8843, 0.9727, 0.9679, 0.9445, 0.9943, 0.9794, 0.9526, 0.9312, 0.9971,
         0.9991, 0.9545, 0.9645, 0.9692, 0.9791, 0.9155, 0.9705, 0.9932, 0.9991,
         0.9770, 0.9746, 0.9754, 0.9872, 0.9821, 0.9177, 0.9634, 0.9983, 0.9632,
         0.9256, 0.9737, 0.9760, 0.8107, 0.9824, 0.9734, 0.9271, 0.9837, 0.9471,
         0.9333, 0.9918, 0.9954, 0.9942, 0.7361, 0.9597, 0.9659, 0.9964, 0.9291,
         0.7067, 0.1180, 0.9541, 0.9629, 0.9744, 0.9921, 0.9894, 0.9797, 0.9981,
         0.9948, 0.9570, 0.8518, 0.7741, 0.9689, 0.9653, 0.7292, 0.3234, 0.9941,
         0.9690, 0.9968, 0.3375, 0.9546, 0.9902, 0.9884, 0.9842, 0.8890, 0.9925,
         0.9365, 0.9717, 0.9106, 0.9908, 0.4321, 0.9740, 0.9967, 0.9966, 0.9910,
         0.8259, 0.9292, 0.6018, 0.9664, 0.3193, 0.9820, 0.0453, 0.9710, 0.9763,
         0.7436, 0.0894, 0.7831, 0.9629, 0.9627, 0.9940, 0.9930, 0.9895, 0.9673,
         0.9890, 0.9576, 0.9901, 0.9595, 0.6003],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.9865, 0.9965, 0.9782, 0.8805,
         0.9639, 0.9933, 0.9908, 0.9960, 0.9847, 0.9959, 0.9743, 0.9653, 0.9756,
         0.9956, 0.9881, 0.0359, 0.0418, 0.8733, 0.6878, 0.0203, 0.8210, 0.6924,
         0.3953, 0.4778, 0.6053, 0.9318, 0.9126, 0.3885, 0.8209, 0.8297, 0.7526,
         0.0097, 0.0948, 0.0307, 0.9829, 0.8342, 0.9615, 0.7758, 0.8555, 0.8982,
         0.9717, 0.9262, 0.9110, 0.6861, 0.5722, 0.2808, 0.1755, 0.9321, 0.7756,
         0.5415, 0.6521, 0.9866, 0.9864, 0.9653, 0.9888, 0.9951, 0.9956, 0.9767,
         0.9317, 0.8930, 0.9922, 0.9744, 0.7365, 0.9417, 0.6333, 0.6866, 0.9642,
         0.1519, 0.1175, 0.2271, 0.2786, 0.9249, 0.8933, 0.0151, 0.6980, 0.7107,
         0.2569, 0.1841, 0.1083, 0.8536, 0.9342, 0.5189, 0.9610, 0.9751, 0.9246,
         0.2689, 0.1535, 0.2748, 0.9574, 0.9619, 0.9447, 0.9795, 0.9748, 0.7727,
         0.9822, 0.9661, 0.9928, 0.9965, 0.9935, 0.9824, 0.9632, 0.1811, 0.8990,
         0.9577, 0.9730, 0.9913, 0.2758, 0.8473, 0.8944, 0.7531, 0.5111, 0.9693,
         0.8530, 0.9387, 0.9509, 0.9908, 0.9610, 0.9953, 0.9871, 0.9689, 0.9565,
         0.9553, 0.9939, 0.9552, 0.5787, 0.2701, 0.2168, 0.2771, 0.8924, 0.9685,
         0.9806, 0.9653, 0.9722, 0.9985, 0.5551, 0.9967, 0.9968, 0.8073, 0.7466,
         0.8191, 0.9452, 0.9445, 0.9687, 0.9773, 0.9438, 0.8714, 0.9211, 0.8173,
         0.9705, 0.9496, 0.9744, 0.9954, 0.9856, 0.9963, 0.7315, 0.8171, 0.5593,
         0.9424, 0.9707, 0.9494, 0.9957, 0.9755, 0.9936, 0.9976, 0.8126, 0.9855,
         0.9676, 0.9952, 0.9743, 0.9875, 0.9935, 0.9369, 0.9807, 0.9777, 0.9771,
         0.9748, 0.9756, 0.9591, 0.9750, 0.3290, 0.4597, 0.1208, 0.9576, 0.7952,
         0.9316, 0.6267, 0.8881, 0.9175, 0.7814, 0.6957, 0.9887, 0.9091, 0.9680,
         0.9842, 0.9634, 0.9907, 0.9881, 0.9933, 0.9798, 0.9615, 0.9657, 0.9928,
         0.8946, 0.6511, 0.7390, 0.9616, 0.7463, 0.9254, 0.8515, 0.9748, 0.9733,
         0.9549, 0.9498, 0.9358, 0.9252, 0.7323, 0.9485, 0.5900, 0.4297, 0.9696,
         0.9690, 0.9737, 0.9916, 0.8235, 0.6463, 0.9920, 0.6511, 0.1045, 0.9916,
         0.9793, 0.9638, 0.9595, 0.9905, 0.9745, 0.9681, 0.9451, 0.7905, 0.9858,
         0.9065, 0.7538, 0.9757, 0.9261, 0.4928]], device='cuda:1',
       grad_fn=<SqueezeBackward1>)
The shape is: torch.Size([4, 248])
The shape of new_sequence: torch.Size([4, 248])
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([6])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3518)
idx:  5
Gate loss:  tensor(0.3492, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3420)
idx:  6
Gate loss:  tensor(0.6881, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11920, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3181)
idx:  7
Gate loss:  tensor(0.9726, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2778)
idx:  8
Gate loss:  tensor(1.2375, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3351)
idx:  9
Gate loss:  tensor(1.5530, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3366)
idx:  10
Gate loss:  tensor(1.8802, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3846)
idx:  11
Gate loss:  tensor(2.2285, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4172)
idx:  12
Gate loss:  tensor(2.6210, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4025)
idx:  13
Gate loss:  tensor(3.0011, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(360, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3838)
idx:  14
Gate loss:  tensor(3.3763, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(360, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3747)
idx:  15
Gate loss:  tensor(3.7191, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3641)
idx:  16
Gate loss:  tensor(4.0658, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2885)
idx:  17
Gate loss:  tensor(4.3443, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3186)
idx:  18
Gate loss:  tensor(4.6597, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3141)
idx:  19
Gate loss:  tensor(4.9662, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3532)
idx:  20
Gate loss:  tensor(5.2134, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3109, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3899)
idx:  21
Gate loss:  tensor(5.6015, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29995, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3358)
idx:  22
Gate loss:  tensor(5.9363, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3787)
idx:  23
Gate loss:  tensor(6.3057, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3488)
idx:  24
Gate loss:  tensor(6.6509, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3461)
idx:  25
Gate loss:  tensor(6.9960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3439)
idx:  26
Gate loss:  tensor(7.2897, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3271)
idx:  27
Gate loss:  tensor(7.6157, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3178)
idx:  28
Gate loss:  tensor(7.9292, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3066)
idx:  29
Gate loss:  tensor(8.2071, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3192)
idx:  30
Gate loss:  tensor(8.5239, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3146)
idx:  31
Gate loss:  tensor(8.8144, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3773)
idx:  32
Gate loss:  tensor(9.1910, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3692)
idx:  33
Gate loss:  tensor(9.5592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4337)
idx:  34
Gate loss:  tensor(9.9300, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3109, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3978)
idx:  35
Gate loss:  tensor(10.3264, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3936)
idx:  36
Gate loss:  tensor(10.6965, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3617)
idx:  37
Gate loss:  tensor(11.0387, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3806)
idx:  38
Gate loss:  tensor(11.4123, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(360, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4016)
idx:  39
Gate loss:  tensor(11.7978, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4302)
idx:  40
Gate loss:  tensor(12.2216, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3844)
idx:  41
Gate loss:  tensor(12.6042, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(319, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4425)
idx:  42
Gate loss:  tensor(12.9724, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4623)
idx:  43
Gate loss:  tensor(13.3809, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3972)
idx:  44
Gate loss:  tensor(13.7410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(319, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3918)
idx:  45
Gate loss:  tensor(14.0984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3702)
idx:  46
Gate loss:  tensor(14.4671, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3481)
idx:  47
Gate loss:  tensor(14.8087, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5044)
idx:  48
Gate loss:  tensor(15.1146, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3984)
idx:  49
Gate loss:  tensor(15.4802, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(360, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4201)
idx:  50
Gate loss:  tensor(15.8755, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4457)
idx:  51
Gate loss:  tensor(16.2301, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3270)
idx:  52
Gate loss:  tensor(16.5522, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3321)
idx:  53
Gate loss:  tensor(16.8786, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2444)
idx:  54
Gate loss:  tensor(17.0727, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3696)
idx:  55
Gate loss:  tensor(17.3954, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6466, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4136)
idx:  56
Gate loss:  tensor(17.6277, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3310)
idx:  58
Gate loss:  tensor(17.9333, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3752)
idx:  59
Gate loss:  tensor(18.2658, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5183)
idx:  61
Gate loss:  tensor(18.6714, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1827, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3728)
idx:  63
Gate loss:  tensor(18.9865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5645)
idx:  64
Gate loss:  tensor(19.4869, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1819, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4151)
idx:  65
Gate loss:  tensor(19.6995, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4246)
idx:  66
Gate loss:  tensor(20.0972, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1269, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4215)
idx:  67
Gate loss:  tensor(20.5051, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5141)
idx:  68
Gate loss:  tensor(20.9620, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5025)
idx:  69
Gate loss:  tensor(21.4475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5361)
idx:  70
Gate loss:  tensor(21.9703, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4607)
idx:  71
Gate loss:  tensor(22.4206, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(360, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5271)
idx:  72
Gate loss:  tensor(22.9219, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3257)
idx:  73
Gate loss:  tensor(23.2443, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4920)
idx:  74
Gate loss:  tensor(23.7136, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4331, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4507)
idx:  75
Gate loss:  tensor(23.9867, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4040)
idx:  76
Gate loss:  tensor(24.3778, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4648)
idx:  80
Gate loss:  tensor(24.8155, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4018)
idx:  81
Gate loss:  tensor(25.2108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3552)
idx:  82
Gate loss:  tensor(25.5528, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4305)
idx:  83
Gate loss:  tensor(25.9282, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4105)
idx:  84
Gate loss:  tensor(26.1984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4268)
idx:  85
Gate loss:  tensor(26.5713, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3027)
idx:  86
Gate loss:  tensor(26.8645, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3204)
idx:  87
Gate loss:  tensor(27.0640, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4604)
idx:  90
Gate loss:  tensor(27.4887, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4710)
idx:  91
Gate loss:  tensor(27.9568, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5276)
idx:  92
Gate loss:  tensor(28.4447, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7621, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5016)
idx:  93
Gate loss:  tensor(28.9421, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5164)
idx:  94
Gate loss:  tensor(29.4402, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(319, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4953)
idx:  95
Gate loss:  tensor(29.9003, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5586)
idx:  96
Gate loss:  tensor(30.4382, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4927)
idx:  97
Gate loss:  tensor(30.9099, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4446)
idx:  98
Gate loss:  tensor(31.3460, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4335)
idx:  99
Gate loss:  tensor(31.7215, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5316)
idx:  100
Gate loss:  tensor(32.1224, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4765)
idx:  101
Gate loss:  tensor(32.5877, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(319, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4578)
idx:  102
Gate loss:  tensor(32.9858, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5718)
idx:  103
Gate loss:  tensor(33.5479, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5480)
idx:  104
Gate loss:  tensor(34.0127, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4292)
idx:  106
Gate loss:  tensor(34.4288, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5297)
idx:  107
Gate loss:  tensor(34.7126, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6055)
idx:  108
Gate loss:  tensor(35.2838, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5514)
idx:  109
Gate loss:  tensor(35.7996, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7101)
idx:  111
Gate loss:  tensor(36.4162, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5313)
idx:  112
Gate loss:  tensor(36.9283, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6683)
idx:  113
Gate loss:  tensor(37.5722, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6000)
idx:  114
Gate loss:  tensor(38.0178, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4015)
idx:  115
Gate loss:  tensor(38.4105, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29933, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4994)
idx:  116
Gate loss:  tensor(38.7157, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4147)
idx:  117
Gate loss:  tensor(39.1151, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5553)
idx:  118
Gate loss:  tensor(39.6139, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4363)
idx:  121
Gate loss:  tensor(40.0404, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3783)
idx:  124
Gate loss:  tensor(40.4096, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4458)
idx:  125
Gate loss:  tensor(40.8506, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4636)
idx:  126
Gate loss:  tensor(41.2937, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3109, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4822)
idx:  127
Gate loss:  tensor(41.7732, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5124)
idx:  128
Gate loss:  tensor(42.2409, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6187)
idx:  129
Gate loss:  tensor(42.8407, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4816)
idx:  130
Gate loss:  tensor(43.3129, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5053)
idx:  131
Gate loss:  tensor(43.7853, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4742)
idx:  132
Gate loss:  tensor(44.2456, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5147)
idx:  133
Gate loss:  tensor(44.6968, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29907, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5591)
idx:  134
Gate loss:  tensor(45.1376, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4548)
idx:  135
Gate loss:  tensor(45.5742, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5578)
idx:  136
Gate loss:  tensor(46.1188, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6200)
idx:  137
Gate loss:  tensor(46.7274, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5767)
idx:  138
Gate loss:  tensor(47.2966, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5611)
idx:  139
Gate loss:  tensor(47.7864, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4816)
idx:  140
Gate loss:  tensor(48.2617, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29907, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6031)
idx:  141
Gate loss:  tensor(48.6646, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5481)
idx:  142
Gate loss:  tensor(49.1867, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6517)
idx:  143
Gate loss:  tensor(49.8207, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8139)
idx:  144
Gate loss:  tensor(50.3975, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7378)
idx:  145
Gate loss:  tensor(51.0618, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7879)
idx:  146
Gate loss:  tensor(51.8468, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7946)
idx:  147
Gate loss:  tensor(52.6370, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9097)
idx:  148
Gate loss:  tensor(53.4232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6741)
idx:  149
Gate loss:  tensor(54.0920, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29907, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6477)
idx:  150
Gate loss:  tensor(54.5167, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6004)
idx:  151
Gate loss:  tensor(55.1045, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6860)
idx:  152
Gate loss:  tensor(55.7655, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8899)
idx:  153
Gate loss:  tensor(56.2349, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8295)
idx:  154
Gate loss:  tensor(56.6812, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5907)
idx:  155
Gate loss:  tensor(57.2562, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6325)
idx:  156
Gate loss:  tensor(57.7515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29928, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6861)
idx:  157
Gate loss:  tensor(58.1901, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6728)
idx:  158
Gate loss:  tensor(58.8505, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8951, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6313)
idx:  159
Gate loss:  tensor(59.4773, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6587)
idx:  160
Gate loss:  tensor(60.0665, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7355)
idx:  161
Gate loss:  tensor(60.7868, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7230)
idx:  162
Gate loss:  tensor(61.4906, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6976)
idx:  163
Gate loss:  tensor(62.1548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7023)
idx:  164
Gate loss:  tensor(62.8476, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(577, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7168)
idx:  165
Gate loss:  tensor(63.5338, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7235)
idx:  166
Gate loss:  tensor(64.2440, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0633)
idx:  167
Gate loss:  tensor(65.1950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29928, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8466)
idx:  168
Gate loss:  tensor(65.8981, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6311)
idx:  169
Gate loss:  tensor(66.5006, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0656)
idx:  170
Gate loss:  tensor(67.5242, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9373)
idx:  171
Gate loss:  tensor(68.2997, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9774)
idx:  172
Gate loss:  tensor(69.2741, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8405)
idx:  173
Gate loss:  tensor(70.0888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6626)
idx:  174
Gate loss:  tensor(70.7360, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29928, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4001)
idx:  175
Gate loss:  tensor(71.7164, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7850)
idx:  176
Gate loss:  tensor(72.4109, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9509)
idx:  177
Gate loss:  tensor(73.3041, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0030)
idx:  179
Gate loss:  tensor(74.3017, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7287)
idx:  180
Gate loss:  tensor(75.8781, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4543)
idx:  181
Gate loss:  tensor(76.7344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3659)
idx:  182
Gate loss:  tensor(78.0430, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0817)
idx:  183
Gate loss:  tensor(79.1096, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29928, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5984)
idx:  184
Gate loss:  tensor(80.2180, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0650)
idx:  185
Gate loss:  tensor(81.2402, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3110)
idx:  186
Gate loss:  tensor(82.3332, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8396)
idx:  187
Gate loss:  tensor(83.4766, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3794)
idx:  188
Gate loss:  tensor(84.7950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0638)
idx:  189
Gate loss:  tensor(85.8340, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2316)
idx:  190
Gate loss:  tensor(86.6815, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1469)
idx:  192
Gate loss:  tensor(87.6536, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(505, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3367)
idx:  193
Gate loss:  tensor(88.8957, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2070)
idx:  194
Gate loss:  tensor(90.0442, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2427)
idx:  195
Gate loss:  tensor(90.8850, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1819, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2489)
idx:  196
Gate loss:  tensor(91.8270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1852)
idx:  197
Gate loss:  tensor(92.9745, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0827)
idx:  198
Gate loss:  tensor(93.9669, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5439)
idx:  199
Gate loss:  tensor(95.1897, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6853)
idx:  200
Gate loss:  tensor(96.8228, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6782)
idx:  201
Gate loss:  tensor(98.1384, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5664)
idx:  203
Gate loss:  tensor(99.5067, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5341)
idx:  204
Gate loss:  tensor(101.0166, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29933, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6130)
idx:  205
Gate loss:  tensor(102.3083, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6100)
idx:  206
Gate loss:  tensor(103.8288, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6792)
idx:  207
Gate loss:  tensor(105.4564, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7092)
idx:  208
Gate loss:  tensor(106.7692, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9199)
idx:  209
Gate loss:  tensor(108.3124, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7886)
idx:  210
Gate loss:  tensor(110.0929, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29907, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8442)
idx:  211
Gate loss:  tensor(111.0764, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0232)
idx:  212
Gate loss:  tensor(113.0849, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8379)
idx:  213
Gate loss:  tensor(115.8207, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5392)
idx:  214
Gate loss:  tensor(117.5169, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6497)
idx:  215
Gate loss:  tensor(119.8744, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0803)
idx:  216
Gate loss:  tensor(121.9405, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29928, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0864)
idx:  217
Gate loss:  tensor(123.5643, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9288)
idx:  218
Gate loss:  tensor(125.3984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5751)
idx:  219
Gate loss:  tensor(127.9405, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6933)
idx:  220
Gate loss:  tensor(130.2782, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4027)
idx:  221
Gate loss:  tensor(132.5867, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4763)
idx:  222
Gate loss:  tensor(134.9838, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2089)
idx:  223
Gate loss:  tensor(137.7174, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10150, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8218)
idx:  225
Gate loss:  tensor(139.9764, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.9094)
idx:  226
Gate loss:  tensor(143.8270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5559)
idx:  227
Gate loss:  tensor(147.2991, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1438, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1800)
idx:  228
Gate loss:  tensor(150.4508, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.6656)
idx:  229
Gate loss:  tensor(155.0602, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(350, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([258])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.7770)
idx:  230
Gate loss:  tensor(159.7703, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.8560)
idx:  231
Gate loss:  tensor(166.3132, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(315, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.7149)
idx:  232
Gate loss:  tensor(171.8268, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.5778)
idx:  233
Gate loss:  tensor(178.2678, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([257])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.2770)
idx:  234
Gate loss:  tensor(185.3866, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(360, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([257])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.8317)
idx:  235
Gate loss:  tensor(195.0449, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.4406)
idx:  236
Gate loss:  tensor(204.4340, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.6313)
idx:  237
Gate loss:  tensor(213.0014, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.0429)
idx:  238
Gate loss:  tensor(221.8396, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(607, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.1072)
idx:  239
Gate loss:  tensor(233.7213, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.5945)
idx:  240
Gate loss:  tensor(248.0801, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-17.0178)
idx:  241
Gate loss:  tensor(264.8809, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.2330)
idx:  242
Gate loss:  tensor(278.4040, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-22.9111)
idx:  243
Gate loss:  tensor(300.8119, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-29.2539)
idx:  244
Gate loss:  tensor(329.9570, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-36.1532)
idx:  245
Gate loss:  tensor(361.7794, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.4593)
idx:  246
Gate loss:  tensor(377.1767, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 225
The shape of hidden_states: torch.Size([4096])
The topk tensor(366, device='cuda:1')
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3350)
idx:  5
Gate loss:  tensor(377.4181, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3446)
idx:  7
Gate loss:  tensor(377.6609, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3753)
idx:  8
Gate loss:  tensor(377.9191, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(937, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3617)
idx:  9
Gate loss:  tensor(378.1388, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3308)
idx:  10
Gate loss:  tensor(378.4041, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3792)
idx:  11
Gate loss:  tensor(378.7406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3445)
idx:  12
Gate loss:  tensor(378.9745, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3143, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4102)
idx:  13
Gate loss:  tensor(379.3102, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3254)
idx:  14
Gate loss:  tensor(379.6324, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3762)
idx:  17
Gate loss:  tensor(379.9924, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3505)
idx:  18
Gate loss:  tensor(380.3417, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2711)
idx:  19
Gate loss:  tensor(380.6122, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2845)
idx:  20
Gate loss:  tensor(380.8943, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2662)
idx:  21
Gate loss:  tensor(381.1108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3209)
idx:  22
Gate loss:  tensor(381.4268, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3231)
idx:  23
Gate loss:  tensor(381.7495, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2546)
idx:  24
Gate loss:  tensor(382.0035, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(769, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2432)
idx:  25
Gate loss:  tensor(382.2446, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2499)
idx:  26
Gate loss:  tensor(382.4863, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2866)
idx:  27
Gate loss:  tensor(382.7635, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3032)
idx:  28
Gate loss:  tensor(383.0410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3192)
idx:  29
Gate loss:  tensor(383.3548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2714)
idx:  30
Gate loss:  tensor(383.6228, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2702)
idx:  31
Gate loss:  tensor(383.8671, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3037)
idx:  32
Gate loss:  tensor(384.1588, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3123)
idx:  33
Gate loss:  tensor(384.4655, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2307)
idx:  34
Gate loss:  tensor(384.6886, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(348, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3288)
idx:  35
Gate loss:  tensor(384.9546, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3256)
idx:  37
Gate loss:  tensor(385.2795, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6496, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3349)
idx:  38
Gate loss:  tensor(385.6142, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2614)
idx:  39
Gate loss:  tensor(385.8741, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1522)
idx:  40
Gate loss:  tensor(386.0210, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2136)
idx:  41
Gate loss:  tensor(386.1523, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2684)
idx:  42
Gate loss:  tensor(386.3225, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4071)
idx:  45
Gate loss:  tensor(386.7017, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2412)
idx:  46
Gate loss:  tensor(386.9283, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1576, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4522)
idx:  47
Gate loss:  tensor(387.1855, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3663)
idx:  48
Gate loss:  tensor(387.5244, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3143, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3898)
idx:  49
Gate loss:  tensor(387.8971, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3651)
idx:  50
Gate loss:  tensor(388.2619, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6496, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3103)
idx:  51
Gate loss:  tensor(388.5710, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3006)
idx:  52
Gate loss:  tensor(388.8694, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2682)
idx:  53
Gate loss:  tensor(389.1353, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3556)
idx:  54
Gate loss:  tensor(389.4508, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3113)
idx:  55
Gate loss:  tensor(389.7410, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3090)
idx:  56
Gate loss:  tensor(390.0464, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2901)
idx:  57
Gate loss:  tensor(390.3263, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2000, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2679)
idx:  58
Gate loss:  tensor(390.5861, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2286)
idx:  59
Gate loss:  tensor(390.8087, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2278)
idx:  60
Gate loss:  tensor(391.0362, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3172)
idx:  61
Gate loss:  tensor(391.3515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2050)
idx:  62
Gate loss:  tensor(391.4705, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2556)
idx:  63
Gate loss:  tensor(391.7209, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2647)
idx:  64
Gate loss:  tensor(391.9621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3114)
idx:  65
Gate loss:  tensor(392.2386, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2779)
idx:  66
Gate loss:  tensor(392.5136, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3339)
idx:  67
Gate loss:  tensor(392.8422, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3378)
idx:  68
Gate loss:  tensor(393.1756, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2781)
idx:  69
Gate loss:  tensor(393.4513, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2937)
idx:  70
Gate loss:  tensor(393.7406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3867)
idx:  71
Gate loss:  tensor(394.0743, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4748)
idx:  72
Gate loss:  tensor(394.5314, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3372)
idx:  73
Gate loss:  tensor(394.8530, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3188)
idx:  74
Gate loss:  tensor(395.1705, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3255)
idx:  75
Gate loss:  tensor(395.4938, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3665)
idx:  76
Gate loss:  tensor(395.8346, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17668, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3654)
idx:  77
Gate loss:  tensor(396.1957, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4772)
idx:  78
Gate loss:  tensor(396.6208, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2851)
idx:  79
Gate loss:  tensor(396.8058, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3147)
idx:  81
Gate loss:  tensor(397.0697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4505, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3270)
idx:  82
Gate loss:  tensor(397.3818, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3400)
idx:  83
Gate loss:  tensor(397.7138, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7200, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3723)
idx:  84
Gate loss:  tensor(398.0696, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4022)
idx:  85
Gate loss:  tensor(398.4636, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2220)
idx:  86
Gate loss:  tensor(398.6830, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3483)
idx:  87
Gate loss:  tensor(398.9091, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2363)
idx:  88
Gate loss:  tensor(399.0481, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28956, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3174)
idx:  89
Gate loss:  tensor(399.2548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29915, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4167)
idx:  90
Gate loss:  tensor(399.6492, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1813, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4500)
idx:  91
Gate loss:  tensor(400.0653, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1813, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4951)
idx:  92
Gate loss:  tensor(400.5587, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4445)
idx:  93
Gate loss:  tensor(401.0001, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4434)
idx:  94
Gate loss:  tensor(401.4421, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4361)
idx:  95
Gate loss:  tensor(401.8701, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3507)
idx:  96
Gate loss:  tensor(402.2175, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3586)
idx:  97
Gate loss:  tensor(402.5011, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3010)
idx:  98
Gate loss:  tensor(402.7914, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7200, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3555)
idx:  99
Gate loss:  tensor(403.1327, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1813, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3869)
idx:  100
Gate loss:  tensor(403.5183, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3514)
idx:  101
Gate loss:  tensor(403.8681, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3417)
idx:  102
Gate loss:  tensor(404.2077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3632)
idx:  103
Gate loss:  tensor(404.5697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3152)
idx:  104
Gate loss:  tensor(404.8844, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2769)
idx:  105
Gate loss:  tensor(405.1592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4783)
idx:  106
Gate loss:  tensor(405.4944, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2837)
idx:  107
Gate loss:  tensor(405.7778, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2265)
idx:  108
Gate loss:  tensor(405.9431, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2966)
idx:  109
Gate loss:  tensor(406.1388, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3052)
idx:  111
Gate loss:  tensor(406.4178, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3307)
idx:  112
Gate loss:  tensor(406.6902, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3926)
idx:  113
Gate loss:  tensor(407.0786, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4927)
idx:  114
Gate loss:  tensor(407.5646, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3971)
idx:  115
Gate loss:  tensor(407.9575, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2822)
idx:  116
Gate loss:  tensor(408.2354, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2486)
idx:  117
Gate loss:  tensor(408.4802, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3339)
idx:  118
Gate loss:  tensor(408.8128, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3336)
idx:  119
Gate loss:  tensor(409.1440, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29916, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3688)
idx:  120
Gate loss:  tensor(409.4969, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5034)
idx:  121
Gate loss:  tensor(409.9975, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4722)
idx:  122
Gate loss:  tensor(410.4614, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4969)
idx:  123
Gate loss:  tensor(410.7712, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3718)
idx:  124
Gate loss:  tensor(411.1259, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3556)
idx:  125
Gate loss:  tensor(411.4777, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4040)
idx:  126
Gate loss:  tensor(411.8752, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4527)
idx:  127
Gate loss:  tensor(412.2773, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5153)
idx:  128
Gate loss:  tensor(412.7911, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4364)
idx:  129
Gate loss:  tensor(413.2035, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6666)
idx:  130
Gate loss:  tensor(413.8610, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4264)
idx:  131
Gate loss:  tensor(414.2043, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8439, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3682)
idx:  133
Gate loss:  tensor(414.3910, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2109, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6586)
idx:  134
Gate loss:  tensor(414.9672, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(292, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5527)
idx:  135
Gate loss:  tensor(415.5138, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1438, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5466)
idx:  136
Gate loss:  tensor(416.0475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4958, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5548)
idx:  137
Gate loss:  tensor(416.5684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4759)
idx:  138
Gate loss:  tensor(417.0140, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3448)
idx:  139
Gate loss:  tensor(417.3458, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4778)
idx:  140
Gate loss:  tensor(417.7935, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3544)
idx:  141
Gate loss:  tensor(418.1431, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4537)
idx:  142
Gate loss:  tensor(418.5836, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29916, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3907)
idx:  143
Gate loss:  tensor(418.9739, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4823)
idx:  144
Gate loss:  tensor(419.4546, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5463)
idx:  145
Gate loss:  tensor(419.9940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5603)
idx:  146
Gate loss:  tensor(420.3519, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4556)
idx:  147
Gate loss:  tensor(420.8043, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5625)
idx:  148
Gate loss:  tensor(421.3615, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([251])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8132)
idx:  149
Gate loss:  tensor(422.1322, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7238)
idx:  150
Gate loss:  tensor(422.8466, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5927)
idx:  151
Gate loss:  tensor(423.3871, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3880)
idx:  152
Gate loss:  tensor(423.7684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4375)
idx:  153
Gate loss:  tensor(424.1028, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4963)
idx:  154
Gate loss:  tensor(424.3903, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13296, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4703)
idx:  155
Gate loss:  tensor(424.6945, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29873, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6524)
idx:  156
Gate loss:  tensor(425.3156, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1461, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7242)
idx:  157
Gate loss:  tensor(425.9267, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(292, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6624)
idx:  158
Gate loss:  tensor(426.5824, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8513)
idx:  159
Gate loss:  tensor(427.4242, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7306)
idx:  160
Gate loss:  tensor(427.8666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(515, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6595)
idx:  161
Gate loss:  tensor(428.5238, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6840)
idx:  162
Gate loss:  tensor(429.1469, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6305)
idx:  163
Gate loss:  tensor(429.7084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5636)
idx:  164
Gate loss:  tensor(430.2420, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4290)
idx:  165
Gate loss:  tensor(430.6419, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6034)
idx:  166
Gate loss:  tensor(431.2205, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5978)
idx:  167
Gate loss:  tensor(431.8130, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4262)
idx:  168
Gate loss:  tensor(432.2161, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29916, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4442)
idx:  169
Gate loss:  tensor(432.6600, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4002)
idx:  170
Gate loss:  tensor(433.0590, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6262)
idx:  171
Gate loss:  tensor(433.6815, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7808)
idx:  172
Gate loss:  tensor(434.4286, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7420)
idx:  173
Gate loss:  tensor(435.1329, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7525)
idx:  174
Gate loss:  tensor(435.7739, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6387)
idx:  175
Gate loss:  tensor(436.4024, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4356)
idx:  176
Gate loss:  tensor(437.5336, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7236)
idx:  177
Gate loss:  tensor(438.0089, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3640, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6694)
idx:  179
Gate loss:  tensor(438.4355, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(292, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7878)
idx:  180
Gate loss:  tensor(438.9962, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8255)
idx:  181
Gate loss:  tensor(439.6141, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7649)
idx:  182
Gate loss:  tensor(440.2940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7869)
idx:  183
Gate loss:  tensor(441.0612, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7799)
idx:  184
Gate loss:  tensor(441.7639, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9841)
idx:  185
Gate loss:  tensor(442.5130, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8245)
idx:  186
Gate loss:  tensor(443.3353, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6870)
idx:  187
Gate loss:  tensor(443.9884, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8505)
idx:  188
Gate loss:  tensor(444.8237, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7218)
idx:  189
Gate loss:  tensor(445.5404, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6896)
idx:  190
Gate loss:  tensor(446.2257, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8614)
idx:  191
Gate loss:  tensor(447.8959, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2715)
idx:  193
Gate loss:  tensor(448.8651, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9489)
idx:  194
Gate loss:  tensor(449.8071, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5744)
idx:  195
Gate loss:  tensor(451.1808, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([253])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5079)
idx:  196
Gate loss:  tensor(452.0577, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3568)
idx:  198
Gate loss:  tensor(453.3974, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7200, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7728)
idx:  199
Gate loss:  tensor(455.1579, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1813, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1578)
idx:  200
Gate loss:  tensor(456.3084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3324)
idx:  201
Gate loss:  tensor(457.6254, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9348)
idx:  202
Gate loss:  tensor(459.5475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([257])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9799)
idx:  203
Gate loss:  tensor(462.5055, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2141)
idx:  204
Gate loss:  tensor(464.3889, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3315)
idx:  205
Gate loss:  tensor(466.5768, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([252])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7589)
idx:  206
Gate loss:  tensor(468.3260, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0931)
idx:  207
Gate loss:  tensor(470.3446, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9509)
idx:  208
Gate loss:  tensor(471.2666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7200, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3843)
idx:  209
Gate loss:  tensor(472.6427, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1813, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3866)
idx:  210
Gate loss:  tensor(474.0240, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6700)
idx:  211
Gate loss:  tensor(475.6891, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4890)
idx:  212
Gate loss:  tensor(477.1720, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4548)
idx:  213
Gate loss:  tensor(479.6017, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([253])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2829)
idx:  214
Gate loss:  tensor(482.3880, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5807)
idx:  215
Gate loss:  tensor(485.8825, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3717)
idx:  216
Gate loss:  tensor(490.2470, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3621)
idx:  217
Gate loss:  tensor(492.5625, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([256])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.4097)
idx:  218
Gate loss:  tensor(495.4229, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8554)
idx:  219
Gate loss:  tensor(498.2669, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9458)
idx:  220
Gate loss:  tensor(500.2018, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3343)
idx:  221
Gate loss:  tensor(503.3221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([252])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0463)
idx:  222
Gate loss:  tensor(506.3519, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6644)
idx:  223
Gate loss:  tensor(508.0088, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3075)
idx:  224
Gate loss:  tensor(509.2640, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7469)
idx:  225
Gate loss:  tensor(512.1406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.0450)
idx:  227
Gate loss:  tensor(517.0377, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([256])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.9243)
idx:  228
Gate loss:  tensor(520.8218, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.7112)
idx:  229
Gate loss:  tensor(525.4871, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7200, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5517)
idx:  230
Gate loss:  tensor(528.0266, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0048)
idx:  231
Gate loss:  tensor(531.0139, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.8069)
idx:  232
Gate loss:  tensor(536.7975, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2172)
idx:  233
Gate loss:  tensor(539.9445, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([258])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.8032)
idx:  234
Gate loss:  tensor(551.6406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6515, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([256])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.6013)
idx:  235
Gate loss:  tensor(561.1603, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(382, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([258])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.4230)
idx:  236
Gate loss:  tensor(567.5280, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(348, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.5623)
idx:  237
Gate loss:  tensor(574.0363, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.2287)
idx:  239
Gate loss:  tensor(584.2437, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6496, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([256])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.6279)
idx:  240
Gate loss:  tensor(595.8646, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.6755)
idx:  241
Gate loss:  tensor(606.5293, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([255])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.9556)
idx:  242
Gate loss:  tensor(617.4606, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([252])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-20.9804)
idx:  243
Gate loss:  tensor(637.3542, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.4069)
idx:  244
Gate loss:  tensor(654.3100, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-19.0364)
idx:  245
Gate loss:  tensor(673.2922, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-40.2728)
idx:  246
Gate loss:  tensor(710.3262, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 229
The shape of hidden_states: torch.Size([4096])
The topk tensor(1051, device='cuda:1')
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3511)
idx:  5
Gate loss:  tensor(710.6099, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(756, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2286)
idx:  7
Gate loss:  tensor(710.8299, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(380, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2294)
idx:  8
Gate loss:  tensor(711.0567, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2483)
idx:  9
Gate loss:  tensor(711.2875, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(982, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2691)
idx:  10
Gate loss:  tensor(711.5520, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5716, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2193)
idx:  11
Gate loss:  tensor(711.7549, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2173)
idx:  12
Gate loss:  tensor(711.9719, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(450, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2255)
idx:  13
Gate loss:  tensor(712.0984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9109, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2826)
idx:  14
Gate loss:  tensor(712.3713, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3743, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3018)
idx:  15
Gate loss:  tensor(712.6722, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3313)
idx:  16
Gate loss:  tensor(712.9356, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3050, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2570)
idx:  17
Gate loss:  tensor(713.1838, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2583)
idx:  18
Gate loss:  tensor(713.4383, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2927)
idx:  19
Gate loss:  tensor(713.6787, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2533, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2689)
idx:  20
Gate loss:  tensor(713.8684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2538)
idx:  21
Gate loss:  tensor(714.0969, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2930)
idx:  22
Gate loss:  tensor(714.3696, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3084)
idx:  23
Gate loss:  tensor(714.6021, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6019, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3656)
idx:  24
Gate loss:  tensor(714.9517, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1473, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3771)
idx:  25
Gate loss:  tensor(715.3159, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3209)
idx:  26
Gate loss:  tensor(715.6326, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3625)
idx:  27
Gate loss:  tensor(715.9863, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3040)
idx:  28
Gate loss:  tensor(716.2704, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3471)
idx:  29
Gate loss:  tensor(716.5665, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3211)
idx:  30
Gate loss:  tensor(716.8820, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2560)
idx:  31
Gate loss:  tensor(717.1334, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3214)
idx:  32
Gate loss:  tensor(717.3710, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2704)
idx:  33
Gate loss:  tensor(717.6127, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2857)
idx:  34
Gate loss:  tensor(717.8353, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2932)
idx:  35
Gate loss:  tensor(718.0397, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3557)
idx:  36
Gate loss:  tensor(718.3402, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2843)
idx:  37
Gate loss:  tensor(718.5790, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3254)
idx:  38
Gate loss:  tensor(718.8350, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3277)
idx:  39
Gate loss:  tensor(719.1557, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3152)
idx:  40
Gate loss:  tensor(719.3535, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(408, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2311)
idx:  41
Gate loss:  tensor(719.5724, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1269, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2294)
idx:  42
Gate loss:  tensor(719.7955, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1555)
idx:  43
Gate loss:  tensor(719.9355, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2444)
idx:  44
Gate loss:  tensor(720.1154, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2339)
idx:  45
Gate loss:  tensor(720.3415, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4800, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3218)
idx:  46
Gate loss:  tensor(720.5858, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([253])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3607)
idx:  47
Gate loss:  tensor(720.9068, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3365)
idx:  48
Gate loss:  tensor(721.2372, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9109, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2960)
idx:  49
Gate loss:  tensor(721.5315, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2229)
idx:  50
Gate loss:  tensor(721.7492, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2533)
idx:  51
Gate loss:  tensor(721.9846, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1565)
idx:  52
Gate loss:  tensor(722.1180, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2772)
idx:  53
Gate loss:  tensor(722.3100, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2600)
idx:  56
Gate loss:  tensor(722.5571, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2149)
idx:  57
Gate loss:  tensor(722.7354, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3476)
idx:  59
Gate loss:  tensor(722.9554, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3402)
idx:  60
Gate loss:  tensor(723.2756, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4800, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3111)
idx:  61
Gate loss:  tensor(723.4752, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3547)
idx:  62
Gate loss:  tensor(723.8201, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2689)
idx:  63
Gate loss:  tensor(724.0469, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3154)
idx:  64
Gate loss:  tensor(724.3558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3084)
idx:  65
Gate loss:  tensor(724.6630, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2943)
idx:  66
Gate loss:  tensor(724.9489, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3445)
idx:  67
Gate loss:  tensor(725.2025, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4225)
idx:  69
Gate loss:  tensor(725.6121, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4067)
idx:  70
Gate loss:  tensor(725.9931, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3484)
idx:  71
Gate loss:  tensor(726.2675, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4142)
idx:  72
Gate loss:  tensor(726.5912, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26204, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4102)
idx:  73
Gate loss:  tensor(726.8452, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3703)
idx:  74
Gate loss:  tensor(727.1620, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2671)
idx:  75
Gate loss:  tensor(727.4184, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(599, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3681)
idx:  76
Gate loss:  tensor(727.7734, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3025)
idx:  77
Gate loss:  tensor(727.9821, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2588)
idx:  78
Gate loss:  tensor(728.2391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2962)
idx:  79
Gate loss:  tensor(728.5095, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3668)
idx:  80
Gate loss:  tensor(728.8757, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3747)
idx:  81
Gate loss:  tensor(729.2432, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3501)
idx:  82
Gate loss:  tensor(729.5611, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4098)
idx:  83
Gate loss:  tensor(729.9659, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3385)
idx:  84
Gate loss:  tensor(730.3032, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3952)
idx:  85
Gate loss:  tensor(730.6956, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5765)
idx:  86
Gate loss:  tensor(731.2358, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2953)
idx:  87
Gate loss:  tensor(731.5294, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1334, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2702)
idx:  88
Gate loss:  tensor(731.7636, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10150, device='cuda:1')
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
torch.Size([92])
torch.Size([1])
torch.Size([92])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4669)
idx:  91
Gate loss:  tensor(732.1784, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2211, device='cuda:1')
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
torch.Size([93])
torch.Size([1])
torch.Size([93])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4409)
idx:  92
Gate loss:  tensor(732.6114, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4218)
idx:  93
Gate loss:  tensor(733.0108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26204, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4559)
idx:  94
Gate loss:  tensor(733.3865, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4685)
idx:  95
Gate loss:  tensor(733.8369, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3686)
idx:  96
Gate loss:  tensor(734.1719, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4949)
idx:  97
Gate loss:  tensor(734.6077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3113)
idx:  98
Gate loss:  tensor(734.9172, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(773, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3631)
idx:  99
Gate loss:  tensor(735.2250, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4924)
idx:  100
Gate loss:  tensor(735.6959, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4446)
idx:  101
Gate loss:  tensor(736.1241, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4460)
idx:  102
Gate loss:  tensor(736.5602, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4320)
idx:  103
Gate loss:  tensor(736.9888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5664)
idx:  104
Gate loss:  tensor(737.4842, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(937, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4282)
idx:  105
Gate loss:  tensor(737.8433, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4946)
idx:  106
Gate loss:  tensor(738.3248, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4828)
idx:  107
Gate loss:  tensor(738.7976, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5111)
idx:  108
Gate loss:  tensor(739.2610, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1473, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4926)
idx:  109
Gate loss:  tensor(739.7420, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10150, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4224)
idx:  110
Gate loss:  tensor(740.1637, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4274)
idx:  111
Gate loss:  tensor(740.5906, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5249)
idx:  112
Gate loss:  tensor(741.0640, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25187, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3361)
idx:  113
Gate loss:  tensor(741.3684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3963)
idx:  114
Gate loss:  tensor(741.7546, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3918)
idx:  115
Gate loss:  tensor(742.1362, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4736)
idx:  116
Gate loss:  tensor(742.5954, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5423)
idx:  117
Gate loss:  tensor(743.0372, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5312)
idx:  118
Gate loss:  tensor(743.4675, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5354)
idx:  119
Gate loss:  tensor(744.0027, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5764)
idx:  120
Gate loss:  tensor(744.4529, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6743, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4918)
idx:  121
Gate loss:  tensor(744.8085, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5012)
idx:  122
Gate loss:  tensor(745.3023, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4264)
idx:  123
Gate loss:  tensor(745.7224, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3870)
idx:  124
Gate loss:  tensor(746.0467, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5130)
idx:  125
Gate loss:  tensor(746.5482, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3269)
idx:  126
Gate loss:  tensor(746.8716, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10150, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6048)
idx:  127
Gate loss:  tensor(747.4458, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2211, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5708)
idx:  128
Gate loss:  tensor(748.0008, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4958)
idx:  129
Gate loss:  tensor(748.4554, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5652)
idx:  130
Gate loss:  tensor(748.9840, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1653, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6922)
idx:  131
Gate loss:  tensor(749.6604, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
torch.Size([133])
torch.Size([1])
torch.Size([133])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4116)
idx:  132
Gate loss:  tensor(750.0697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4221)
idx:  133
Gate loss:  tensor(750.4830, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6457)
idx:  134
Gate loss:  tensor(751.0832, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4477)
idx:  135
Gate loss:  tensor(751.5280, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5276)
idx:  136
Gate loss:  tensor(752.0461, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3108)
idx:  137
Gate loss:  tensor(752.3566, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3234)
idx:  138
Gate loss:  tensor(752.6662, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3631)
idx:  139
Gate loss:  tensor(752.9832, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1576, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4136)
idx:  140
Gate loss:  tensor(753.3048, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3855)
idx:  141
Gate loss:  tensor(753.5749, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2211, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4450)
idx:  142
Gate loss:  tensor(754.0052, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5712)
idx:  143
Gate loss:  tensor(754.5401, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26204, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5256)
idx:  144
Gate loss:  tensor(755.0049, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5211)
idx:  145
Gate loss:  tensor(755.5117, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4791)
idx:  146
Gate loss:  tensor(755.9754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2825, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4809)
idx:  147
Gate loss:  tensor(756.4296, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5284)
idx:  148
Gate loss:  tensor(756.9551, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24421, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4996)
idx:  149
Gate loss:  tensor(757.4443, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4826)
idx:  150
Gate loss:  tensor(757.9041, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6598)
idx:  151
Gate loss:  tensor(758.5184, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13615, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6244)
idx:  152
Gate loss:  tensor(759.1411, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4661)
idx:  153
Gate loss:  tensor(759.6067, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5520)
idx:  154
Gate loss:  tensor(760.1335, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(21006, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5880)
idx:  155
Gate loss:  tensor(760.7007, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4916)
idx:  156
Gate loss:  tensor(761.1772, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5436)
idx:  157
Gate loss:  tensor(761.7095, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5709)
idx:  158
Gate loss:  tensor(762.2321, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1473, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5875)
idx:  159
Gate loss:  tensor(762.8023, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5532)
idx:  160
Gate loss:  tensor(763.3517, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6982)
idx:  161
Gate loss:  tensor(764.0494, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6784)
idx:  162
Gate loss:  tensor(764.7122, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25187, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6340)
idx:  163
Gate loss:  tensor(765.3300, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6237)
idx:  164
Gate loss:  tensor(765.9384, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5382)
idx:  165
Gate loss:  tensor(766.4697, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5382)
idx:  166
Gate loss:  tensor(766.9982, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6542)
idx:  167
Gate loss:  tensor(767.5986, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10150, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5301)
idx:  168
Gate loss:  tensor(768.1093, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4372)
idx:  169
Gate loss:  tensor(768.5458, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5647)
idx:  170
Gate loss:  tensor(769.0897, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6743, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5782)
idx:  171
Gate loss:  tensor(769.6249, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2058, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6415)
idx:  172
Gate loss:  tensor(770.2496, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5026)
idx:  173
Gate loss:  tensor(770.7401, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1105, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5929)
idx:  174
Gate loss:  tensor(771.2208, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7771)
idx:  175
Gate loss:  tensor(771.9842, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6291)
idx:  176
Gate loss:  tensor(772.5966, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6449)
idx:  177
Gate loss:  tensor(773.1945, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7797)
idx:  178
Gate loss:  tensor(773.9615, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8300)
idx:  179
Gate loss:  tensor(774.7476, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9418)
idx:  180
Gate loss:  tensor(775.6265, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1653, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0799)
idx:  181
Gate loss:  tensor(776.6976, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9439)
idx:  182
Gate loss:  tensor(777.6371, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9037)
idx:  183
Gate loss:  tensor(778.5356, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1436)
idx:  184
Gate loss:  tensor(779.3774, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
torch.Size([186])
torch.Size([1])
torch.Size([186])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1969)
idx:  185
Gate loss:  tensor(780.5260, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3837)
idx:  186
Gate loss:  tensor(781.8624, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3946)
idx:  187
Gate loss:  tensor(783.2520, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([258])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2308)
idx:  188
Gate loss:  tensor(784.3956, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8272)
idx:  189
Gate loss:  tensor(784.9802, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0020)
idx:  191
Gate loss:  tensor(785.9362, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8116)
idx:  192
Gate loss:  tensor(786.7177, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(817, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9322)
idx:  193
Gate loss:  tensor(787.6260, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1318)
idx:  194
Gate loss:  tensor(788.7488, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2925)
idx:  195
Gate loss:  tensor(790.0276, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2595)
idx:  196
Gate loss:  tensor(791.2616, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4208, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1684)
idx:  197
Gate loss:  tensor(792.4278, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8781)
idx:  198
Gate loss:  tensor(793.3013, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0320)
idx:  199
Gate loss:  tensor(794.2889, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3468)
idx:  200
Gate loss:  tensor(795.4362, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4800, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2497)
idx:  201
Gate loss:  tensor(796.4035, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9162)
idx:  202
Gate loss:  tensor(797.2913, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1246)
idx:  203
Gate loss:  tensor(798.3769, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8916)
idx:  204
Gate loss:  tensor(799.0270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2919)
idx:  206
Gate loss:  tensor(800.3113, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2088)
idx:  207
Gate loss:  tensor(801.4827, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1959)
idx:  208
Gate loss:  tensor(802.6748, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(342, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8685)
idx:  210
Gate loss:  tensor(804.4586, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7861)
idx:  211
Gate loss:  tensor(806.2272, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5210)
idx:  212
Gate loss:  tensor(807.7306, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2975)
idx:  213
Gate loss:  tensor(809.0076, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29974, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7195)
idx:  214
Gate loss:  tensor(810.5363, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4135)
idx:  215
Gate loss:  tensor(811.9391, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3963)
idx:  216
Gate loss:  tensor(813.2468, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0757)
idx:  217
Gate loss:  tensor(815.2637, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8760)
idx:  218
Gate loss:  tensor(816.9719, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9479)
idx:  219
Gate loss:  tensor(818.9019, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(342, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0143)
idx:  221
Gate loss:  tensor(820.8638, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9794)
idx:  222
Gate loss:  tensor(823.8335, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
torch.Size([224])
torch.Size([1])
torch.Size([224])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2794)
idx:  223
Gate loss:  tensor(826.1052, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2967)
idx:  224
Gate loss:  tensor(828.3813, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29922, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.6707)
idx:  225
Gate loss:  tensor(831.4130, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8160)
idx:  226
Gate loss:  tensor(834.9589, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2885)
idx:  227
Gate loss:  tensor(836.9378, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4488)
idx:  228
Gate loss:  tensor(840.2708, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([253])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.5011)
idx:  230
Gate loss:  tensor(845.6727, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
torch.Size([233])
torch.Size([1])
torch.Size([233])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.7785)
idx:  232
Gate loss:  tensor(848.3706, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([250])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8190)
idx:  233
Gate loss:  tensor(852.0992, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.5671)
idx:  234
Gate loss:  tensor(856.2389, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4800, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.0264)
idx:  236
Gate loss:  tensor(861.7413, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([258])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.3956)
idx:  237
Gate loss:  tensor(870.7888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.4817)
idx:  238
Gate loss:  tensor(881.8425, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9109, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.3084)
idx:  239
Gate loss:  tensor(891.0947, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([256])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.4268)
idx:  240
Gate loss:  tensor(902.4415, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.4298)
idx:  241
Gate loss:  tensor(914.7409, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([253])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.1514)
idx:  242
Gate loss:  tensor(927.4617, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([254])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.5173)
idx:  243
Gate loss:  tensor(939.8414, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-24.7177)
idx:  244
Gate loss:  tensor(963.5105, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-20.0088)
idx:  245
Gate loss:  tensor(983.3204, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.1825)
idx:  246
Gate loss:  tensor(1009.4034, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 229
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3012)
idx:  5
Gate loss:  tensor(1009.7006, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2944)
idx:  6
Gate loss:  tensor(1009.9940, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2933)
idx:  7
Gate loss:  tensor(1010.2808, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3248)
idx:  8
Gate loss:  tensor(1010.5668, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(701, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3689)
idx:  9
Gate loss:  tensor(1010.9224, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3202)
idx:  10
Gate loss:  tensor(1011.2405, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2920)
idx:  11
Gate loss:  tensor(1011.5298, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3291)
idx:  12
Gate loss:  tensor(1011.8575, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(366, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3358)
idx:  13
Gate loss:  tensor(1012.1882, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3186)
idx:  14
Gate loss:  tensor(1012.5055, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6673, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3456)
idx:  15
Gate loss:  tensor(1012.8422, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4569, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3108)
idx:  16
Gate loss:  tensor(1013.1423, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(475, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3329)
idx:  17
Gate loss:  tensor(1013.4670, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2574)
idx:  18
Gate loss:  tensor(1013.7232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1714)
idx:  19
Gate loss:  tensor(1013.8926, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2556)
idx:  22
Gate loss:  tensor(1014.1158, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(435, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2303)
idx:  23
Gate loss:  tensor(1014.2742, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2949)
idx:  25
Gate loss:  tensor(1014.5164, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(478, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2626)
idx:  26
Gate loss:  tensor(1014.6982, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3426)
idx:  29
Gate loss:  tensor(1014.9056, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3238)
idx:  30
Gate loss:  tensor(1015.2073, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(379, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3329)
idx:  31
Gate loss:  tensor(1015.5110, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2324)
idx:  33
Gate loss:  tensor(1015.7018, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3644)
idx:  34
Gate loss:  tensor(1016.0042, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(379, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2799)
idx:  35
Gate loss:  tensor(1016.2148, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2619)
idx:  39
Gate loss:  tensor(1016.4722, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2569)
idx:  40
Gate loss:  tensor(1016.6866, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3537)
idx:  41
Gate loss:  tensor(1017.0267, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1422, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3139)
idx:  42
Gate loss:  tensor(1017.2702, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18240, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3507)
idx:  43
Gate loss:  tensor(1017.5702, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2618)
idx:  44
Gate loss:  tensor(1017.8053, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2707)
idx:  45
Gate loss:  tensor(1018.0684, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2054)
idx:  46
Gate loss:  tensor(1018.2585, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1801)
idx:  47
Gate loss:  tensor(1018.4226, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1769)
idx:  48
Gate loss:  tensor(1018.5439, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2408)
idx:  49
Gate loss:  tensor(1018.6817, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2610)
idx:  52
Gate loss:  tensor(1018.9250, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2044)
idx:  53
Gate loss:  tensor(1019.0836, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1576, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2188)
idx:  54
Gate loss:  tensor(1019.2021, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2768)
idx:  55
Gate loss:  tensor(1019.3826, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3272)
idx:  56
Gate loss:  tensor(1019.7054, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(770, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2698)
idx:  57
Gate loss:  tensor(1019.9715, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6673, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2945)
idx:  58
Gate loss:  tensor(1020.2557, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3242)
idx:  59
Gate loss:  tensor(1020.5763, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4274)
idx:  60
Gate loss:  tensor(1021.0016, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3343)
idx:  61
Gate loss:  tensor(1021.3344, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4408)
idx:  62
Gate loss:  tensor(1021.7650, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17885, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3628)
idx:  63
Gate loss:  tensor(1022.1030, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(475, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3971)
idx:  64
Gate loss:  tensor(1022.4576, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2297)
idx:  65
Gate loss:  tensor(1022.6854, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20260, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4067)
idx:  66
Gate loss:  tensor(1023.0818, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2183, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3668)
idx:  67
Gate loss:  tensor(1023.3519, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3381)
idx:  68
Gate loss:  tensor(1023.6703, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(21669, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3136)
idx:  69
Gate loss:  tensor(1023.8689, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3238)
idx:  70
Gate loss:  tensor(1024.0913, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2586)
idx:  71
Gate loss:  tensor(1024.3407, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3120)
idx:  76
Gate loss:  tensor(1024.6293, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(435, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2819)
idx:  77
Gate loss:  tensor(1024.8811, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3573)
idx:  79
Gate loss:  tensor(1025.1305, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2929)
idx:  80
Gate loss:  tensor(1025.3386, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4183)
idx:  84
Gate loss:  tensor(1025.6957, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4109)
idx:  85
Gate loss:  tensor(1026.0795, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4347, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3768)
idx:  86
Gate loss:  tensor(1026.2750, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4393)
idx:  87
Gate loss:  tensor(1026.6971, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
torch.Size([89])
torch.Size([1])
torch.Size([89])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4645)
idx:  88
Gate loss:  tensor(1027.1500, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(323, device='cuda:1')
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
torch.Size([90])
torch.Size([1])
torch.Size([90])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3753)
idx:  89
Gate loss:  tensor(1027.4969, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(511, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3781)
idx:  93
Gate loss:  tensor(1027.8590, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3158)
idx:  94
Gate loss:  tensor(1028.1628, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4073)
idx:  95
Gate loss:  tensor(1028.5476, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4887)
idx:  96
Gate loss:  tensor(1029.0264, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5839, device='cuda:1')
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
torch.Size([98])
torch.Size([1])
torch.Size([98])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3309)
idx:  97
Gate loss:  tensor(1029.3490, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4169)
idx:  98
Gate loss:  tensor(1029.6711, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3687)
idx:  99
Gate loss:  tensor(1030.0333, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2022, device='cuda:1')
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
torch.Size([101])
torch.Size([1])
torch.Size([101])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4566)
idx:  100
Gate loss:  tensor(1030.4744, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4247)
idx:  101
Gate loss:  tensor(1030.8960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4722)
idx:  102
Gate loss:  tensor(1031.3666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
torch.Size([104])
torch.Size([1])
torch.Size([104])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4444)
idx:  103
Gate loss:  tensor(1031.8081, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11909, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4125)
idx:  104
Gate loss:  tensor(1032.2134, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3274)
idx:  105
Gate loss:  tensor(1032.5287, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3999)
idx:  107
Gate loss:  tensor(1032.8882, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4089)
idx:  108
Gate loss:  tensor(1033.2798, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4244)
idx:  109
Gate loss:  tensor(1033.6927, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(505, device='cuda:1')
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
torch.Size([111])
torch.Size([1])
torch.Size([111])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4359)
idx:  110
Gate loss:  tensor(1034.1249, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(800, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4536)
idx:  112
Gate loss:  tensor(1034.5092, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
torch.Size([114])
torch.Size([1])
torch.Size([114])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3236)
idx:  113
Gate loss:  tensor(1034.7986, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3843)
idx:  114
Gate loss:  tensor(1035.0880, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1797, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4334)
idx:  115
Gate loss:  tensor(1035.3096, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5143)
idx:  116
Gate loss:  tensor(1035.8081, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(607, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5659)
idx:  117
Gate loss:  tensor(1036.2908, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6640)
idx:  118
Gate loss:  tensor(1036.9141, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5839, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4904)
idx:  119
Gate loss:  tensor(1037.3805, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
torch.Size([121])
torch.Size([1])
torch.Size([121])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5807)
idx:  120
Gate loss:  tensor(1037.9558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(770, device='cuda:1')
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
torch.Size([122])
torch.Size([1])
torch.Size([122])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5285)
idx:  121
Gate loss:  tensor(1038.4637, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
torch.Size([123])
torch.Size([1])
torch.Size([123])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4949)
idx:  122
Gate loss:  tensor(1038.9563, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
torch.Size([124])
torch.Size([1])
torch.Size([124])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4860)
idx:  123
Gate loss:  tensor(1039.4360, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6815)
idx:  124
Gate loss:  tensor(1040.0963, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17885, device='cuda:1')
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
torch.Size([126])
torch.Size([1])
torch.Size([126])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6271)
idx:  125
Gate loss:  tensor(1040.6960, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(475, device='cuda:1')
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
torch.Size([127])
torch.Size([1])
torch.Size([127])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6337)
idx:  126
Gate loss:  tensor(1041.3015, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5081)
idx:  127
Gate loss:  tensor(1041.8065, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7557)
idx:  128
Gate loss:  tensor(1042.5283, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3718)
idx:  129
Gate loss:  tensor(1042.7435, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
torch.Size([134])
torch.Size([1])
torch.Size([134])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5028)
idx:  133
Gate loss:  tensor(1043.1923, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(674, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4054)
idx:  134
Gate loss:  tensor(1043.5850, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5392)
idx:  135
Gate loss:  tensor(1044.1136, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6673, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6101)
idx:  136
Gate loss:  tensor(1044.7025, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6673, device='cuda:1')
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
torch.Size([138])
torch.Size([1])
torch.Size([138])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6001)
idx:  137
Gate loss:  tensor(1045.2859, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4746)
idx:  138
Gate loss:  tensor(1045.7598, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1670, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4650)
idx:  139
Gate loss:  tensor(1046.0178, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
torch.Size([141])
torch.Size([1])
torch.Size([141])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6110)
idx:  140
Gate loss:  tensor(1046.6268, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6501)
idx:  141
Gate loss:  tensor(1047.2749, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8329)
idx:  142
Gate loss:  tensor(1047.9473, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29991, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5836)
idx:  143
Gate loss:  tensor(1048.3829, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19995, device='cuda:1')
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
torch.Size([145])
torch.Size([1])
torch.Size([145])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5960)
idx:  144
Gate loss:  tensor(1048.8711, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5322)
idx:  145
Gate loss:  tensor(1049.3741, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7176)
idx:  146
Gate loss:  tensor(1050.0519, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2602, device='cuda:1')
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
torch.Size([148])
torch.Size([1])
torch.Size([148])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6778)
idx:  147
Gate loss:  tensor(1050.7085, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4923)
idx:  148
Gate loss:  tensor(1051.1897, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
torch.Size([150])
torch.Size([1])
torch.Size([150])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5247)
idx:  149
Gate loss:  tensor(1051.6848, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
torch.Size([151])
torch.Size([1])
torch.Size([151])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5679)
idx:  150
Gate loss:  tensor(1052.1797, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7332)
idx:  151
Gate loss:  tensor(1052.8550, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6356)
idx:  152
Gate loss:  tensor(1053.3745, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5810)
idx:  153
Gate loss:  tensor(1053.9385, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
torch.Size([155])
torch.Size([1])
torch.Size([155])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7407)
idx:  154
Gate loss:  tensor(1054.6418, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(508, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5125)
idx:  155
Gate loss:  tensor(1055.1412, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(367, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7319)
idx:  156
Gate loss:  tensor(1055.8698, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10434, device='cuda:1')
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
torch.Size([158])
torch.Size([1])
torch.Size([158])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5199)
idx:  157
Gate loss:  tensor(1056.3822, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4753)
idx:  158
Gate loss:  tensor(1056.8558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
torch.Size([160])
torch.Size([1])
torch.Size([160])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4749)
idx:  159
Gate loss:  tensor(1057.2032, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
torch.Size([161])
torch.Size([1])
torch.Size([161])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6266)
idx:  160
Gate loss:  tensor(1057.7152, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9190, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4622)
idx:  161
Gate loss:  tensor(1057.9738, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
torch.Size([163])
torch.Size([1])
torch.Size([163])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6385)
idx:  162
Gate loss:  tensor(1058.5754, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8063)
idx:  163
Gate loss:  tensor(1059.3582, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(770, device='cuda:1')
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
torch.Size([165])
torch.Size([1])
torch.Size([165])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6785)
idx:  164
Gate loss:  tensor(1060.0023, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5828)
idx:  165
Gate loss:  tensor(1060.5826, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6172)
idx:  166
Gate loss:  tensor(1061.1847, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6755, device='cuda:1')
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
torch.Size([168])
torch.Size([1])
torch.Size([168])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5650)
idx:  167
Gate loss:  tensor(1061.7461, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7273)
idx:  168
Gate loss:  tensor(1062.4717, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7378)
idx:  169
Gate loss:  tensor(1063.0713, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9886, device='cuda:1')
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
torch.Size([171])
torch.Size([1])
torch.Size([171])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7097)
idx:  170
Gate loss:  tensor(1063.7708, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2305, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7101)
idx:  171
Gate loss:  tensor(1064.4579, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6924)
idx:  172
Gate loss:  tensor(1065.1470, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6755, device='cuda:1')
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
torch.Size([174])
torch.Size([1])
torch.Size([174])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9227)
idx:  173
Gate loss:  tensor(1066.0459, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
torch.Size([175])
torch.Size([1])
torch.Size([175])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7373)
idx:  174
Gate loss:  tensor(1066.7740, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
torch.Size([176])
torch.Size([1])
torch.Size([176])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5692)
idx:  175
Gate loss:  tensor(1067.3396, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6791)
idx:  176
Gate loss:  tensor(1067.9760, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7300)
idx:  177
Gate loss:  tensor(1068.6919, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8213)
idx:  178
Gate loss:  tensor(1069.4950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6913)
idx:  179
Gate loss:  tensor(1070.1705, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
torch.Size([181])
torch.Size([1])
torch.Size([181])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9814)
idx:  180
Gate loss:  tensor(1071.1272, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17885, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7987)
idx:  181
Gate loss:  tensor(1071.9065, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(475, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7065)
idx:  182
Gate loss:  tensor(1072.5841, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
torch.Size([184])
torch.Size([1])
torch.Size([184])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5375)
idx:  183
Gate loss:  tensor(1073.1082, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1079, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1225)
idx:  187
Gate loss:  tensor(1074.1831, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9464)
idx:  188
Gate loss:  tensor(1074.9357, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
torch.Size([190])
torch.Size([1])
torch.Size([190])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9176)
idx:  189
Gate loss:  tensor(1075.7905, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1772)
idx:  190
Gate loss:  tensor(1076.5282, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
torch.Size([192])
torch.Size([1])
torch.Size([192])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1181)
idx:  191
Gate loss:  tensor(1077.5212, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0847)
idx:  192
Gate loss:  tensor(1078.5164, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1950, device='cuda:1')
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
torch.Size([194])
torch.Size([1])
torch.Size([194])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1460)
idx:  193
Gate loss:  tensor(1079.4117, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4251, device='cuda:1')
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
torch.Size([195])
torch.Size([1])
torch.Size([195])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1723)
idx:  194
Gate loss:  tensor(1080.2273, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0651)
idx:  195
Gate loss:  tensor(1081.2804, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1665)
idx:  196
Gate loss:  tensor(1082.3408, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
torch.Size([198])
torch.Size([1])
torch.Size([198])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3134)
idx:  197
Gate loss:  tensor(1083.6122, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(770, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0675)
idx:  198
Gate loss:  tensor(1084.6628, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6673, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1413)
idx:  199
Gate loss:  tensor(1085.7625, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3316)
idx:  200
Gate loss:  tensor(1087.0817, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2563)
idx:  201
Gate loss:  tensor(1088.3230, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
torch.Size([203])
torch.Size([1])
torch.Size([203])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4906)
idx:  202
Gate loss:  tensor(1089.8036, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
torch.Size([204])
torch.Size([1])
torch.Size([204])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9616)
idx:  203
Gate loss:  tensor(1091.7256, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17885, device='cuda:1')
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
torch.Size([205])
torch.Size([1])
torch.Size([205])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7783)
idx:  204
Gate loss:  tensor(1093.4353, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(475, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6903)
idx:  205
Gate loss:  tensor(1095.0677, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3758)
idx:  206
Gate loss:  tensor(1096.4336, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4961)
idx:  207
Gate loss:  tensor(1098.6666, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2352)
idx:  208
Gate loss:  tensor(1099.4709, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6528)
idx:  209
Gate loss:  tensor(1100.6924, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
torch.Size([211])
torch.Size([1])
torch.Size([211])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4797)
idx:  210
Gate loss:  tensor(1102.1154, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1745)
idx:  211
Gate loss:  tensor(1102.9919, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3656)
idx:  212
Gate loss:  tensor(1104.2556, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1950, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5202)
idx:  213
Gate loss:  tensor(1105.5502, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6564)
idx:  214
Gate loss:  tensor(1107.1648, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2649)
idx:  215
Gate loss:  tensor(1108.3959, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2602, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6234)
idx:  216
Gate loss:  tensor(1109.9462, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6611)
idx:  217
Gate loss:  tensor(1111.5239, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
torch.Size([219])
torch.Size([1])
torch.Size([219])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5784)
idx:  218
Gate loss:  tensor(1113.0010, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0160)
idx:  219
Gate loss:  tensor(1114.8662, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3997, device='cuda:1')
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
torch.Size([221])
torch.Size([1])
torch.Size([221])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4551)
idx:  220
Gate loss:  tensor(1115.9318, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
torch.Size([222])
torch.Size([1])
torch.Size([222])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1688)
idx:  221
Gate loss:  tensor(1117.9888, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1454, device='cuda:1')
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
torch.Size([223])
torch.Size([1])
torch.Size([223])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3277)
idx:  222
Gate loss:  tensor(1119.3622, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4643)
idx:  224
Gate loss:  tensor(1121.7515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6673, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6901)
idx:  225
Gate loss:  tensor(1124.3582, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
torch.Size([227])
torch.Size([1])
torch.Size([227])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6002)
idx:  226
Gate loss:  tensor(1126.8900, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6443)
idx:  227
Gate loss:  tensor(1129.5120, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
torch.Size([229])
torch.Size([1])
torch.Size([229])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4893)
idx:  228
Gate loss:  tensor(1131.5619, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
torch.Size([230])
torch.Size([1])
torch.Size([230])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3380)
idx:  229
Gate loss:  tensor(1133.0730, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2222)
idx:  230
Gate loss:  tensor(1135.2775, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1859, device='cuda:1')
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
torch.Size([232])
torch.Size([1])
torch.Size([232])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1021)
idx:  231
Gate loss:  tensor(1137.2972, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(363, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0728)
idx:  233
Gate loss:  tensor(1141.3357, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11289, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.1913)
idx:  234
Gate loss:  tensor(1145.4403, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.8992)
idx:  235
Gate loss:  tensor(1151.1260, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17885, device='cuda:1')
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
torch.Size([237])
torch.Size([1])
torch.Size([237])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.3172)
idx:  236
Gate loss:  tensor(1156.2278, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(475, device='cuda:1')
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
torch.Size([238])
torch.Size([1])
torch.Size([238])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.5686)
idx:  237
Gate loss:  tensor(1163.7242, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.9967)
idx:  238
Gate loss:  tensor(1169.5680, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
torch.Size([240])
torch.Size([1])
torch.Size([240])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.7179)
idx:  239
Gate loss:  tensor(1176.0713, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([253])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.8879)
idx:  240
Gate loss:  tensor(1188.2515, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.5041)
idx:  241
Gate loss:  tensor(1198.9264, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
torch.Size([243])
torch.Size([1])
torch.Size([243])
Inside custom generate sample func
reasoning_path shape:  torch.Size([252])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.3967)
idx:  242
Gate loss:  tensor(1211.1470, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
torch.Size([244])
torch.Size([1])
torch.Size([244])
Inside custom generate sample func
reasoning_path shape:  torch.Size([252])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.0131)
idx:  243
Gate loss:  tensor(1225.6628, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4251, device='cuda:1')
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
torch.Size([245])
torch.Size([1])
torch.Size([245])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.7232)
idx:  244
Gate loss:  tensor(1246.5596, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
torch.Size([246])
torch.Size([1])
torch.Size([246])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-21.9831)
idx:  245
Gate loss:  tensor(1268.0092, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([259])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-26.6700)
idx:  246
Gate loss:  tensor(1292.7075, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 210
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/./train.py", line 845, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/./train.py", line 827, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/./train.py", line 394, in train
    with open(f'sampels-{batch}.json', 'w') as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 36] File name too long: "sampels-{'input_ids': tensor([[    1,   835,  4290, 29901,    13,  8439,   526,  3023,  3694,   319,\n         29892,   350, 29892,   315, 29892,   322,   360, 29889,   319,   338,\n         29871, 29906, 29955, 29889,   350,   338, 29871, 29955,  7621,  1135,\n           319, 29889,   315,   338, 29871, 29929,  3109,  1135,   350, 29889,\n           360,   338,  8951,   278,  1353,   310,   315, 29889,  8449,  1353,\n           338,   278, 10150, 29973, 29871,    13,  2277, 29937,  6466, 29901,\n            13, 12024, 29915, 29879,  8147,   278,  1819,   310,   350, 29892,\n           315, 29892,   322,   360,  4331,   491,  4331, 29889,    13,    13,\n         29909,   338,  2183,   408, 29871, 29906, 29955, 29889,    13,    13,\n         29933,   338, 29871, 29955,  7621,  1135,   319, 29892,   577, 29901,\n            13, 29933,   353,   319,   718, 29871, 29955,    13, 29933,   353,\n         29871, 29906, 29955,   718, 29871, 29955,    13, 29933,   353, 29871,\n         29941, 29946,    13,    13, 29907,   338, 29871, 29929,  3109,  1135,\n           350, 29892,   577, 29901,    13, 29907,   353,   350,   448, 29871,\n         29929,    13, 29907,   353, 29871, 29941, 29946,   448, 29871, 29929,\n            13, 29907,   353, 29871, 29906, 29945,    13,    13, 29928,   338,\n          8951,   278,  1353,   310,   315, 29892,   577, 29901,    13, 29928,\n           353, 29871, 29906,   334,   315,    13, 29928,   353, 29871, 29906,\n           334, 29871, 29906, 29945,    13, 29928,   353, 29871, 29945, 29900,\n            13,    13, 10454,   591,   505,   599,   278,  3694, 29901,    13,\n         29909,   353, 29871, 29906, 29955,    13, 29933,   353, 29871, 29941,\n         29946,    13, 29907,   353, 29871, 29906, 29945,    13, 29928,   353,\n         29871, 29945, 29900,    13,    13,  1576, 10150,  1353,  4249,   319,\n         29892,   350, 29892,   315, 29892,   322,   360,   338,   360, 29892,\n           607,   338, 29871, 29945, 29900, 29889,     2,     2],\n        [    1,   835,  4290, 29901,    13,  3644,   278,  2533,   310,   278,\n          1023,  6515,   310,   278,  3143,   382,   348,  2397,   756,  6496,\n           338, 29871, 29896, 29941, 29955, 29892,   825,   338,   278,  7200,\n          1353,   310,   278,  1023,  6515,   382,   348,  2397,   756,  6496,\n         29973, 29871,    13,  2277, 29937,  6466, 29901,    13, 10401,   263,\n          3143,   338,  6496, 29892,   278,  1023, 14870,  6515,   526,  2337,\n         18942,  3694, 29889,  4001,   278,  2533,   310,  1438,  1023,  6515,\n           338, 29871, 29896, 29941, 29955, 29892,   591,   508,   731,   701,\n           385,  6306,   304,  1284,   278,  1813,  3694, 29889,    13,    13,\n         12024,   278,  7968,  1813,  1353,   367,   921, 29889,  1987,   278,\n          7200,  1813,  1353,   338,   921,   718, 29871, 29896, 29889,    13,\n            13,  1576,  2533,   310,  1438,  1023,  6515,   338,   921,   718,\n           313, 29916,   718, 29871, 29896, 29897,   353, 29871, 29896, 29941,\n         29955, 29889,    13,    13,  1523,  2109,   292,   763,  4958, 29892,\n           591,   679, 29871, 29906, 29916,   718, 29871, 29896,   353, 29871,\n         29896, 29941, 29955, 29889,    13,    13,  4035, 29873,  1461,   292,\n         29871, 29896,   515,  1716, 11192, 29892,   591,   679, 29871, 29906,\n         29916,   353, 29871, 29896, 29941, 29953, 29889,    13,    13, 29928,\n          3640,   292,  1716, 11192,   491, 29871, 29906, 29892,   591,   679,\n           921,   353, 29871, 29953, 29947, 29889,    13,    13,  6295,   278,\n          7968,  1813,  1353,   338, 29871, 29953, 29947, 29892,   322,   278,\n          7200,  1813,  1353,   338, 29871, 29953, 29947,   718, 29871, 29896,\n           353, 29871, 29953, 29929, 29889,    13,    13,  8439,  1079, 29892,\n           278,  7200,  1353,   310,   278,  1023,  6515,   382,   348,  2397,\n           756,  6496,   338, 29871, 29953, 29929, 29889,     2],\n        [    1,   835,  4290, 29901,    13, 29909,   266,
