Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.97s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                    [0m| 0/225 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/modeling_llama.py:1373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
Preparing 4D causal attention mask with cache position
torch.Size([2492])
tensor(0.8346, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 624])
gate shape:  torch.Size([4, 624])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([486])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([486])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([486])
torch.Size([1])
torch.Size([486])
Inside custom generate sample func
torch.Size([486])
torch.Size([1])
torch.Size([486])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1225)
idx:  485
Gate loss:  tensor(2.1225, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([512])
torch.Size([1])
torch.Size([512])
Inside custom generate sample func
torch.Size([512])
torch.Size([1])
torch.Size([512])
Inside custom generate sample func
torch.Size([512])
torch.Size([1])
torch.Size([512])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5054)
idx:  511
Gate loss:  tensor(4.6280, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([592])
torch.Size([1])
torch.Size([592])
Inside custom generate sample func
torch.Size([592])
torch.Size([1])
torch.Size([592])
Inside custom generate sample func
torch.Size([592])
torch.Size([1])
torch.Size([592])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.2259)
idx:  591
Gate loss:  tensor(14.8539, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4981)
idx:  102
Gate loss:  tensor(15.3520, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
torch.Size([308])
torch.Size([1])
torch.Size([308])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8100)
idx:  307
Gate loss:  tensor(16.1619, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0005)
idx:  377
Gate loss:  tensor(17.1624, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2615)
idx:  114
Gate loss:  tensor(17.4240, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3977)
idx:  288
Gate loss:  tensor(17.8217, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([571])
torch.Size([1])
torch.Size([571])
Inside custom generate sample func
torch.Size([571])
torch.Size([1])
torch.Size([571])
Inside custom generate sample func
torch.Size([571])
torch.Size([1])
torch.Size([571])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4443)
idx:  570
Gate loss:  tensor(20.2660, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2919, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2172)
idx:  190
Gate loss:  tensor(20.4832, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29898, device='cuda:1')
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
torch.Size([206])
torch.Size([1])
torch.Size([206])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2158)
idx:  205
Gate loss:  tensor(20.6990, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
torch.Size([335])
torch.Size([1])
torch.Size([335])
Inside custom generate sample func
reasoning_path shape:  torch.Size([635])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2190)
idx:  334
Gate loss:  tensor(20.9180, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Training Epoch: 1/1, step 0/225 completed (loss: 0.834577202796936):   0%|[34m                        [0m| 1/225 [00:29<1:49:05, 29.22s/it][0m
Preparing 4D causal attention mask with cache position
torch.Size([1480])
tensor(4.4568, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 371])
gate shape:  torch.Size([4, 371])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3124)
idx:  63
Gate loss:  tensor(0.3124, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4251, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2939)
idx:  72
Gate loss:  tensor(0.6064, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6559)
idx:  146
Gate loss:  tensor(1.2623, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(726, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9577)
idx:  165
Gate loss:  tensor(2.2200, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7504, device='cuda:1')
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
torch.Size([220])
torch.Size([1])
torch.Size([220])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9168)
idx:  219
Gate loss:  tensor(3.1368, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(317, device='cuda:1')
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
torch.Size([287])
torch.Size([1])
torch.Size([287])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5797)
idx:  286
Gate loss:  tensor(4.7165, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2397, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1996)
idx:  50
Gate loss:  tensor(4.9161, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3220)
idx:  98
Gate loss:  tensor(5.2381, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3880)
idx:  207
Gate loss:  tensor(5.6260, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2712)
idx:  68
Gate loss:  tensor(5.8972, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
torch.Size([241])
torch.Size([1])
torch.Size([241])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2583)
idx:  240
Gate loss:  tensor(7.1555, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
torch.Size([313])
torch.Size([1])
torch.Size([313])
Inside custom generate sample func
reasoning_path shape:  torch.Size([382])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2644)
idx:  312
Gate loss:  tensor(8.4198, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1100])
tensor(1.9314, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 276])
gate shape:  torch.Size([4, 276])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3263)
idx:  2
Gate loss:  tensor(0.3263, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4096)
idx:  43
Gate loss:  tensor(0.7359, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1304, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1827)
idx:  198
Gate loss:  tensor(1.9186, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2699, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3508)
idx:  16
Gate loss:  tensor(2.2694, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2354)
idx:  42
Gate loss:  tensor(2.5049, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
torch.Size([152])
torch.Size([1])
torch.Size([152])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5310)
idx:  151
Gate loss:  tensor(3.0358, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(18046, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3765)
idx:  73
Gate loss:  tensor(3.4123, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4094, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4820)
idx:  106
Gate loss:  tensor(3.8943, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
torch.Size([208])
torch.Size([1])
torch.Size([208])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7682)
idx:  207
Gate loss:  tensor(4.6625, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
torch.Size([106])
torch.Size([1])
torch.Size([106])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2316)
idx:  105
Gate loss:  tensor(4.8941, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
torch.Size([116])
torch.Size([1])
torch.Size([116])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2482)
idx:  115
Gate loss:  tensor(5.1423, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1269, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([287])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4058)
idx:  119
Gate loss:  tensor(5.5481, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1220])
tensor(1.5940, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 306])
gate shape:  torch.Size([4, 306])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
torch.Size([173])
torch.Size([1])
torch.Size([173])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3334)
idx:  172
Gate loss:  tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6645)
idx:  212
Gate loss:  tensor(0.9980, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(26563, device='cuda:1')
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
torch.Size([297])
torch.Size([1])
torch.Size([297])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.7484)
idx:  296
Gate loss:  tensor(7.7464, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
torch.Size([135])
torch.Size([1])
torch.Size([135])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4203)
idx:  134
Gate loss:  tensor(8.1667, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5150)
idx:  156
Gate loss:  tensor(8.6817, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12770, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6098)
idx:  188
Gate loss:  tensor(9.2915, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3530)
idx:  118
Gate loss:  tensor(9.6445, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
torch.Size([214])
torch.Size([1])
torch.Size([214])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4356)
idx:  213
Gate loss:  tensor(10.0801, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
torch.Size([262])
torch.Size([1])
torch.Size([262])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8592)
idx:  261
Gate loss:  tensor(11.9393, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1917)
idx:  51
Gate loss:  tensor(12.1311, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(302, device='cuda:1')
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
torch.Size([180])
torch.Size([1])
torch.Size([180])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3386)
idx:  179
Gate loss:  tensor(12.4697, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([317])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5477)
idx:  192
Gate loss:  tensor(13.0174, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1548])
tensor(1.2153, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 388])
gate shape:  torch.Size([4, 388])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1915)
idx:  35
Gate loss:  tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2165)
idx:  43
Gate loss:  tensor(0.4080, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
torch.Size([378])
torch.Size([1])
torch.Size([378])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.4476)
idx:  377
Gate loss:  tensor(6.8556, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(17800, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2870)
idx:  112
Gate loss:  tensor(7.1425, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2375)
idx:  131
Gate loss:  tensor(7.3800, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13105, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4173)
idx:  230
Gate loss:  tensor(7.7973, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2372)
idx:  166
Gate loss:  tensor(8.0345, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
torch.Size([242])
torch.Size([1])
torch.Size([242])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3391)
idx:  241
Gate loss:  tensor(8.3737, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
torch.Size([300])
torch.Size([1])
torch.Size([300])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5201)
idx:  299
Gate loss:  tensor(8.8938, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
torch.Size([213])
torch.Size([1])
torch.Size([213])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3559)
idx:  212
Gate loss:  tensor(9.2497, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
torch.Size([289])
torch.Size([1])
torch.Size([289])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4458)
idx:  288
Gate loss:  tensor(9.6955, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
torch.Size([295])
torch.Size([1])
torch.Size([295])
Inside custom generate sample func
reasoning_path shape:  torch.Size([399])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5587)
idx:  294
Gate loss:  tensor(10.2542, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([800])
tensor(1.1239, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 201])
gate shape:  torch.Size([4, 201])
The shape of hidden_states: torch.Size([4096])
The topk tensor(3841, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1470)
idx:  153
Gate loss:  tensor(1.1470, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(674, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5231)
idx:  184
Gate loss:  tensor(3.6701, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([211])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.3093)
idx:  192
Gate loss:  tensor(10.9793, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(2719, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4864)
idx:  39
Gate loss:  tensor(11.4657, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4196)
idx:  41
Gate loss:  tensor(11.8852, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2719, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5913)
idx:  117
Gate loss:  tensor(12.4765, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3908)
idx:  19
Gate loss:  tensor(12.8673, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1235, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3093)
idx:  63
Gate loss:  tensor(13.1766, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1372, device='cuda:1')
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
torch.Size([91])
torch.Size([1])
torch.Size([91])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6035)
idx:  90
Gate loss:  tensor(13.7801, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3943)
idx:  80
Gate loss:  tensor(14.1744, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
torch.Size([103])
torch.Size([1])
torch.Size([103])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4587)
idx:  102
Gate loss:  tensor(14.6331, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(386, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([212])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3430)
idx:  146
Gate loss:  tensor(15.9762, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1048])
tensor(1.3746, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 263])
gate shape:  torch.Size([4, 263])
The shape of hidden_states: torch.Size([4096])
The topk tensor(14267, device='cuda:1')
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
torch.Size([199])
torch.Size([1])
torch.Size([199])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4311)
idx:  198
Gate loss:  tensor(1.4311, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(847, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4072)
idx:  206
Gate loss:  tensor(2.8383, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
torch.Size([239])
torch.Size([1])
torch.Size([239])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4826)
idx:  238
Gate loss:  tensor(6.3209, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5906)
idx:  138
Gate loss:  tensor(6.9115, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6264)
idx:  145
Gate loss:  tensor(7.5379, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7005)
idx:  171
Gate loss:  tensor(8.2384, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3074)
idx:  22
Gate loss:  tensor(8.5458, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3480)
idx:  36
Gate loss:  tensor(8.8938, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8572, device='cuda:1')
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
reasoning_path shape:  torch.Size([272])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-15.7698)
idx:  259
Gate loss:  tensor(24.6636, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2359)
idx:  106
Gate loss:  tensor(24.8996, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
torch.Size([166])
torch.Size([1])
torch.Size([166])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5616)
idx:  165
Gate loss:  tensor(25.4612, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
torch.Size([225])
torch.Size([1])
torch.Size([225])
Inside custom generate sample func
reasoning_path shape:  torch.Size([274])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1823)
idx:  224
Gate loss:  tensor(26.6435, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1380])
tensor(1.2125, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 346])
gate shape:  torch.Size([4, 346])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3995)
idx:  78
Gate loss:  tensor(0.3995, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12066, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6286)
idx:  136
Gate loss:  tensor(1.0281, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([351])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9802)
idx:  186
Gate loss:  tensor(2.0083, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(11536, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3726)
idx:  136
Gate loss:  tensor(2.3809, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
torch.Size([189])
torch.Size([1])
torch.Size([189])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4826)
idx:  188
Gate loss:  tensor(2.8636, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1078, device='cuda:1')
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
torch.Size([331])
torch.Size([1])
torch.Size([331])
Inside custom generate sample func
reasoning_path shape:  torch.Size([349])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.5709)
idx:  330
Gate loss:  tensor(11.4345, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
torch.Size([177])
torch.Size([1])
torch.Size([177])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2954)
idx:  176
Gate loss:  tensor(11.7299, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(567, device='cuda:1')
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
torch.Size([234])
torch.Size([1])
torch.Size([234])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8232)
idx:  233
Gate loss:  tensor(12.5531, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(348, device='cuda:1')
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
torch.Size([250])
torch.Size([1])
torch.Size([250])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8783)
idx:  249
Gate loss:  tensor(13.4313, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(6730, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1349)
idx:  59
Gate loss:  tensor(13.5662, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(445, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4081)
idx:  186
Gate loss:  tensor(13.9743, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
torch.Size([301])
torch.Size([1])
torch.Size([301])
Inside custom generate sample func
reasoning_path shape:  torch.Size([357])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4165)
idx:  300
Gate loss:  tensor(15.3909, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([556])
tensor(1.3036, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 140])
gate shape:  torch.Size([4, 140])
The shape of hidden_states: torch.Size([4096])
The topk tensor(3064, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6428)
idx:  6
Gate loss:  tensor(0.6428, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5412)
idx:  14
Gate loss:  tensor(1.1840, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13340, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7153)
idx:  18
Gate loss:  tensor(1.8993, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3860)
idx:  116
Gate loss:  tensor(5.2853, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1105, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8700)
idx:  118
Gate loss:  tensor(9.1553, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-24.0701)
idx:  138
Gate loss:  tensor(33.2255, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7857)
idx:  43
Gate loss:  tensor(34.0111, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1950, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2715)
idx:  59
Gate loss:  tensor(35.2826, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4487)
idx:  93
Gate loss:  tensor(36.7313, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(835, device='cuda:1')
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5131)
idx:  0
Gate loss:  tensor(37.2443, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6753)
idx:  66
Gate loss:  tensor(37.9196, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30646, device='cuda:1')
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
torch.Size([115])
torch.Size([1])
torch.Size([115])
Inside custom generate sample func
reasoning_path shape:  torch.Size([151])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6389)
idx:  114
Gate loss:  tensor(39.5585, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([248])
tensor(1.4052, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 63])
gate shape:  torch.Size([4, 63])
The shape of hidden_states: torch.Size([4096])
The topk tensor(835, device='cuda:1')
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
torch.Size([1])
torch.Size([1])
torch.Size([1])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1299)
idx:  0
Gate loss:  tensor(2.1299, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8377)
idx:  10
Gate loss:  tensor(4.9675, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.7624)
idx:  44
Gate loss:  tensor(12.7299, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6585)
idx:  7
Gate loss:  tensor(14.3884, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.6635)
idx:  45
Gate loss:  tensor(21.0519, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.8320)
idx:  47
Gate loss:  tensor(28.8839, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9530)
idx:  15
Gate loss:  tensor(29.8369, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3078)
idx:  21
Gate loss:  tensor(31.1447, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.1913)
idx:  58
Gate loss:  tensor(40.3360, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29899, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0606)
idx:  6
Gate loss:  tensor(41.3966, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3759)
idx:  36
Gate loss:  tensor(43.7725, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1363, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([74])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.6935)
idx:  55
Gate loss:  tensor(50.4660, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([1144])
tensor(1.2216, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 287])
gate shape:  torch.Size([4, 287])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3614)
idx:  23
Gate loss:  tensor(0.3614, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2277, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2324)
idx:  55
Gate loss:  tensor(0.5938, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2534, device='cuda:1')
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
torch.Size([283])
torch.Size([1])
torch.Size([283])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.6593)
idx:  282
Gate loss:  tensor(17.2531, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2064)
idx:  71
Gate loss:  tensor(17.4596, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6466, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2454)
idx:  75
Gate loss:  tensor(17.7050, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3542)
idx:  106
Gate loss:  tensor(18.0591, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2717)
idx:  14
Gate loss:  tensor(18.3308, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(265, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4192)
idx:  52
Gate loss:  tensor(18.7500, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3103)
idx:  108
Gate loss:  tensor(19.0603, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29879, device='cuda:1')
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
torch.Size([95])
torch.Size([1])
torch.Size([95])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2832)
idx:  94
Gate loss:  tensor(19.3435, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7153)
idx:  225
Gate loss:  tensor(20.0588, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25402, device='cuda:1')
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
torch.Size([284])
torch.Size([1])
torch.Size([284])
Inside custom generate sample func
reasoning_path shape:  torch.Size([298])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.0931)
idx:  283
Gate loss:  tensor(36.1519, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([860])
tensor(1.3736, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 216])
gate shape:  torch.Size([4, 216])
The shape of hidden_states: torch.Size([4096])
The topk tensor(13096, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6741)
idx:  80
Gate loss:  tensor(0.6741, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3309, device='cuda:1')
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
torch.Size([100])
torch.Size([1])
torch.Size([100])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6786)
idx:  99
Gate loss:  tensor(1.3527, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2699, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([220])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-18.7153)
idx:  211
Gate loss:  tensor(20.0680, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4514)
idx:  19
Gate loss:  tensor(20.5194, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
torch.Size([110])
torch.Size([1])
torch.Size([110])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8023)
idx:  109
Gate loss:  tensor(21.3217, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([222])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.8870)
idx:  201
Gate loss:  tensor(26.2087, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([219])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7598)
idx:  145
Gate loss:  tensor(26.9684, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
torch.Size([149])
torch.Size([1])
torch.Size([149])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5861)
idx:  148
Gate loss:  tensor(27.5546, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
torch.Size([212])
torch.Size([1])
torch.Size([212])
Inside custom generate sample func
reasoning_path shape:  torch.Size([221])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.1424)
idx:  211
Gate loss:  tensor(40.6970, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(3291, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([219])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5411)
idx:  79
Gate loss:  tensor(41.2380, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11192, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0313)
idx:  146
Gate loss:  tensor(42.2694, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([227])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.2017)
idx:  199
Gate loss:  tensor(45.4711, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([2036])
tensor(1.2977, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 510])
gate shape:  torch.Size([4, 510])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
torch.Size([113])
torch.Size([1])
torch.Size([113])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6676)
idx:  112
Gate loss:  tensor(0.6676, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7958)
idx:  199
Gate loss:  tensor(1.4634, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(922, device='cuda:1')
torch.Size([485])
torch.Size([1])
torch.Size([485])
Inside custom generate sample func
torch.Size([485])
torch.Size([1])
torch.Size([485])
Inside custom generate sample func
torch.Size([485])
torch.Size([1])
torch.Size([485])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.4213)
idx:  484
Gate loss:  tensor(10.8847, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4312, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4224)
idx:  178
Gate loss:  tensor(11.3071, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4631, device='cuda:1')
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
torch.Size([321])
torch.Size([1])
torch.Size([321])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5938)
idx:  320
Gate loss:  tensor(11.9009, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
torch.Size([357])
torch.Size([1])
torch.Size([357])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9317)
idx:  356
Gate loss:  tensor(12.8326, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
torch.Size([228])
torch.Size([1])
torch.Size([228])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4777)
idx:  227
Gate loss:  tensor(13.3103, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5222)
idx:  265
Gate loss:  tensor(13.8324, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([480])
torch.Size([1])
torch.Size([480])
Inside custom generate sample func
torch.Size([480])
torch.Size([1])
torch.Size([480])
Inside custom generate sample func
torch.Size([480])
torch.Size([1])
torch.Size([480])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.9470)
idx:  479
Gate loss:  tensor(17.7795, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2721)
idx:  48
Gate loss:  tensor(18.0516, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2387)
idx:  265
Gate loss:  tensor(18.2902, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7586, device='cuda:1')
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
torch.Size([359])
torch.Size([1])
torch.Size([359])
Inside custom generate sample func
reasoning_path shape:  torch.Size([521])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4688)
idx:  358
Gate loss:  tensor(18.7591, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([1168])
tensor(1.2507, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 293])
gate shape:  torch.Size([4, 293])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2714)
idx:  118
Gate loss:  tensor(0.2714, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
torch.Size([132])
torch.Size([1])
torch.Size([132])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3992)
idx:  131
Gate loss:  tensor(0.6706, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
torch.Size([266])
torch.Size([1])
torch.Size([266])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4553)
idx:  265
Gate loss:  tensor(2.1258, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2398)
idx:  81
Gate loss:  tensor(2.3657, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5047)
idx:  169
Gate loss:  tensor(2.8703, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19087, device='cuda:1')
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
torch.Size([265])
torch.Size([1])
torch.Size([265])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1761)
idx:  264
Gate loss:  tensor(4.0464, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2272)
idx:  43
Gate loss:  tensor(4.2736, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3694, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1784)
idx:  56
Gate loss:  tensor(4.4520, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
torch.Size([182])
torch.Size([1])
torch.Size([182])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4864)
idx:  181
Gate loss:  tensor(4.9384, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(297, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2770)
idx:  40
Gate loss:  tensor(5.2154, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1082, device='cuda:1')
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
torch.Size([162])
torch.Size([1])
torch.Size([162])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4601)
idx:  161
Gate loss:  tensor(5.6755, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
torch.Size([191])
torch.Size([1])
torch.Size([191])
Inside custom generate sample func
reasoning_path shape:  torch.Size([304])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5256)
idx:  190
Gate loss:  tensor(6.2011, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1340])
tensor(1.2781, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 336])
gate shape:  torch.Size([4, 336])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1472, device='cuda:1')
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
torch.Size([112])
torch.Size([1])
torch.Size([112])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3037)
idx:  111
Gate loss:  tensor(0.3037, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
torch.Size([159])
torch.Size([1])
torch.Size([159])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3639)
idx:  158
Gate loss:  tensor(0.6675, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
torch.Size([201])
torch.Size([1])
torch.Size([201])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5896)
idx:  200
Gate loss:  tensor(1.2571, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(11909, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4040)
idx:  139
Gate loss:  tensor(1.6612, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(19995, device='cuda:1')
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
torch.Size([254])
torch.Size([1])
torch.Size([254])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8221)
idx:  253
Gate loss:  tensor(2.4832, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11536, device='cuda:1')
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
torch.Size([260])
torch.Size([1])
torch.Size([260])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7585)
idx:  259
Gate loss:  tensor(3.2418, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(1065, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1904)
idx:  43
Gate loss:  tensor(3.4322, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
torch.Size([326])
torch.Size([1])
torch.Size([326])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1415)
idx:  325
Gate loss:  tensor(6.5737, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(27881, device='cuda:1')
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
torch.Size([333])
torch.Size([1])
torch.Size([333])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-15.2070)
idx:  332
Gate loss:  tensor(21.7807, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([346])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1958)
idx:  50
Gate loss:  tensor(21.9765, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29914, device='cuda:1')
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
torch.Size([235])
torch.Size([1])
torch.Size([235])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5760)
idx:  234
Gate loss:  tensor(22.5525, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
torch.Size([306])
torch.Size([1])
torch.Size([306])
Inside custom generate sample func
reasoning_path shape:  torch.Size([347])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0998)
idx:  305
Gate loss:  tensor(23.6523, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([420])
tensor(1.8239, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 106])
gate shape:  torch.Size([4, 106])
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1847)
idx:  7
Gate loss:  tensor(1.1847, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4371)
idx:  29
Gate loss:  tensor(2.6218, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(17040, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6736)
idx:  35
Gate loss:  tensor(4.2954, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7667)
idx:  18
Gate loss:  tensor(5.0620, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(623, device='cuda:1')
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
torch.Size([96])
torch.Size([1])
torch.Size([96])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.6929)
idx:  95
Gate loss:  tensor(14.7549, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.5969)
idx:  104
Gate loss:  tensor(31.3518, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4347, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4048)
idx:  14
Gate loss:  tensor(32.7566, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3942)
idx:  38
Gate loss:  tensor(34.1509, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2046)
idx:  45
Gate loss:  tensor(35.3554, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(8041, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8593)
idx:  15
Gate loss:  tensor(36.2147, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6515)
idx:  43
Gate loss:  tensor(36.8662, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([117])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1052)
idx:  71
Gate loss:  tensor(38.9714, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1004])
tensor(1.3607, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 252])
gate shape:  torch.Size([4, 252])
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3992)
idx:  61
Gate loss:  tensor(0.3992, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0002)
idx:  171
Gate loss:  tensor(1.3994, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7913)
idx:  208
Gate loss:  tensor(3.1907, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(11953, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4121)
idx:  30
Gate loss:  tensor(3.6028, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3207)
idx:  41
Gate loss:  tensor(3.9235, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2135, device='cuda:1')
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
torch.Size([130])
torch.Size([1])
torch.Size([130])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5944)
idx:  129
Gate loss:  tensor(4.5179, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3562)
idx:  54
Gate loss:  tensor(4.8742, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
torch.Size([154])
torch.Size([1])
torch.Size([154])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4527)
idx:  153
Gate loss:  tensor(5.3268, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
torch.Size([226])
torch.Size([1])
torch.Size([226])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2559)
idx:  225
Gate loss:  tensor(7.5828, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2592)
idx:  70
Gate loss:  tensor(7.8420, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3526)
idx:  116
Gate loss:  tensor(8.1946, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(448, device='cuda:1')
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
torch.Size([178])
torch.Size([1])
torch.Size([178])
Inside custom generate sample func
reasoning_path shape:  torch.Size([263])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5571)
idx:  177
Gate loss:  tensor(8.7517, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1072])
tensor(0.9958, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 269])
gate shape:  torch.Size([4, 269])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
torch.Size([3])
torch.Size([1])
torch.Size([3])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2498)
idx:  2
Gate loss:  tensor(0.2498, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
torch.Size([117])
torch.Size([1])
torch.Size([117])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3949)
idx:  116
Gate loss:  tensor(0.6447, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(298, device='cuda:1')
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
torch.Size([169])
torch.Size([1])
torch.Size([169])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8066)
idx:  168
Gate loss:  tensor(1.4514, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
torch.Size([157])
torch.Size([1])
torch.Size([157])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4816)
idx:  156
Gate loss:  tensor(1.9330, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3946)
idx:  163
Gate loss:  tensor(2.3276, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
torch.Size([248])
torch.Size([1])
torch.Size([248])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5829)
idx:  247
Gate loss:  tensor(3.9105, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
torch.Size([153])
torch.Size([1])
torch.Size([153])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4956)
idx:  152
Gate loss:  tensor(4.4061, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8345)
idx:  201
Gate loss:  tensor(5.2406, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
torch.Size([236])
torch.Size([1])
torch.Size([236])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5627)
idx:  235
Gate loss:  tensor(6.8033, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
torch.Size([193])
torch.Size([1])
torch.Size([193])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7758)
idx:  192
Gate loss:  tensor(7.5791, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1716, device='cuda:1')
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
torch.Size([210])
torch.Size([1])
torch.Size([210])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7852)
idx:  209
Gate loss:  tensor(8.3643, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
torch.Size([218])
torch.Size([1])
torch.Size([218])
Inside custom generate sample func
reasoning_path shape:  torch.Size([280])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6321)
idx:  217
Gate loss:  tensor(8.9965, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1508])
tensor(1.0055, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 378])
gate shape:  torch.Size([4, 378])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2394)
idx:  59
Gate loss:  tensor(0.2394, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
torch.Size([99])
torch.Size([1])
torch.Size([99])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4193)
idx:  98
Gate loss:  tensor(0.6587, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
torch.Size([207])
torch.Size([1])
torch.Size([207])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5255)
idx:  206
Gate loss:  tensor(1.1842, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1755)
idx:  28
Gate loss:  tensor(1.3596, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29909, device='cuda:1')
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
torch.Size([349])
torch.Size([1])
torch.Size([349])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6854)
idx:  348
Gate loss:  tensor(3.0450, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([377])
torch.Size([1])
torch.Size([377])
Inside custom generate sample func
torch.Size([377])
torch.Size([1])
torch.Size([377])
Inside custom generate sample func
torch.Size([377])
torch.Size([1])
torch.Size([377])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.2077)
idx:  376
Gate loss:  tensor(19.2527, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4208)
idx:  201
Gate loss:  tensor(19.6736, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
torch.Size([286])
torch.Size([1])
torch.Size([286])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7151)
idx:  285
Gate loss:  tensor(20.3887, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
torch.Size([344])
torch.Size([1])
torch.Size([344])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8427)
idx:  343
Gate loss:  tensor(22.2314, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(334, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3313)
idx:  127
Gate loss:  tensor(22.5628, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
torch.Size([196])
torch.Size([1])
torch.Size([196])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4294)
idx:  195
Gate loss:  tensor(22.9922, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(20745, device='cuda:1')
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
torch.Size([251])
torch.Size([1])
torch.Size([251])
Inside custom generate sample func
reasoning_path shape:  torch.Size([389])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3783)
idx:  250
Gate loss:  tensor(23.3705, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([756])
tensor(1.0904, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 190])
gate shape:  torch.Size([4, 190])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8952)
idx:  29
Gate loss:  tensor(0.8952, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6466, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4709)
idx:  54
Gate loss:  tensor(1.3661, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1849, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([200])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.1745)
idx:  178
Gate loss:  tensor(8.5406, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(6466, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3866)
idx:  47
Gate loss:  tensor(8.9272, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1023, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4514)
idx:  53
Gate loss:  tensor(9.3786, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2909, device='cuda:1')
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
torch.Size([136])
torch.Size([1])
torch.Size([136])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3943)
idx:  135
Gate loss:  tensor(10.7730, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5103)
idx:  87
Gate loss:  tensor(11.2833, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3250, device='cuda:1')
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
torch.Size([125])
torch.Size([1])
torch.Size([125])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9725)
idx:  124
Gate loss:  tensor(12.2557, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1144, device='cuda:1')
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
torch.Size([170])
torch.Size([1])
torch.Size([170])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1675)
idx:  169
Gate loss:  tensor(15.4232, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5299)
idx:  101
Gate loss:  tensor(15.9532, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
torch.Size([142])
torch.Size([1])
torch.Size([142])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0490)
idx:  141
Gate loss:  tensor(17.0022, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
torch.Size([188])
torch.Size([1])
torch.Size([188])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.5699)
idx:  187
Gate loss:  tensor(28.5721, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
torch.Size([760])
tensor(1.0777, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 191])
gate shape:  torch.Size([4, 191])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
torch.Size([128])
torch.Size([1])
torch.Size([128])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4998)
idx:  127
Gate loss:  tensor(0.4998, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(366, device='cuda:1')
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
torch.Size([143])
torch.Size([1])
torch.Size([143])
Inside custom generate sample func
reasoning_path shape:  torch.Size([201])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0441)
idx:  142
Gate loss:  tensor(1.5439, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3058, device='cuda:1')
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
torch.Size([183])
torch.Size([1])
torch.Size([183])
Inside custom generate sample func
reasoning_path shape:  torch.Size([196])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.2072)
idx:  182
Gate loss:  tensor(6.7512, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29916, device='cuda:1')
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
torch.Size([164])
torch.Size([1])
torch.Size([164])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1770)
idx:  163
Gate loss:  tensor(7.9282, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
torch.Size([172])
torch.Size([1])
torch.Size([172])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4402)
idx:  171
Gate loss:  tensor(10.3684, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
torch.Size([185])
torch.Size([1])
torch.Size([185])
Inside custom generate sample func
reasoning_path shape:  torch.Size([198])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.4396)
idx:  184
Gate loss:  tensor(15.8080, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(7477, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4139)
idx:  96
Gate loss:  tensor(16.2219, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5302)
idx:  104
Gate loss:  tensor(16.7521, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(491, device='cuda:1')
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
torch.Size([120])
torch.Size([1])
torch.Size([120])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5979)
idx:  119
Gate loss:  tensor(17.3500, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4469)
idx:  61
Gate loss:  tensor(17.7969, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2436, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4405)
idx:  77
Gate loss:  tensor(18.2374, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(921, device='cuda:1')
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
torch.Size([105])
torch.Size([1])
torch.Size([105])
Inside custom generate sample func
reasoning_path shape:  torch.Size([202])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5955)
idx:  104
Gate loss:  tensor(18.8329, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([656])
tensor(1.1731, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 165])
gate shape:  torch.Size([4, 165])
The shape of hidden_states: torch.Size([4096])
The topk tensor(4290, device='cuda:1')
torch.Size([2])
torch.Size([1])
torch.Size([2])
Inside custom generate sample func
torch.Size([2])
torch.Size([1])
torch.Size([2])
Inside custom generate sample func
torch.Size([2])
torch.Size([1])
torch.Size([2])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3757)
idx:  1
Gate loss:  tensor(0.3757, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5902)
idx:  29
Gate loss:  tensor(0.9659, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
torch.Size([119])
torch.Size([1])
torch.Size([119])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7616)
idx:  118
Gate loss:  tensor(2.7275, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3806)
idx:  66
Gate loss:  tensor(3.1080, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
torch.Size([97])
torch.Size([1])
torch.Size([97])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4961)
idx:  96
Gate loss:  tensor(3.6041, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
torch.Size([131])
torch.Size([1])
torch.Size([131])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1787)
idx:  130
Gate loss:  tensor(4.7828, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(428, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7921)
idx:  79
Gate loss:  tensor(5.5749, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3018, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4411)
idx:  117
Gate loss:  tensor(7.0160, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 2
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6516)
idx:  50
Gate loss:  tensor(7.6676, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5717)
idx:  84
Gate loss:  tensor(8.2392, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
torch.Size([108])
torch.Size([1])
torch.Size([108])
Inside custom generate sample func
reasoning_path shape:  torch.Size([176])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0426)
idx:  107
Gate loss:  tensor(9.2818, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1028])
tensor(1.2225, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 258])
gate shape:  torch.Size([4, 258])
The shape of hidden_states: torch.Size([4096])
The topk tensor(12024, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3236)
idx:  49
Gate loss:  tensor(0.3236, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1284, device='cuda:1')
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
torch.Size([216])
torch.Size([1])
torch.Size([216])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0759)
idx:  215
Gate loss:  tensor(1.3995, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(324, device='cuda:1')
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
torch.Size([247])
torch.Size([1])
torch.Size([247])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.8883)
idx:  246
Gate loss:  tensor(8.2879, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(23197, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3028)
idx:  25
Gate loss:  tensor(8.5907, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3318)
idx:  69
Gate loss:  tensor(8.9224, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
torch.Size([200])
torch.Size([1])
torch.Size([200])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0751)
idx:  199
Gate loss:  tensor(9.9976, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
torch.Size([140])
torch.Size([1])
torch.Size([140])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3886)
idx:  139
Gate loss:  tensor(10.3861, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
torch.Size([146])
torch.Size([1])
torch.Size([146])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5111)
idx:  145
Gate loss:  tensor(10.8972, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
torch.Size([202])
torch.Size([1])
torch.Size([202])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6753)
idx:  201
Gate loss:  tensor(11.5725, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(4023, device='cuda:1')
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
torch.Size([102])
torch.Size([1])
torch.Size([102])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2325)
idx:  101
Gate loss:  tensor(11.8049, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
torch.Size([167])
torch.Size([1])
torch.Size([167])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5784)
idx:  166
Gate loss:  tensor(12.3834, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
torch.Size([217])
torch.Size([1])
torch.Size([217])
Inside custom generate sample func
reasoning_path shape:  torch.Size([269])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4179)
idx:  216
Gate loss:  tensor(13.8013, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([936])
tensor(1.0527, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 235])
gate shape:  torch.Size([4, 235])
The shape of hidden_states: torch.Size([4096])
The topk tensor(612, device='cuda:1')
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
torch.Size([129])
torch.Size([1])
torch.Size([129])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4451)
idx:  128
Gate loss:  tensor(0.4451, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
torch.Size([137])
torch.Size([1])
torch.Size([137])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4764)
idx:  136
Gate loss:  tensor(0.9215, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
torch.Size([156])
torch.Size([1])
torch.Size([156])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6355)
idx:  155
Gate loss:  tensor(1.5571, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(289, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1342)
idx:  30
Gate loss:  tensor(1.6912, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
torch.Size([147])
torch.Size([1])
torch.Size([147])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4215)
idx:  146
Gate loss:  tensor(2.1127, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
torch.Size([209])
torch.Size([1])
torch.Size([209])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6071)
idx:  208
Gate loss:  tensor(3.7198, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(623, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5958)
idx:  10
Gate loss:  tensor(4.3156, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
torch.Size([94])
torch.Size([1])
torch.Size([94])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4942)
idx:  93
Gate loss:  tensor(4.8099, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
torch.Size([187])
torch.Size([1])
torch.Size([187])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7973)
idx:  186
Gate loss:  tensor(5.6071, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29875, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3192)
idx:  14
Gate loss:  tensor(5.9263, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8332, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.5213)
idx:  138
Gate loss:  tensor(6.4476, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
torch.Size([179])
torch.Size([1])
torch.Size([179])
Inside custom generate sample func
reasoning_path shape:  torch.Size([246])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6229)
idx:  178
Gate loss:  tensor(7.0704, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([1384])
tensor(1.3060, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 347])
gate shape:  torch.Size([4, 347])
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
torch.Size([139])
torch.Size([1])
torch.Size([139])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2892)
idx:  138
Gate loss:  tensor(0.2892, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
torch.Size([144])
torch.Size([1])
torch.Size([144])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1853)
idx:  143
Gate loss:  tensor(0.4745, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1612, device='cuda:1')
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
torch.Size([324])
torch.Size([1])
torch.Size([324])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3220)
idx:  323
Gate loss:  tensor(2.7966, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2523)
idx:  83
Gate loss:  tensor(3.0489, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3001, device='cuda:1')
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
torch.Size([107])
torch.Size([1])
torch.Size([107])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.3427)
idx:  106
Gate loss:  tensor(3.3916, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5837, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2651)
idx:  108
Gate loss:  tensor(3.6567, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1387)
idx:  61
Gate loss:  tensor(3.7954, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(472, device='cuda:1')
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
torch.Size([109])
torch.Size([1])
torch.Size([109])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1287)
idx:  108
Gate loss:  tensor(3.9241, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2913, device='cuda:1')
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
torch.Size([337])
torch.Size([1])
torch.Size([337])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.6160)
idx:  336
Gate loss:  tensor(7.5401, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.1328)
idx:  12
Gate loss:  tensor(7.6728, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
torch.Size([197])
torch.Size([1])
torch.Size([197])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.2445)
idx:  196
Gate loss:  tensor(7.9173, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
torch.Size([293])
torch.Size([1])
torch.Size([293])
Inside custom generate sample func
reasoning_path shape:  torch.Size([358])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6911)
idx:  292
Gate loss:  tensor(8.6084, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
Preparing 4D causal attention mask with cache position
torch.Size([2284])
tensor(1.2547, device='cuda:1', grad_fn=<MeanBackward0>)
The shape of new_sequence: torch.Size([4, 572])
gate shape:  torch.Size([4, 572])
The shape of hidden_states: torch.Size([4096])
The topk tensor(1338, device='cuda:1')
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
torch.Size([215])
torch.Size([1])
torch.Size([215])
Inside custom generate sample func
reasoning_path shape:  torch.Size([583])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6658)
idx:  214
Gate loss:  tensor(0.6658, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7936, device='cuda:1')
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
torch.Size([231])
torch.Size([1])
torch.Size([231])
Inside custom generate sample func
reasoning_path shape:  torch.Size([583])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7759)
idx:  230
Gate loss:  tensor(1.4417, device='cuda:0', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7936, device='cuda:1')
torch.Size([460])
torch.Size([1])
torch.Size([460])
Inside custom generate sample func
torch.Size([460])
torch.Size([1])
torch.Size([460])
Inside custom generate sample func
torch.Size([460])
torch.Size([1])
torch.Size([460])
Inside custom generate sample func
reasoning_path shape:  torch.Size([583])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7563)
idx:  459
Gate loss:  tensor(3.1980, device='cuda:0', grad_fn=<AddBackward0>)
The count is: 3
The shape of hidden_states: torch.Size([4096])
The topk tensor(18177, device='cuda:1')
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
torch.Size([118])
torch.Size([1])
torch.Size([118])
Inside custom generate sample func
reasoning_path shape:  torch.Size([583])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 847, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 829, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 389, in train
    outputs = model(**batch, tokenizer=tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/modeling_llama.py", line 1407, in forward
    packed_reasoning_path, _, packed_reasoning_path_casual_mask, packed = get_packed_inputs(reasoning_path, max_length=4090, pad_token_id=self.config.eos_token_id, thought_index=thought_index)
                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/pack_input_ids.py", line 127, in get_packed_inputs
    casual_mask = build_batched_causal_mask_from_attention_mask(attention_mask, torch.float).to(input_ids[0].device)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/pack_input_ids.py", line 107, in build_batched_causal_mask_from_attention_mask
    masks = zero_positions & masks_per_j  # Shape: [bz, seq_len, seq_len, seq_len]
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.01 GiB. GPU 0 has a total capacity of 93.12 GiB of which 415.12 MiB is free. Including non-PyTorch memory, this process has 92.70 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 15.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
