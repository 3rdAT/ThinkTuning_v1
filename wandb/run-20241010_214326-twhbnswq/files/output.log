Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.01s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.91k/6.91k [00:00<00:00, 33.4MB/s]
train-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84.2M/84.2M [00:01<00:00, 65.9MB/s]
Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200035/200035 [00:01<00:00, 191894.88 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:00<00:00, 2147.41 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2118.87 examples/s]
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                              [0m| 0/225 [00:00<?, ?it/s][0m
torch.Size([792])
tensor(1.1759, device='cuda:1', grad_fn=<MeanBackward0>)
The gate values are: tensor([[0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.1192, 0.8890, 0.9574, 0.9738,
         0.9896, 0.9799, 0.9969, 0.9953, 0.9194, 0.9904, 0.9673, 0.8832, 0.9117,
         0.9892, 0.9913, 0.9871, 0.9381, 0.9948, 0.6024, 0.0288, 0.8019, 0.9513,
         0.9932, 0.9863, 0.9697, 0.5349, 0.9968, 0.9948, 0.5412, 0.9877, 0.9957,
         0.9741, 0.9917, 0.9929, 0.8627, 0.7367, 0.1805, 0.3665, 0.9398, 0.9863,
         0.9978, 0.9919, 0.9661, 0.9522, 0.5349, 0.9675, 0.9728, 0.6765, 0.9458,
         0.9832, 0.9713, 0.9890, 0.9871, 0.9847, 0.9864, 0.7662, 0.4556, 0.7628,
         0.3742, 0.2270, 0.9111, 0.8392, 0.4019, 0.2894, 0.9504, 0.8683, 0.9644,
         0.9019, 0.9773, 0.9770, 0.9670, 0.8836, 0.6253, 0.8863, 0.1568, 0.9868,
         0.7292, 0.9818, 0.9917, 0.9440, 0.9835, 0.9875, 0.9698, 0.5374, 0.7097,
         0.9799, 0.7985, 0.8801, 0.9979, 0.8521, 0.9887, 0.9712, 0.9897, 0.9946,
         0.8340, 0.3167, 0.3580, 0.4815, 0.8417, 0.9500, 0.6930, 0.9666, 0.9441,
         0.9132, 0.2363, 0.9665, 0.5471, 0.1069, 0.3696, 0.9712, 0.3092, 0.7539,
         0.9804, 0.9404, 0.8453, 0.5182, 0.9955, 0.9973, 0.9822, 0.9937, 0.9882,
         0.9901, 0.9788, 0.9975, 0.9293, 0.9929, 0.6212, 0.8991, 0.9910, 0.9301,
         0.9654, 0.9980, 0.9981, 0.9242, 0.8625, 0.3383, 0.8984, 0.6663, 0.8397,
         0.8786, 0.9690, 0.7319, 0.3058, 0.8944, 0.7982, 0.9946, 0.8603, 0.9911,
         0.9717, 0.9056, 0.9778, 0.9911, 0.3536, 0.9823, 0.9986, 0.9696, 0.9324,
         0.9986, 0.9821, 0.9898, 0.9925, 0.9696, 0.9528, 0.9834, 0.7163, 0.9906,
         0.6626, 0.9582, 0.9896, 0.9564, 0.9191, 0.9463, 0.8984, 0.9552, 0.8703,
         0.9803, 0.9849, 0.9928, 0.9448, 0.9872, 0.9724, 0.9343, 0.5416, 0.9606,
         0.9914, 0.9156, 0.9408, 0.9394, 0.9511, 0.9691, 0.9420, 0.9727, 0.8808,
         0.5522],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.7204, 0.9603, 0.9738, 0.7526,
         0.4844, 0.9908, 0.9850, 0.9914, 0.9851, 0.9982, 0.9678, 0.9675, 0.9964,
         0.9934, 0.9909, 0.8941, 0.8943, 0.9763, 0.9811, 0.9898, 0.9686, 0.6422,
         0.4755, 0.6819, 0.4337, 0.2121, 0.9223, 0.7649, 0.5878, 0.7310, 0.9445,
         0.9867, 0.9789, 0.8516, 0.9782, 0.1202, 0.9840, 0.9728, 0.9405, 0.9733,
         0.9889, 0.9617, 0.9659, 0.5156, 0.9836, 0.9963, 0.9980, 0.9616, 0.9851,
         0.9782, 0.9851, 0.9367, 0.9928, 0.9803, 0.6424, 0.1703, 0.6972, 0.4594,
         0.4062, 0.9906, 0.9943, 0.8999, 0.9813, 0.9325, 0.9116, 0.9825, 0.9729,
         0.9798, 0.9869, 0.8855, 0.8663, 0.8009, 0.9838, 0.9729, 0.9094, 0.8923,
         0.7046, 0.9713, 0.9835, 0.9845, 0.7935, 0.9387, 0.7641, 0.9881, 0.0963,
         0.1881, 0.2383, 0.4989, 0.7224, 0.9737, 0.8854, 0.9929, 0.9852, 0.6556,
         0.9584, 0.6004, 0.9513, 0.9606, 0.9263, 0.8607, 0.9728, 0.2124, 0.9784,
         0.9676, 0.7785, 0.9582, 0.9820, 0.8532, 0.5484, 0.9677, 0.9230, 0.9974,
         0.9283, 0.8761, 0.9700, 0.6593, 0.8990, 0.8130, 0.5320, 0.9785, 0.9044,
         0.9675, 0.9863, 0.9484, 0.8835, 0.5197, 0.8997, 0.4715, 0.9740, 0.9225,
         0.5472, 0.7747, 0.9329, 0.8929, 0.8981, 0.7258, 0.9842, 0.9172, 0.5174,
         0.9107, 0.9602, 0.8347, 0.9522, 0.7340, 0.9481, 0.9771, 0.6772, 0.9643,
         0.9663, 0.9345, 0.9322, 0.8207, 0.9483, 0.8971, 0.9291, 0.5415, 0.9786,
         0.8855, 0.6588, 0.9514, 0.9050, 0.7070, 0.9456, 0.9667, 0.7880, 0.9768,
         0.8917, 0.9414, 0.6099, 0.9881, 0.5648, 0.3715, 0.9834, 0.8489, 0.1218,
         0.8481, 0.9109, 0.9951, 0.9963, 0.8159, 0.9549, 0.8792, 0.4102, 0.8729,
         0.9446, 0.9944, 0.9931, 0.9902, 0.9409, 0.9840, 0.5339, 0.9910, 0.8761,
         0.5896],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.4665, 0.8850, 0.6958, 0.8065,
         0.7836, 0.9933, 0.9791, 0.9685, 0.8147, 0.9810, 0.8019, 0.2298, 0.9721,
         0.9891, 0.9549, 0.7944, 0.9628, 0.9489, 0.5233, 0.9494, 0.7639, 0.9690,
         0.9378, 0.7669, 0.5273, 0.3673, 0.1394, 0.9040, 0.8569, 0.4304, 0.7995,
         0.5169, 0.7957, 0.9061, 0.6560, 0.8930, 0.9524, 0.9688, 0.9589, 0.9246,
         0.9638, 0.3510, 0.1225, 0.1069, 0.9709, 0.8562, 0.7470, 0.4084, 0.7767,
         0.7865, 0.7849, 0.9665, 0.9523, 0.5092, 0.9958, 0.9646, 0.9671, 0.9803,
         0.9788, 0.9829, 0.7538, 0.9934, 0.9390, 0.7128, 0.9374, 0.9241, 0.9810,
         0.9554, 0.9002, 0.8654, 0.9818, 0.5137, 0.8552, 0.9153, 0.9450, 0.6626,
         0.8246, 0.1431, 0.9330, 0.8878, 0.8343, 0.5851, 0.8045, 0.8824, 0.2992,
         0.9560, 0.9229, 0.3216, 0.9696, 0.8667, 0.8701, 0.8307, 0.8357, 0.9677,
         0.6598, 0.3752, 0.8812, 0.9164, 0.9518, 0.9900, 0.9227, 0.9913, 0.9836,
         0.9969, 0.9670, 0.9236, 0.9291, 0.3168, 0.8924, 0.7205, 0.8203, 0.9723,
         0.8697, 0.8275, 0.9762, 0.9216, 0.7431, 0.9989, 0.9607, 0.7109, 0.9682,
         0.7607, 0.9937, 0.9718, 0.9971, 0.9748, 0.9501, 0.9591, 0.7811, 0.9854,
         0.9678, 0.4222, 0.9876, 0.9975, 0.9838, 0.9746, 0.9021, 0.9962, 0.9756,
         0.8152, 0.9845, 0.9072, 0.9875, 0.9910, 0.7436, 0.9989, 0.9964, 0.9810,
         0.9484, 0.9971, 0.8756, 0.6636, 0.9289, 0.9354, 0.9647, 0.4597, 0.9982,
         0.8228, 0.8107, 0.9956, 0.9447, 0.9954, 0.9704, 0.9978, 0.9781, 0.9750,
         0.9724, 0.9974, 0.9923, 0.8580, 0.7814, 0.9984, 0.9791, 0.7153, 0.9964,
         0.9658, 0.9934, 0.9192, 0.7612, 0.9988, 0.8934, 0.1095, 0.8828, 0.9588,
         0.9782, 0.9797, 0.9869, 0.9464, 0.9940, 0.9783, 0.8464, 0.9942, 0.9314,
         0.6527],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.5056, 0.7281, 0.0112, 0.0217,
         0.0539, 0.7288, 0.3317, 0.4219, 0.9919, 0.9830, 0.9997, 0.9943, 0.9516,
         0.2746, 0.8223, 0.6607, 0.3126, 0.9193, 0.6701, 0.3250, 0.9662, 0.7112,
         0.5034, 0.7373, 0.9516, 0.9811, 0.9866, 0.7945, 0.9699, 0.8961, 0.7983,
         0.5512, 0.9798, 0.6797, 0.9486, 0.8290, 0.9412, 0.5188, 0.5261, 0.8624,
         0.8656, 0.8603, 0.8594, 0.8720, 0.8230, 0.8249, 0.9601, 0.9685, 0.9252,
         0.7202, 0.1409, 0.9957, 0.9708, 0.8435, 0.6365, 0.4624, 0.1769, 0.1323,
         0.9060, 0.7493, 0.3695, 0.3895, 0.7238, 0.4059, 0.2581, 0.8317, 0.4165,
         0.8413, 0.3469, 0.4160, 0.9003, 0.9155, 0.7846, 0.9097, 0.9075, 0.8551,
         0.5316, 0.9350, 0.9190, 0.7330, 0.1630, 0.9148, 0.9925, 0.9828, 0.7572,
         0.9548, 0.8956, 0.3300, 0.4074, 0.5472, 0.9650, 0.4573, 0.7749, 0.9847,
         0.9803, 0.5062, 0.2056, 0.1850, 0.5657, 0.0956, 0.9070, 0.9351, 0.9978,
         0.9908, 0.6908, 0.8687, 0.9570, 0.3441, 0.4658, 0.4340, 0.6525, 0.9157,
         0.9513, 0.9960, 0.9840, 0.6949, 0.9882, 0.9723, 0.9488, 0.6558, 0.3580,
         0.3159, 0.9387, 0.9225, 0.9830, 0.9814, 0.8676, 0.9779, 0.9729, 0.8770,
         0.2056, 0.0813, 0.8493, 0.9402, 0.7667, 0.8578, 0.7796, 0.6729, 0.9605,
         0.9844, 0.7401, 0.7008, 0.7239, 0.4257, 0.9673, 0.2000, 0.9323, 0.9846,
         0.9908, 0.8691, 0.6642, 0.9862, 0.0917, 0.9082, 0.9838, 0.9865, 0.9757,
         0.8587, 0.9840, 0.1677, 0.3128, 0.5495, 0.9606, 0.9556, 0.8661, 0.6286,
         0.9933, 0.9371, 0.9677, 0.7264, 0.1083, 0.8010, 0.2367, 0.0981, 0.7666,
         0.9950, 0.9722, 0.8257, 0.8096, 0.6081, 0.4231, 0.9123, 0.5501, 0.2097,
         0.8578, 0.7609, 0.6926, 0.9084, 0.9295, 0.9158, 0.6387, 0.9619, 0.7808,
         0.5790]], device='cuda:1', grad_fn=<SqueezeBackward1>)
The shape is: torch.Size([4, 199])
The shape of new_sequence: torch.Size([4, 199])
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([7])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Traceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 848, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 830, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 389, in train
    outputs = model(**batch)
              ^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/modeling_llama.py", line 1375, in forward
    sampled_text_generate = tokenizer.decode(new_greedy_sequence_decoding[0])
                            ^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'decode'
