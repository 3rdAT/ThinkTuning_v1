Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.44s/it]
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
The gate was not present, so initializing it!
/data/data/arrv/env/test/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                        [0m| 0/225 [00:00<?, ?it/s][0m/data/data/arrv/ThinkTuning_v1/src/think_tuner.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
The indices are: tensor([  0,   4,   7,   8,  11,  12,  18,  19,  26,  33,  50,  51,  54,  59,
         69,  70,  73,  84,  96, 101, 107, 108, 120, 159, 173, 174, 179, 186,
        189, 199, 200], device='cuda:3')
The indices are: tensor([  0,   4,   5,   6,   7,  18,  25,  26,  27,  30,  31,  56,  71,  82,
         84,  91,  92, 112, 125, 139, 161, 172, 177, 182, 199, 200],
       device='cuda:3')
The indices are: tensor([  0,   4,   5,   6,   7,   8,   9,  10,  12,  13,  14,  18,  22,  24,
         27,  35,  39,  40,  46,  47,  50,  51,  58,  61,  66,  68,  81,  95,
         96, 103, 111, 131, 132, 140, 170, 175, 188, 195, 196, 199, 200],
       device='cuda:3')
The indices are: tensor([  0,   4,  20,  48,  49,  52,  56,  75,  76,  77,  79, 162, 200],
       device='cuda:3')
The selected-indices are: [[33, 96, 189], [91, 6, 182], [12, 95, 10], [75, 0, 162]]
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
torch.Size([1, 2130])
Training Epoch: 1/1, step 0/225 completed (loss: 1.1209124326705933):   0%|[34mâ–Ž                                                          [0m| 1/225 [01:29<5:35:50, 89.96s/it][0m
The indices are: tensor([  0,   1,   2,   4,  20,  26,  34,  49,  50,  51,  55,  56,  60,  62,
         63,  65,  78,  81,  91,  92,  97, 100, 101, 102, 103, 104, 105, 107,
        108, 110, 114, 116, 118, 119, 122, 123, 125, 126, 134, 135, 139, 141,
        144, 148, 150, 152, 153, 156, 157, 162, 184, 187, 190, 191, 192, 196,
        202, 208, 214, 220, 224, 225, 237, 241, 246, 247], device='cuda:3')
The indices are: tensor([  0,   1,   2,   4,   6,   9,  12,  15,  16,  17,  21,  36,  42,  47,
         48,  50,  51,  57,  58,  59,  69,  70,  75,  76,  86,  88,  89,  95,
         98, 102, 103, 109, 110, 117, 120, 132, 133, 134, 138, 154, 155, 156,
        165, 177, 178, 187, 196, 197, 203, 204, 221, 225, 226, 228, 237, 238,
        242, 243, 247], device='cuda:3')
The indices are: tensor([  0,   1,   2,   4,   5,   6,  10,  12,  15,  16,  19,  20,  32,  35,
         46,  53,  54,  58,  64,  68,  73,  75,  77,  78,  79,  86,  89,  90,
         91,  94,  96,  97,  98,  99, 101, 105, 106, 122, 125, 126, 127, 129,
        130, 131, 133, 134, 139, 140, 141, 144, 146, 147, 148, 149, 151, 155,
        156, 172, 175, 176, 177, 179, 180, 181, 182, 183, 184, 189, 190, 191,
        192, 193, 197, 204, 205, 209, 210, 212, 213, 214, 220, 225, 226, 227,
        231, 232, 234, 235, 236, 237, 240, 241, 247], device='cuda:3')
The indices are: tensor([  0,   1,   2,   4,  11,  16,  20,  21,  22,  24,  26,  27,  29,  31,
         32,  35,  36,  37,  38,  48,  49,  50,  52,  54,  63,  67,  69,  70,
         71,  72,  73,  74,  75,  76,  78,  79,  81,  83,  86,  91,  92,  95,
         96,  97,  98, 101, 107, 112, 113, 119, 125, 127, 128, 130, 131, 132,
        133, 134, 138, 140, 142, 144, 147, 155, 162, 167, 169, 181, 187, 188,
        195, 204, 207, 208, 214, 219, 220, 221, 222, 223, 227, 229, 231, 232,
        236, 241, 244, 245, 247], device='cuda:3')
The selected-indices are: [[78, 190, 91], [36, 156, 95], [0, 10, 182], [1, 71, 142]]
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2600])
torch.Size([1, 2596])
torch.Size([1, 2600])
The indices are: tensor([  0,   1,   2,   3,   4,   6,   7,   8,   9,  10,  11,  12,  14,  15,
         17,  18,  26,  68,  69,  71,  76,  77,  81,  89,  90, 131, 142, 158,
        159, 161, 187, 188, 189, 201, 223, 237, 316, 352, 380, 424, 428, 429,
        431, 469, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484,
        485], device='cuda:3')
The indices are: tensor([  0,   1,   2,   3,   4,   7,   8,   9,  10,  11,  12,  24,  25,  31,
         32,  34,  40,  48,  56,  62,  63,  64,  66,  75,  87,  91,  95,  96,
         98, 102, 103, 104, 107, 109, 112, 117, 122, 130, 138, 145, 153, 154,
        157, 160, 161, 162, 168, 197, 200, 204, 210, 216, 237, 245, 248, 252,
        256, 262, 269, 297, 304, 316, 317, 318, 324, 327, 334, 362, 378, 387,
        399, 403, 420, 456, 466, 478, 479, 482, 483, 484, 485],
       device='cuda:3')
The indices are: tensor([  0,   1,   2,   3,   4,   5,   6,   9,  11,  27,  34,  56,  57,  61,
         64,  78,  90,  92, 107, 113, 132, 134, 136, 138, 150, 174, 202, 203,
        219, 223, 227, 231, 235, 238, 242, 246, 284, 287, 296, 306, 310, 313,
        323, 327, 330, 339, 340, 353, 384, 385, 396, 397, 398, 406, 410, 412,
        420, 422, 430, 434, 438, 446, 460, 483, 484, 485], device='cuda:3')
The indices are: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   9,  10,  11,  12,  13,  14,
         15,  16,  17,  18,  23,  24,  26,  28,  32,  34,  35,  36,  43,  44,
         46,  49,  57,  58,  59,  61,  64,  65,  67,  68,  70,  73,  78,  82,
         83,  84,  85,  86,  87,  89,  92, 100, 101, 102, 105, 106, 109, 117,
        137, 138, 149, 150, 152, 157, 158, 159, 162, 164, 171, 174, 176, 178,
        180, 181, 183, 187, 190, 193, 202, 208, 210, 218, 219, 220, 241, 251,
        285, 287, 292, 294, 297, 305, 313, 317, 326, 337, 344, 346, 347, 373,
        379, 380, 400, 403, 404, 418, 421, 460, 472, 473, 474, 485],
       device='cuda:3')
The selected-indices are: [[7, 69, 189], [154, 317, 34], [34, 434, 385], [67, 106, 208]]
torch.Size([2, 3984])
torch.Size([2, 3984])
torch.Size([2, 3984])
torch.Size([2, 3984])
torch.Size([2, 3984])
torch.Size([2, 3984])
torch.Size([2, 3984])
torch.Size([2, 3982])
torch.Size([2, 3984])
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 863, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 845, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 393, in train
    outputs = think_tuner_step(batch, model=model, tokenizer=tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 304, in think_tuner_step
    total_gate_loss, total_reinforce_loss, total_nll_thought, logy = start_thinking(batch["input_ids"], outputs.last_hidden_state, logits,  batch["labels"], unreduced_loss, model, tokenizer)
                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/src/think_tuner.py", line 228, in start_thinking
    new_greedy_sequence_decoding = model.generate(**batch, max_new_tokens=10, do_sample=True, temperature=1.0 ,use_cache= True, top_p=1.0)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/generation/utils.py", line 2024, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/generation/utils.py", line 2982, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 617, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/test/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
