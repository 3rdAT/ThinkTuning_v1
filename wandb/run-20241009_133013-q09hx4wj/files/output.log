Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.01s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                              [0m| 0/225 [00:00<?, ?it/s][0m
Preparing 4D causal attention mask with cache position
torch.Size([192])
tensor(1.6885, device='cuda:3', grad_fn=<MeanBackward0>)
The gate values are: tensor([[[0.1958, 0.9759],
         [0.2726, 0.9757],
         [0.1058, 0.8276],
         [0.3714, 0.9822],
         [0.3916, 0.9947],
         [0.6985, 0.0966],
         [0.9251, 0.2602],
         [0.9615, 0.0502],
         [0.9517, 0.1491],
         [0.9841, 0.7665],
         [0.7660, 0.5562],
         [0.4729, 0.9094],
         [0.3850, 0.2569],
         [0.9925, 0.0644],
         [0.9536, 0.1066],
         [0.9900, 0.4744],
         [0.8543, 0.3646],
         [0.8289, 0.9820],
         [0.5641, 0.9837],
         [0.2382, 0.9857],
         [0.9674, 0.0145],
         [0.9377, 0.0881],
         [0.7065, 0.9955],
         [0.9483, 0.9644],
         [0.7368, 0.1857],
         [0.5168, 0.5769],
         [0.3469, 0.1516],
         [0.9861, 0.0285],
         [0.9956, 0.0669],
         [0.8238, 0.0512],
         [0.9735, 0.1727],
         [0.9155, 0.1233],
         [0.9260, 0.0214],
         [0.9696, 0.1682],
         [0.9629, 0.8870],
         [0.9086, 0.8029],
         [0.9589, 0.5792],
         [0.9080, 0.1360],
         [0.4470, 0.2566],
         [0.4610, 0.2601],
         [0.4536, 0.2791],
         [0.4319, 0.3031],
         [0.4061, 0.3203],
         [0.3838, 0.3273],
         [0.3756, 0.3265],
         [0.3874, 0.3209],
         [0.4055, 0.3172],
         [0.4121, 0.3196],
         [0.4014, 0.3254]],

        [[0.1958, 0.9759],
         [0.2726, 0.9757],
         [0.1058, 0.8276],
         [0.3714, 0.9822],
         [0.3916, 0.9947],
         [0.1290, 0.9169],
         [0.5059, 0.6279],
         [0.3973, 0.8929],
         [0.6223, 0.1744],
         [0.9666, 0.0491],
         [0.7756, 0.1908],
         [0.9926, 0.3036],
         [0.9097, 0.7462],
         [0.9519, 0.9895],
         [0.6860, 0.8650],
         [0.3032, 0.7655],
         [0.2958, 0.5822],
         [0.9733, 0.4046],
         [0.9904, 0.6422],
         [0.9516, 0.2111],
         [0.8758, 0.9914],
         [0.6906, 0.9855],
         [0.2810, 0.9863],
         [0.9729, 0.0174],
         [0.9376, 0.2328],
         [0.7789, 0.9952],
         [0.9159, 0.9825],
         [0.6466, 0.4100],
         [0.4747, 0.5204],
         [0.2777, 0.2910],
         [0.9763, 0.2897],
         [0.9973, 0.0362],
         [0.8718, 0.3785],
         [0.8729, 0.0765],
         [0.9910, 0.1612],
         [0.6196, 0.9158],
         [0.9774, 0.0154],
         [0.9788, 0.0982],
         [0.8785, 0.2011],
         [0.4930, 0.1871],
         [0.4955, 0.2049],
         [0.4828, 0.2291],
         [0.4709, 0.2437],
         [0.4622, 0.2450],
         [0.4535, 0.2373],
         [0.4492, 0.2311],
         [0.4498, 0.2362],
         [0.4491, 0.2516],
         [0.4458, 0.2665]],

        [[0.1958, 0.9759],
         [0.2726, 0.9757],
         [0.1058, 0.8276],
         [0.3714, 0.9822],
         [0.3916, 0.9947],
         [0.7852, 0.2340],
         [0.2661, 0.6920],
         [0.8681, 0.0084],
         [0.7183, 0.9305],
         [0.6661, 0.1583],
         [0.9670, 0.8704],
         [0.6026, 0.1006],
         [0.9896, 0.1099],
         [0.9620, 0.0268],
         [0.4547, 0.6767],
         [0.9928, 0.0800],
         [0.9950, 0.0166],
         [0.9909, 0.0146],
         [0.8967, 0.8419],
         [0.9982, 0.5479],
         [0.9854, 0.9664],
         [0.9079, 0.9609],
         [0.9931, 0.5149],
         [0.9574, 0.6792],
         [0.8864, 0.1219],
         [0.6854, 0.9853],
         [0.6002, 0.9820],
         [0.3119, 0.9856],
         [0.9330, 0.0270],
         [0.9082, 0.1125],
         [0.5419, 0.9880],
         [0.8723, 0.9864],
         [0.9880, 0.1561],
         [0.9909, 0.0506],
         [0.9959, 0.0196],
         [0.9156, 0.1522],
         [0.9751, 0.0078],
         [0.9387, 0.0444],
         [0.6680, 0.3332],
         [0.9882, 0.0262],
         [0.9933, 0.0608],
         [0.9906, 0.0423],
         [0.9289, 0.6129],
         [0.9897, 0.0083],
         [0.9959, 0.1008],
         [0.8361, 0.7293],
         [0.9943, 0.0435],
         [0.9539, 0.1238],
         [0.4334, 0.1797]],

        [[0.1958, 0.9759],
         [0.2726, 0.9757],
         [0.1058, 0.8276],
         [0.3714, 0.9822],
         [0.3916, 0.9947],
         [0.1290, 0.9169],
         [0.5059, 0.6279],
         [0.3973, 0.8929],
         [0.6720, 0.9879],
         [0.9726, 0.0476],
         [0.9858, 0.0820],
         [0.9202, 0.9226],
         [0.8202, 0.9733],
         [0.9430, 0.3955],
         [0.7828, 0.6494],
         [0.9786, 0.1505],
         [0.9531, 0.0395],
         [0.4317, 0.7484],
         [0.9930, 0.1584],
         [0.9931, 0.0666],
         [0.9734, 0.1017],
         [0.8581, 0.8882],
         [0.9977, 0.2609],
         [0.9511, 0.5234],
         [0.8739, 0.1149],
         [0.6302, 0.9725],
         [0.5256, 0.9722],
         [0.2970, 0.9822],
         [0.9441, 0.0249],
         [0.8700, 0.0874],
         [0.3823, 0.9889],
         [0.7770, 0.9558],
         [0.9798, 0.0867],
         [0.9856, 0.0375],
         [0.9973, 0.0105],
         [0.9109, 0.0658],
         [0.9670, 0.0066],
         [0.9218, 0.0334],
         [0.5511, 0.2653],
         [0.9859, 0.0181],
         [0.9922, 0.0489],
         [0.9889, 0.0482],
         [0.9133, 0.5664],
         [0.9839, 0.0088],
         [0.9898, 0.0758],
         [0.4657, 0.5075],
         [0.9931, 0.0451],
         [0.9594, 0.1210],
         [0.4509, 0.1906]]], device='cuda:3', grad_fn=<SigmoidBackward0>)
The shape is: torch.Size([4, 49, 2])
The shape of new_sequence: torch.Size([4, 49])
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 840, in <module>
    fire.Fire(main)
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 822, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 415, in train
    model_loss.backward()
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/data/data/arrv/envs/tv1/lib/python3.12/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
