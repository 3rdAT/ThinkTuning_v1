Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.85s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['gate.0.bias', 'gate.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
<class 'datasets.arrow_dataset.Dataset'>
--> Training Set Length = 900
--> Validation Set Length = 100
/data/data/arrv/env/tv/lib/python3.12/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                                                                                              [0m| 0/225 [00:00<?, ?it/s][0m
Preparing 4D causal attention mask with cache position
torch.Size([352])
tensor(1.8703, device='cuda:1', grad_fn=<MeanBackward0>)
The gate values are: tensor([[0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.1154, 0.1682, 0.1875, 0.9954,
         0.9958, 0.8137, 0.0236, 0.9734, 0.2648, 0.1291, 0.9729, 0.9972, 0.9928,
         0.9936, 0.7042, 0.2412, 0.2145, 0.9418, 0.6780, 0.9786, 0.2741, 0.0114,
         0.0043, 0.9515, 0.6949, 0.2168, 0.8970, 0.9932, 0.5830, 0.2128, 0.9686,
         0.5014, 0.5385, 0.5721, 0.9938, 0.9897, 0.8792, 0.4732, 0.2563, 0.9631,
         0.8022, 0.9780, 0.5083, 0.7170, 0.9190, 0.9788, 0.9763, 0.8618, 0.7180,
         0.6292, 0.5102, 0.1709, 0.9178, 0.8643, 0.5030, 0.1022, 0.0231, 0.9477,
         0.8819, 0.9952, 0.9937, 0.8975, 0.9885, 0.9758, 0.9479, 0.6781, 0.9920,
         0.9595, 0.9950, 0.9872, 0.9323, 0.0377, 0.7811, 0.9944, 0.9618, 0.9733,
         0.9347, 0.8116, 0.9900, 0.9777, 0.9853, 0.7167, 0.5579, 0.5510],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.3352, 0.8998, 0.4975, 0.0730,
         0.4716, 0.9942, 0.9984, 0.9003, 0.9659, 0.8598, 0.9792, 0.9664, 0.5698,
         0.5614, 0.9769, 0.6745, 0.9882, 0.9971, 0.9837, 0.9805, 0.9791, 0.4425,
         0.7618, 0.9979, 0.9984, 0.9995, 0.9367, 0.9138, 0.7391, 0.9782, 0.9942,
         0.9924, 0.5804, 0.6210, 0.9829, 0.8264, 0.9616, 0.9090, 0.9863, 0.9855,
         0.9764, 0.7032, 0.9966, 0.9610, 0.8870, 0.6691, 0.7138, 0.3697, 0.1832,
         0.9104, 0.6599, 0.3971, 0.0704, 0.2677, 0.9834, 0.9954, 0.9946, 0.3338,
         0.9962, 0.9807, 0.9262, 0.7344, 0.9817, 0.8894, 0.9836, 0.9988, 0.4114,
         0.1704, 0.9937, 0.9966, 0.9881, 0.9901, 0.9304, 0.9883, 0.7941, 0.9279,
         0.9822, 0.9748, 0.2146, 0.1403, 0.9521, 0.7269, 0.5778, 0.5745],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.9310, 0.9204, 0.9252, 0.4968,
         0.5585, 0.8484, 0.4480, 0.6943, 0.8874, 0.9899, 0.7064, 0.8871, 0.7481,
         0.9908, 0.9840, 0.5802, 0.9664, 0.9965, 0.9892, 0.8023, 0.7729, 0.9910,
         0.9919, 0.5872, 0.6053, 0.9854, 0.9952, 0.9961, 0.7200, 0.7182, 0.9906,
         0.9653, 0.9830, 0.7369, 0.9873, 0.9716, 0.9873, 0.9982, 0.9903, 0.9485,
         0.8610, 0.9816, 0.9813, 0.7640, 0.9035, 0.9982, 0.9698, 0.8813, 0.8199,
         0.2857, 0.1663, 0.9415, 0.9189, 0.7062, 0.9971, 0.9998, 0.9970, 0.9951,
         0.9678, 0.7134, 0.9755, 0.9741, 0.8953, 0.9841, 0.9943, 0.9858, 0.9833,
         0.9854, 0.9908, 0.9868, 0.9736, 0.9904, 0.9727, 0.9943, 0.8727, 0.8158,
         0.9972, 0.9832, 0.8146, 0.8246, 0.9847, 0.9261, 0.5613, 0.5530],
        [0.1958, 0.2726, 0.1058, 0.3714, 0.3916, 0.2740, 0.4229, 0.9678, 0.8465,
         0.8900, 0.5988, 0.8569, 0.7605, 0.5066, 0.9833, 0.4889, 0.7000, 0.6732,
         0.9074, 0.7392, 0.3700, 0.9000, 0.8888, 0.8857, 0.4924, 0.6234, 0.2475,
         0.9660, 0.9272, 0.7498, 0.9129, 0.9482, 0.9947, 0.7400, 0.9541, 0.2770,
         0.3959, 0.9807, 0.4157, 0.9568, 0.9793, 0.9931, 0.8377, 0.9961, 0.9857,
         0.8856, 0.9793, 0.9608, 0.8489, 0.6964, 0.9842, 0.9732, 0.9518, 0.9796,
         0.9869, 0.9928, 0.9863, 0.9268, 0.4393, 0.3785, 0.9926, 0.4755, 0.9757,
         0.9350, 0.6833, 0.5312, 0.9930, 0.9324, 0.9736, 0.9888, 0.9832, 0.9540,
         0.8124, 0.9904, 0.9740, 0.7915, 0.9868, 0.9421, 0.8868, 0.4745, 0.2367,
         0.9339, 0.6312, 0.9589, 0.9923, 0.9724, 0.9964, 0.9471, 0.5190]],
       device='cuda:1', grad_fn=<SqueezeBackward1>)
The shape is: torch.Size([4, 89])
The shape of new_sequence: torch.Size([4, 89])
The shape of hidden_states: torch.Size([4096])
The topk tensor(964, device='cuda:1')
torch.Size([9])
  sampled_token = torch.tensor(topk_indices.indices[i].unsqueeze(0)).to(device=new_sequence.device)  # Add the sampled token
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0658)
idx:  8
Gate loss:  tensor(1.0609, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0731)
idx:  9
Gate loss:  tensor(2.1294, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(380, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3226)
idx:  10
Gate loss:  tensor(3.2056, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1023)
idx:  12
Gate loss:  tensor(4.2785, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(30010, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1437)
idx:  15
Gate loss:  tensor(5.3913, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3313)
idx:  16
Gate loss:  tensor(6.7188, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(964, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0270)
idx:  17
Gate loss:  tensor(7.7384, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(472, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9696)
idx:  18
Gate loss:  tensor(8.7017, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0711)
idx:  19
Gate loss:  tensor(9.4560, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(386, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5577)
idx:  22
Gate loss:  tensor(10.9230, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11904, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2602)
idx:  23
Gate loss:  tensor(11.7774, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3752)
idx:  24
Gate loss:  tensor(13.1232, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(471, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3279)
idx:  28
Gate loss:  tensor(14.3868, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2881)
idx:  29
Gate loss:  tensor(15.2818, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1061, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6654)
idx:  31
Gate loss:  tensor(16.7757, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2085)
idx:  32
Gate loss:  tensor(17.9760, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5072)
idx:  33
Gate loss:  tensor(18.8548, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7087)
idx:  35
Gate loss:  tensor(20.5098, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5848)
idx:  36
Gate loss:  tensor(21.3044, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5214, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5272)
idx:  37
Gate loss:  tensor(22.1268, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(5214, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8433)
idx:  38
Gate loss:  tensor(23.1813, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6203)
idx:  39
Gate loss:  tensor(24.7916, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11977, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3949)
idx:  40
Gate loss:  tensor(26.1721, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7038)
idx:  41
Gate loss:  tensor(27.6701, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(386, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.3817)
idx:  44
Gate loss:  tensor(29.9640, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11904, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9563)
idx:  45
Gate loss:  tensor(31.5333, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7995)
idx:  46
Gate loss:  tensor(33.2932, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9816)
idx:  47
Gate loss:  tensor(34.3004, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9970)
idx:  48
Gate loss:  tensor(35.7322, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2580, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8863)
idx:  49
Gate loss:  tensor(38.3846, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2757)
idx:  50
Gate loss:  tensor(40.6120, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9375)
idx:  51
Gate loss:  tensor(42.5035, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5030)
idx:  52
Gate loss:  tensor(43.7988, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3959)
idx:  53
Gate loss:  tensor(44.8011, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4722)
idx:  54
Gate loss:  tensor(46.3566, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0540)
idx:  55
Gate loss:  tensor(47.4046, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4272)
idx:  57
Gate loss:  tensor(50.5500, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2861)
idx:  58
Gate loss:  tensor(52.5258, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2008, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0524)
idx:  59
Gate loss:  tensor(53.5581, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(10784, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5567)
idx:  62
Gate loss:  tensor(55.9811, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2580, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5513)
idx:  63
Gate loss:  tensor(59.1129, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6133, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.6293)
idx:  64
Gate loss:  tensor(63.7199, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9729)
idx:  65
Gate loss:  tensor(66.6741, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(540, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7769)
idx:  66
Gate loss:  tensor(70.0640, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3614, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1833)
idx:  67
Gate loss:  tensor(73.2108, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.6065)
idx:  68
Gate loss:  tensor(76.7301, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.0362)
idx:  69
Gate loss:  tensor(81.5037, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3634)
idx:  70
Gate loss:  tensor(84.4627, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(386, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.0938)
idx:  71
Gate loss:  tensor(90.5076, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11904, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.4528)
idx:  72
Gate loss:  tensor(95.7395, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.3513)
idx:  73
Gate loss:  tensor(101.0638, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1550, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.1026)
idx:  74
Gate loss:  tensor(106.1012, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(922, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.5268)
idx:  75
Gate loss:  tensor(111.2538, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(28789, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.0466)
idx:  77
Gate loss:  tensor(117.5388, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(871, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.5668)
idx:  78
Gate loss:  tensor(130.0349, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7450, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.0771)
idx:  79
Gate loss:  tensor(138.7657, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.3167)
idx:  80
Gate loss:  tensor(151.7263, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([92])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.2396)
idx:  81
Gate loss:  tensor(163.1671, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-15.4040)
idx:  82
Gate loss:  tensor(175.6694, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(386, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-21.7094)
idx:  83
Gate loss:  tensor(197.1621, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(11904, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-25.6083)
idx:  84
Gate loss:  tensor(222.1991, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-33.5654)
idx:  85
Gate loss:  tensor(255.2704, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-35.6212)
idx:  86
Gate loss:  tensor(280.7992, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-23.5278)
idx:  87
Gate loss:  tensor(293.9242, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 65
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4530)
idx:  6
Gate loss:  tensor(295.2315, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1261)
idx:  10
Gate loss:  tensor(296.3511, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3971, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1745)
idx:  11
Gate loss:  tensor(297.5237, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2522)
idx:  12
Gate loss:  tensor(298.6511, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3583)
idx:  13
Gate loss:  tensor(299.9631, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29929, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5060)
idx:  14
Gate loss:  tensor(301.2580, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5272)
idx:  15
Gate loss:  tensor(302.7533, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12118, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0950)
idx:  16
Gate loss:  tensor(303.8116, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(468, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5980)
idx:  17
Gate loss:  tensor(304.7221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25402, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5417)
idx:  18
Gate loss:  tensor(305.5876, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1386)
idx:  19
Gate loss:  tensor(306.7000, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9415, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4142)
idx:  20
Gate loss:  tensor(307.6538, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1894)
idx:  21
Gate loss:  tensor(308.8293, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3697)
idx:  22
Gate loss:  tensor(310.1950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3468)
idx:  23
Gate loss:  tensor(311.5199, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3143, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4619)
idx:  24
Gate loss:  tensor(312.9533, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1183, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1073)
idx:  25
Gate loss:  tensor(314.0375, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29877, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4072)
idx:  27
Gate loss:  tensor(315.1095, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18093, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2916)
idx:  28
Gate loss:  tensor(316.3983, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(471, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1817)
idx:  29
Gate loss:  tensor(317.5781, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2504)
idx:  30
Gate loss:  tensor(318.8279, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2904)
idx:  31
Gate loss:  tensor(320.0366, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4157)
idx:  32
Gate loss:  tensor(321.3303, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4959)
idx:  33
Gate loss:  tensor(322.4359, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5463)
idx:  34
Gate loss:  tensor(323.9485, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12118, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5260)
idx:  35
Gate loss:  tensor(325.4656, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(12118, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5222)
idx:  36
Gate loss:  tensor(326.9762, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(468, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5868)
idx:  37
Gate loss:  tensor(327.8972, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(25402, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6881)
idx:  38
Gate loss:  tensor(328.9454, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5162)
idx:  39
Gate loss:  tensor(330.4357, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(9415, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7237)
idx:  40
Gate loss:  tensor(331.8601, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(467, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6604)
idx:  41
Gate loss:  tensor(333.4568, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2910)
idx:  42
Gate loss:  tensor(334.6303, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18093, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8700)
idx:  43
Gate loss:  tensor(336.4747, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9330)
idx:  44
Gate loss:  tensor(338.3797, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14200, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9708)
idx:  45
Gate loss:  tensor(340.3039, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(631, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2744)
idx:  46
Gate loss:  tensor(341.9033, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(3143, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0961)
idx:  47
Gate loss:  tensor(343.9924, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4375)
idx:  48
Gate loss:  tensor(345.3738, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5300)
idx:  49
Gate loss:  tensor(346.7309, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2944)
idx:  50
Gate loss:  tensor(347.5971, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8059)
idx:  51
Gate loss:  tensor(348.8861, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.2482)
idx:  54
Gate loss:  tensor(350.9329, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.7463)
idx:  55
Gate loss:  tensor(352.0852, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(18093, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4932)
idx:  59
Gate loss:  tensor(354.5371, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5102)
idx:  60
Gate loss:  tensor(357.0358, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(14200, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0813)
idx:  61
Gate loss:  tensor(360.1004, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8277, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1752)
idx:  63
Gate loss:  tensor(363.2635, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.8057)
idx:  64
Gate loss:  tensor(366.0150, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.1406)
idx:  65
Gate loss:  tensor(367.9978, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6655)
idx:  66
Gate loss:  tensor(369.9554, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9971)
idx:  67
Gate loss:  tensor(372.8978, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1405)
idx:  68
Gate loss:  tensor(375.6909, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8164)
idx:  69
Gate loss:  tensor(379.4446, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1405, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8074)
idx:  70
Gate loss:  tensor(383.2475, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.5448)
idx:  73
Gate loss:  tensor(387.7638, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7621, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.5880)
idx:  74
Gate loss:  tensor(392.3360, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1135, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.5652)
idx:  75
Gate loss:  tensor(396.8470, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.2686)
idx:  76
Gate loss:  tensor(402.0636, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.9285)
idx:  77
Gate loss:  tensor(409.4406, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([99])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.4687)
idx:  78
Gate loss:  tensor(419.7867, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.0344)
idx:  79
Gate loss:  tensor(426.9609, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.1288)
idx:  80
Gate loss:  tensor(438.2155, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.4803)
idx:  81
Gate loss:  tensor(450.4734, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4679, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-13.1603)
idx:  82
Gate loss:  tensor(463.3025, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-25.2327)
idx:  85
Gate loss:  tensor(487.3260, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-35.7343)
idx:  86
Gate loss:  tensor(513.3004, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-26.1254)
idx:  87
Gate loss:  tensor(528.3953, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 69
The shape of hidden_states: torch.Size([4096])
The topk tensor(338, device='cuda:1')
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
torch.Size([6])
torch.Size([1])
torch.Size([6])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2284)
idx:  5
Gate loss:  tensor(529.5389, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
torch.Size([7])
torch.Size([1])
torch.Size([7])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0170)
idx:  6
Gate loss:  tensor(530.4750, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(393, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1642)
idx:  7
Gate loss:  tensor(531.5521, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(937, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2266)
idx:  9
Gate loss:  tensor(532.2372, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2022, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1562)
idx:  10
Gate loss:  tensor(533.2181, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(24298, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1208)
idx:  12
Gate loss:  tensor(533.9963, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1850, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3583)
idx:  13
Gate loss:  tensor(535.2016, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0132)
idx:  14
Gate loss:  tensor(536.2045, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
torch.Size([16])
torch.Size([1])
torch.Size([16])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1038)
idx:  15
Gate loss:  tensor(536.9843, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8041, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1441)
idx:  16
Gate loss:  tensor(537.9993, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4655, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0637)
idx:  17
Gate loss:  tensor(538.7950, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9575)
idx:  18
Gate loss:  tensor(539.7438, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0713)
idx:  19
Gate loss:  tensor(540.7980, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
torch.Size([21])
torch.Size([1])
torch.Size([21])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2050)
idx:  20
Gate loss:  tensor(541.4972, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1009)
idx:  21
Gate loss:  tensor(542.5612, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1831)
idx:  22
Gate loss:  tensor(543.7402, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0803)
idx:  23
Gate loss:  tensor(544.8088, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29953, device='cuda:1')
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
torch.Size([25])
torch.Size([1])
torch.Size([25])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1151)
idx:  24
Gate loss:  tensor(545.7034, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2959)
idx:  25
Gate loss:  tensor(546.7051, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
torch.Size([27])
torch.Size([1])
torch.Size([27])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2145)
idx:  26
Gate loss:  tensor(547.9087, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3983)
idx:  27
Gate loss:  tensor(549.2957, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4720)
idx:  28
Gate loss:  tensor(550.1601, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5972)
idx:  29
Gate loss:  tensor(551.1270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4933)
idx:  30
Gate loss:  tensor(552.5984, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3484)
idx:  31
Gate loss:  tensor(553.9404, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5497)
idx:  32
Gate loss:  tensor(555.4841, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6284)
idx:  33
Gate loss:  tensor(556.6565, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6130)
idx:  34
Gate loss:  tensor(557.8149, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
torch.Size([36])
torch.Size([1])
torch.Size([36])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2905)
idx:  35
Gate loss:  tensor(559.0933, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8307, device='cuda:1')
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
torch.Size([37])
torch.Size([1])
torch.Size([37])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2588)
idx:  36
Gate loss:  tensor(560.3084, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3812)
idx:  37
Gate loss:  tensor(561.6662, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
torch.Size([39])
torch.Size([1])
torch.Size([39])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2547)
idx:  38
Gate loss:  tensor(562.5907, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1784, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0417)
idx:  39
Gate loss:  tensor(564.6064, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8041, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9288)
idx:  40
Gate loss:  tensor(566.4804, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6678)
idx:  41
Gate loss:  tensor(568.1271, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(727, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0293)
idx:  42
Gate loss:  tensor(570.1527, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6847)
idx:  43
Gate loss:  tensor(571.8210, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(385, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0398)
idx:  44
Gate loss:  tensor(573.7558, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6588, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0021)
idx:  45
Gate loss:  tensor(575.4797, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8158, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8472)
idx:  46
Gate loss:  tensor(577.2928, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9253)
idx:  47
Gate loss:  tensor(579.1822, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4060)
idx:  48
Gate loss:  tensor(581.0203, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.0766)
idx:  49
Gate loss:  tensor(582.8965, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29973, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4302)
idx:  50
Gate loss:  tensor(584.3242, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4449)
idx:  51
Gate loss:  tensor(585.7253, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4675)
idx:  52
Gate loss:  tensor(587.0186, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8115)
idx:  53
Gate loss:  tensor(588.5038, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5026)
idx:  56
Gate loss:  tensor(590.8601, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9198)
idx:  57
Gate loss:  tensor(592.6242, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(8439, device='cuda:1')
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
torch.Size([59])
torch.Size([1])
torch.Size([59])
Inside custom generate sample func
reasoning_path shape:  torch.Size([91])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.4035)
idx:  58
Gate loss:  tensor(595.0278, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
torch.Size([60])
torch.Size([1])
torch.Size([60])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5846)
idx:  59
Gate loss:  tensor(597.6049, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6878)
idx:  60
Gate loss:  tensor(600.2920, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(4413, device='cuda:1')
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
torch.Size([62])
torch.Size([1])
torch.Size([62])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.7496)
idx:  61
Gate loss:  tensor(603.0332, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7050)
idx:  62
Gate loss:  tensor(606.7201, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(385, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.0589)
idx:  63
Gate loss:  tensor(609.6805, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6588, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7500)
idx:  64
Gate loss:  tensor(612.3557, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(2038, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3162)
idx:  65
Gate loss:  tensor(615.5907, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([93])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.4418)
idx:  66
Gate loss:  tensor(619.9175, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.0889)
idx:  67
Gate loss:  tensor(623.5782, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29900, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.2883)
idx:  68
Gate loss:  tensor(627.7982, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.7857)
idx:  69
Gate loss:  tensor(631.5625, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.6458)
idx:  70
Gate loss:  tensor(635.1565, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(526, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.3635)
idx:  71
Gate loss:  tensor(639.4470, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.0139)
idx:  72
Gate loss:  tensor(646.3586, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([97])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.7964)
idx:  73
Gate loss:  tensor(652.1019, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.2360)
idx:  74
Gate loss:  tensor(657.2690, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(6588, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.8155)
idx:  75
Gate loss:  tensor(663.9045, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([98])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.2431)
idx:  76
Gate loss:  tensor(671.0778, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.2860)
idx:  77
Gate loss:  tensor(678.1650, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-8.3883)
idx:  78
Gate loss:  tensor(686.5054, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29955, device='cuda:1')
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
torch.Size([80])
torch.Size([1])
torch.Size([80])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-11.1571)
idx:  79
Gate loss:  tensor(696.2426, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
torch.Size([81])
torch.Size([1])
torch.Size([81])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.0028)
idx:  80
Gate loss:  tensor(706.0341, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(322, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-12.5648)
idx:  81
Gate loss:  tensor(718.5633, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([94])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.7233)
idx:  82
Gate loss:  tensor(735.0056, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29947, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([95])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-16.7068)
idx:  83
Gate loss:  tensor(748.6145, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29906, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-20.7926)
idx:  84
Gate loss:  tensor(765.7592, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.0577)
idx:  85
Gate loss:  tensor(792.4025, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-31.8883)
idx:  86
Gate loss:  tensor(821.9335, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-27.0852)
idx:  87
Gate loss:  tensor(837.1355, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 80
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
torch.Size([8])
torch.Size([1])
torch.Size([8])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7814)
idx:  7
Gate loss:  tensor(837.8918, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
torch.Size([9])
torch.Size([1])
torch.Size([9])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7180)
idx:  8
Gate loss:  tensor(838.4996, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
torch.Size([10])
torch.Size([1])
torch.Size([10])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7810)
idx:  9
Gate loss:  tensor(839.1947, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29896, device='cuda:1')
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
torch.Size([11])
torch.Size([1])
torch.Size([11])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6849)
idx:  10
Gate loss:  tensor(839.6048, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
torch.Size([12])
torch.Size([1])
torch.Size([12])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8245)
idx:  11
Gate loss:  tensor(840.3114, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
torch.Size([13])
torch.Size([1])
torch.Size([13])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7770)
idx:  12
Gate loss:  tensor(840.9023, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
torch.Size([14])
torch.Size([1])
torch.Size([14])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.8389)
idx:  13
Gate loss:  tensor(841.3274, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
torch.Size([15])
torch.Size([1])
torch.Size([15])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9368)
idx:  14
Gate loss:  tensor(842.2485, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(265, device='cuda:1')
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
torch.Size([17])
torch.Size([1])
torch.Size([17])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1356)
idx:  16
Gate loss:  tensor(843.0434, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1338, device='cuda:1')
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
torch.Size([18])
torch.Size([1])
torch.Size([18])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9966)
idx:  17
Gate loss:  tensor(843.7143, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
torch.Size([19])
torch.Size([1])
torch.Size([19])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7815)
idx:  18
Gate loss:  tensor(844.4234, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(263, device='cuda:1')
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
torch.Size([20])
torch.Size([1])
torch.Size([20])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9052)
idx:  19
Gate loss:  tensor(845.0925, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
torch.Size([22])
torch.Size([1])
torch.Size([22])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7056)
idx:  21
Gate loss:  tensor(845.7275, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
torch.Size([23])
torch.Size([1])
torch.Size([23])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6274)
idx:  22
Gate loss:  tensor(846.2851, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
torch.Size([24])
torch.Size([1])
torch.Size([24])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.4893)
idx:  23
Gate loss:  tensor(846.7184, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29937, device='cuda:1')
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
torch.Size([26])
torch.Size([1])
torch.Size([26])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7883)
idx:  25
Gate loss:  tensor(847.2098, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29901, device='cuda:1')
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
torch.Size([28])
torch.Size([1])
torch.Size([28])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6864)
idx:  27
Gate loss:  tensor(847.8729, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
torch.Size([29])
torch.Size([1])
torch.Size([29])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.6219)
idx:  28
Gate loss:  tensor(848.4496, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1576, device='cuda:1')
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
torch.Size([30])
torch.Size([1])
torch.Size([30])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7470)
idx:  29
Gate loss:  tensor(849.0097, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16701, device='cuda:1')
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
torch.Size([31])
torch.Size([1])
torch.Size([31])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7310)
idx:  30
Gate loss:  tensor(849.6770, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(411, device='cuda:1')
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
torch.Size([32])
torch.Size([1])
torch.Size([32])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2576)
idx:  31
Gate loss:  tensor(850.8695, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
torch.Size([33])
torch.Size([1])
torch.Size([33])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1285)
idx:  32
Gate loss:  tensor(851.9921, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29946, device='cuda:1')
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
torch.Size([34])
torch.Size([1])
torch.Size([34])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9484)
idx:  33
Gate loss:  tensor(852.6938, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7936, device='cuda:1')
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
torch.Size([35])
torch.Size([1])
torch.Size([35])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0413)
idx:  34
Gate loss:  tensor(853.6873, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
torch.Size([38])
torch.Size([1])
torch.Size([38])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9123)
idx:  37
Gate loss:  tensor(854.5820, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
torch.Size([40])
torch.Size([1])
torch.Size([40])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.9055)
idx:  39
Gate loss:  tensor(855.4484, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(788, device='cuda:1')
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
torch.Size([41])
torch.Size([1])
torch.Size([41])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-0.7872)
idx:  40
Gate loss:  tensor(856.2193, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
torch.Size([42])
torch.Size([1])
torch.Size([42])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0075)
idx:  41
Gate loss:  tensor(857.2198, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
torch.Size([43])
torch.Size([1])
torch.Size([43])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2894)
idx:  42
Gate loss:  tensor(858.2999, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(304, device='cuda:1')
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
torch.Size([44])
torch.Size([1])
torch.Size([44])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.0112)
idx:  43
Gate loss:  tensor(859.3071, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
torch.Size([45])
torch.Size([1])
torch.Size([45])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1469)
idx:  44
Gate loss:  tensor(860.4377, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1353, device='cuda:1')
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
torch.Size([46])
torch.Size([1])
torch.Size([46])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.2269)
idx:  45
Gate loss:  tensor(861.5242, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
torch.Size([47])
torch.Size([1])
torch.Size([47])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1973)
idx:  46
Gate loss:  tensor(862.6967, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7936, device='cuda:1')
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
torch.Size([48])
torch.Size([1])
torch.Size([48])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1079)
idx:  47
Gate loss:  tensor(863.7612, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(265, device='cuda:1')
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
torch.Size([49])
torch.Size([1])
torch.Size([49])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9565)
idx:  48
Gate loss:  tensor(865.4221, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1338, device='cuda:1')
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
torch.Size([50])
torch.Size([1])
torch.Size([50])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.5814)
idx:  49
Gate loss:  tensor(866.5234, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
torch.Size([51])
torch.Size([1])
torch.Size([51])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4644)
idx:  50
Gate loss:  tensor(867.9647, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
torch.Size([52])
torch.Size([1])
torch.Size([52])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8092)
idx:  51
Gate loss:  tensor(869.7255, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16701, device='cuda:1')
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
torch.Size([53])
torch.Size([1])
torch.Size([53])
Inside custom generate sample func
reasoning_path shape:  torch.Size([99])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.8651)
idx:  52
Gate loss:  tensor(871.5007, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
torch.Size([54])
torch.Size([1])
torch.Size([54])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.3260)
idx:  53
Gate loss:  tensor(872.7996, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(591, device='cuda:1')
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
torch.Size([55])
torch.Size([1])
torch.Size([55])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6922)
idx:  54
Gate loss:  tensor(874.4696, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(679, device='cuda:1')
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
torch.Size([56])
torch.Size([1])
torch.Size([56])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6562)
idx:  55
Gate loss:  tensor(876.1138, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
torch.Size([57])
torch.Size([1])
torch.Size([57])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.1698)
idx:  56
Gate loss:  tensor(877.2676, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
torch.Size([58])
torch.Size([1])
torch.Size([58])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.4134)
idx:  57
Gate loss:  tensor(878.5776, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
torch.Size([61])
torch.Size([1])
torch.Size([61])
Inside custom generate sample func
reasoning_path shape:  torch.Size([94])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1816)
idx:  60
Gate loss:  tensor(881.7355, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
torch.Size([63])
torch.Size([1])
torch.Size([63])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.9075)
idx:  62
Gate loss:  tensor(883.5967, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(7936, device='cuda:1')
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
torch.Size([64])
torch.Size([1])
torch.Size([64])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-1.6529)
idx:  63
Gate loss:  tensor(885.1422, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(265, device='cuda:1')
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
torch.Size([65])
torch.Size([1])
torch.Size([65])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9666)
idx:  64
Gate loss:  tensor(887.1691, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1338, device='cuda:1')
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
torch.Size([66])
torch.Size([1])
torch.Size([66])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.5672)
idx:  65
Gate loss:  tensor(888.5328, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
torch.Size([67])
torch.Size([1])
torch.Size([67])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4964)
idx:  66
Gate loss:  tensor(891.0118, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(278, device='cuda:1')
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
torch.Size([68])
torch.Size([1])
torch.Size([68])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6700)
idx:  67
Gate loss:  tensor(893.5013, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(16701, device='cuda:1')
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
torch.Size([69])
torch.Size([1])
torch.Size([69])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.1638)
idx:  68
Gate loss:  tensor(896.5816, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29897, device='cuda:1')
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
torch.Size([70])
torch.Size([1])
torch.Size([70])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.6040)
idx:  69
Gate loss:  tensor(899.1563, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(718, device='cuda:1')
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
torch.Size([71])
torch.Size([1])
torch.Size([71])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.9290)
idx:  70
Gate loss:  tensor(902.0360, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
torch.Size([72])
torch.Size([1])
torch.Size([72])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3980)
idx:  71
Gate loss:  tensor(905.2776, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29941, device='cuda:1')
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
torch.Size([73])
torch.Size([1])
torch.Size([73])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.5071)
idx:  72
Gate loss:  tensor(908.1270, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(353, device='cuda:1')
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
torch.Size([74])
torch.Size([1])
torch.Size([74])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-2.4284)
idx:  73
Gate loss:  tensor(910.5320, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
torch.Size([75])
torch.Size([1])
torch.Size([75])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.0742)
idx:  74
Gate loss:  tensor(915.4743, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
torch.Size([76])
torch.Size([1])
torch.Size([76])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.8830)
idx:  75
Gate loss:  tensor(918.5479, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(313, device='cuda:1')
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
torch.Size([77])
torch.Size([1])
torch.Size([77])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-6.2806)
idx:  76
Gate loss:  tensor(924.7455, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
torch.Size([78])
torch.Size([1])
torch.Size([78])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-3.3546)
idx:  77
Gate loss:  tensor(927.9059, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
torch.Size([79])
torch.Size([1])
torch.Size([79])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-4.4851)
idx:  78
Gate loss:  tensor(931.8834, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29892, device='cuda:1')
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
torch.Size([82])
torch.Size([1])
torch.Size([82])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-5.9154)
idx:  81
Gate loss:  tensor(937.4077, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(1234, device='cuda:1')
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
torch.Size([83])
torch.Size([1])
torch.Size([83])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-7.4412)
idx:  82
Gate loss:  tensor(942.1043, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(310, device='cuda:1')
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
torch.Size([84])
torch.Size([1])
torch.Size([84])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-9.2122)
idx:  83
Gate loss:  tensor(950.9377, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29871, device='cuda:1')
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
torch.Size([85])
torch.Size([1])
torch.Size([85])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-10.9691)
idx:  84
Gate loss:  tensor(961.8228, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29945, device='cuda:1')
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
torch.Size([86])
torch.Size([1])
torch.Size([86])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-14.4284)
idx:  85
Gate loss:  tensor(975.8524, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(29889, device='cuda:1')
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
torch.Size([87])
torch.Size([1])
torch.Size([87])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-15.4758)
idx:  86
Gate loss:  tensor(991.2730, device='cuda:1', grad_fn=<AddBackward0>)
The shape of hidden_states: torch.Size([4096])
The topk tensor(13, device='cuda:1')
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
torch.Size([88])
torch.Size([1])
torch.Size([88])
Inside custom generate sample func
reasoning_path shape:  torch.Size([100])
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
The batchy value is 0
Batchy inside else: 0
Preparing 4D causal attention mask with cache position
0
1
2
Mean reward signals:  tensor(-25.9119)
idx:  87
Gate loss:  tensor(1015.8137, device='cuda:1', grad_fn=<AddBackward0>)
The count is: 70
Training Epoch: 1:   0%|[34mâ–‹                                                                                                                                                 [0m| 1/225 [06:05<22:42:47, 365.03s/it][0mTraceback (most recent call last):
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 848, in <module>
    fire.Fire(main)
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/env/tv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 830, in main
    results = train(
              ^^^^^^
  File "/data/data/arrv/ThinkTuning_v1/train.py", line 440, in train
    'train/thought_loss': thought_loss.detach().float(),
                          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'float' object has no attribute 'detach'
